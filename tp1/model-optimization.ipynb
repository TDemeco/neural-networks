{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - 22.45 Redes Neuronales - Regresion Lógistica y Lineal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lógistica - Busqueda de Modelos Óptimos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 14:42:38.645574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-22 14:42:38.741182: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-22 14:42:39.090949: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tobid/itba/neural-networks/tp1/env/lib\n",
      "2023-05-22 14:42:39.090997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tobid/itba/neural-networks/tp1/env/lib\n",
      "2023-05-22 14:42:39.091002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-22 14:42:39.632565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0e:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-22 14:42:39.636321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0e:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-22 14:42:39.636355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0e:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "/home/tobid/itba/neural-networks/tp1/env/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from os.path import exists\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow_addons as tfa\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import keras_tuner\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and load Fashion MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_max = np.max(train_X)\n",
    "train_X = train_X.astype('float32') / data_max\n",
    "test_X = test_X.astype('float32') / data_max\n",
    "np.max(train_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert it to categorical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1\n",
    "train_y_cat = utils.to_categorical(train_y, num_classes)\n",
    "test_y_cat = utils.to_categorical(test_y, num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring preeliminary optimizer analysis parameters and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log stats and metrics for TensorBoard\n",
    "log_dir = \"logs/softmax/optimizer-analysis\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Accuracy (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Loss (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_f1_score\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"F1 Score Macro (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_f1_score_micro\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"F1 Score Micro (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_recall\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Recall (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_precision\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Precision (val.)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop', 'adamw', 'nadam', 'adagrad', 'adadelta', 'ftrl', 'adamax', 'adafactor']))\n",
    "HP_LEARN_RATE = hp.HParam('learning_rate', hp.Discrete([0.0001, 0.001, 0.01, 0.1]))\n",
    "HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([ 0.9, 0.95, 0.99]))\n",
    "HP_NESTEROV = hp.HParam('nesterov', hp.Discrete([True, False]))\n",
    "HP_RHO = hp.HParam('rho', hp.Discrete([0.92, 0.95, 0.97]))\n",
    "HP_BETA_1 = hp.HParam('beta_1', hp.Discrete([0.86, 0.9, 0.94]))\n",
    "HP_BETA_2 = hp.HParam('beta_2', hp.Discrete([0.97, 0.99, 0.999]))\n",
    "HP_BETA_2_DECAY = hp.HParam('beta_2_decay', hp.Discrete([-0.9, -0.8, -0.7]))\n",
    "HP_WEIGHT_DECAY = hp.HParam('weight_decay', hp.Discrete([0.0001, 0.001, 0.004, 0.01]))\n",
    "HP_LEARN_RATE_POWER = hp.HParam('learning_rate_power', hp.Discrete([-0.8, -0.5, -0.0]))\n",
    "\n",
    "HPARAMS = [HP_OPTIMIZER, HP_LEARN_RATE, HP_MOMENTUM, HP_NESTEROV, HP_RHO, HP_BETA_1, HP_BETA_2, HP_BETA_2_DECAY, HP_WEIGHT_DECAY, HP_LEARN_RATE_POWER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 11:39:19.083040: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=HPARAMS,\n",
    "    metrics=METRICS,\n",
    "  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preeliminary Optimizer Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizó una busqueda preeliminar de los optimizadores que mejor se comportaran, que a pesar de ser un análisis limitado permitió concentrarse en algunos optimizadores específicos para el análisis posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax function to get models for optimizer testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, run_dir):\n",
    "  softmax_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax),\n",
    "  ])\n",
    "\n",
    "  if(hparams[HP_OPTIMIZER] == 'sgd'):\n",
    "    optimizer = optimizers.SGD(learning_rate=hparams[HP_LEARN_RATE], momentum=hparams[HP_MOMENTUM], nesterov=hparams[HP_NESTEROV])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adam'):\n",
    "    optimizer = optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'rmsprop'):\n",
    "    optimizer = optimizers.RMSprop(learning_rate=hparams[HP_LEARN_RATE], rho=hparams[HP_RHO], momentum=hparams[HP_MOMENTUM])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adadelta'):\n",
    "    optimizer = optimizers.Adadelta(learning_rate=hparams[HP_LEARN_RATE], rho=hparams[HP_RHO])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adagrad'):\n",
    "    optimizer = optimizers.Adagrad(learning_rate=hparams[HP_LEARN_RATE])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adamax'):\n",
    "    optimizer = optimizers.Adamax(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'nadam'):\n",
    "    optimizer = optimizers.Nadam(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'ftrl'):\n",
    "    optimizer = optimizers.Ftrl(learning_rate=hparams[HP_LEARN_RATE], learning_rate_power=hparams[HP_LEARN_RATE_POWER])\n",
    "\n",
    "  softmax_model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",
    "  )\n",
    "\n",
    "  callbacks = [\n",
    "      early_stop_callback,\n",
    "      tf.keras.callbacks.TensorBoard(run_dir), # log metrics\n",
    "      hp.KerasCallback(run_dir, hparams),  # log hparams\n",
    "    ]\n",
    "\n",
    "  softmax_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs = 5, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for optimizer in HP_OPTIMIZER.domain.values:\n",
    "  for learning_rate in HP_LEARN_RATE.domain.values:\n",
    "    if(optimizer == 'adagrad'):\n",
    "      hparams = {\n",
    "        HP_OPTIMIZER: optimizer,\n",
    "        HP_LEARN_RATE: learning_rate,\n",
    "      }\n",
    "      run_name = \"/run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      train_test_model(hparams, log_dir + run_name)\n",
    "      session_num += 1\n",
    "    elif(optimizer == 'adadelta'):\n",
    "      for rho in HP_RHO.domain.values:\n",
    "        hparams = {\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "          HP_LEARN_RATE: learning_rate,\n",
    "          HP_RHO: rho,\n",
    "        }\n",
    "        run_name = \"/run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        train_test_model(hparams, log_dir + run_name)\n",
    "        session_num += 1\n",
    "    elif(optimizer == 'ftrl'):\n",
    "      for learning_rate_power in HP_LEARN_RATE_POWER.domain.values:\n",
    "        hparams = {\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "          HP_LEARN_RATE: learning_rate,\n",
    "          HP_LEARN_RATE_POWER: learning_rate_power,\n",
    "        }\n",
    "        run_name = \"/run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        train_test_model(hparams, log_dir + run_name)\n",
    "        session_num += 1\n",
    "    elif(optimizer == 'sgd' or optimizer == 'rmsprop'):\n",
    "      for momentum in HP_MOMENTUM.domain.values:\n",
    "        if(optimizer == 'sgd'):\n",
    "          for nesterov in HP_NESTEROV.domain.values:\n",
    "            hparams = {\n",
    "              HP_OPTIMIZER: optimizer,\n",
    "              HP_LEARN_RATE: learning_rate,\n",
    "              HP_MOMENTUM: momentum,\n",
    "              HP_NESTEROV: nesterov,\n",
    "            }\n",
    "            run_name = \"/run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            train_test_model(hparams, log_dir + run_name)\n",
    "            session_num += 1\n",
    "        else:\n",
    "          for rho in HP_RHO.domain.values:\n",
    "            hparams = {\n",
    "              HP_OPTIMIZER: optimizer,\n",
    "              HP_LEARN_RATE: learning_rate,\n",
    "              HP_MOMENTUM: momentum,\n",
    "              HP_RHO: rho,\n",
    "            }\n",
    "            run_name = \"/run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            train_test_model(hparams, log_dir + run_name)\n",
    "            session_num += 1\n",
    "    elif(optimizer == 'adam' or optimizer == 'adamax' or optimizer == 'nadam'):\n",
    "      for beta_1 in HP_BETA_1.domain.values:\n",
    "        for beta_2 in HP_BETA_2.domain.values:\n",
    "          hparams = {\n",
    "            HP_OPTIMIZER: optimizer,\n",
    "            HP_LEARN_RATE: learning_rate,\n",
    "            HP_BETA_1: beta_1,\n",
    "            HP_BETA_2: beta_2,\n",
    "          }\n",
    "          run_name = \"/run-%d\" % session_num\n",
    "          print('--- Starting trial: %s' % run_name)\n",
    "          print({h.name: hparams[h] for h in hparams})\n",
    "          train_test_model(hparams, log_dir + run_name)\n",
    "          session_num += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TensorBoard (Data under HPARAMS table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos entonces un análisis preeliminar de los distintos optimizadores disponibles, y observamos que los que mejor se comportan en terminos de maximizar la accuracy son SGD, RMSprop, Adam y nAdam.\n",
    "Estos resultados son claramente imperfectos ya que:\n",
    "- Fueron realizados a solo 5 epochs (lo que le da una ventaja a learning rates altos, y no todos los optimizadores se comportan bien con los mismos).\n",
    "- No se analizaron otras cuestiones como agregar capas de dropout, batch normalization o cambiar la función de costo, que podrían mejorar la performance de algunos optimizadores en específico.\n",
    "- Solo se realizó para SoftmaxReg, y otros optimizadores podrían ser mejor para MLP.\n",
    "Sin embargo, sirve como análisis preeliminar para observar cuales optimizadores se comportan bien en general para este problema y realizar un análisis más en profundidad de los mismos, para no requerir tanto tiempo de entrenamiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Hyperparameter tuning callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log stats and metrics for TensorBoard\n",
    "log_dir = \"testing/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to save the weights of the best model\n",
    "checkpoint_filepath = log_dir + '/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to build the models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_softmax(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Test with and without dropout, with different rates (0.1, 0.3, 0.5)\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.2, parent_name=\"dropout\", parent_values=[True])))\n",
    "\n",
    "    # Test with and without batch normalization\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Test learning rates 0.0001, 0.001 and 0.01   \n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\", step=10)\n",
    "\n",
    "    # Test best optimizers from the preeliminary optimizer analysis\n",
    "    optimizer_list = hp.Choice(\"optimizer\", [\"adam\", \"nadam\", \"sgd\", \"rmsprop\"])\n",
    "    if(optimizer_list == \"adam\" or optimizer_list == \"nadam\"):\n",
    "        beta_1 = hp.Float(\"beta_1\", min_value=0.87, max_value=0.99, step=0.04, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        beta_2 = hp.Float(\"beta_2\", min_value=0.939, max_value=0.999, step=0.03, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "    elif(optimizer_list == \"sgd\"):\n",
    "        momentum_1 = hp.Float(\"momentum_1\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"sgd\"])\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=momentum_1)\n",
    "    elif(optimizer_list == \"rmsprop\"):\n",
    "        momentum_2 = hp.Float(\"momentum_2\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        rho = hp.Float(\"rho\", min_value=0.9, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, rho=rho, momentum=momentum_2)\n",
    "\n",
    "    # The loss function is fixed, we will test changing it after we decide which model to use\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de este test se pueden encontrar en la carpeta testing/softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "dropout (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "batch_normalization (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': 10, 'sampling': 'log'}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'nadam', 'sgd', 'rmsprop'], 'ordered': False}\n",
      "beta_1 (Float)\n",
      "{'default': 0.87, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'optimizer', 'values': ['adam', 'nadam']}}], 'min_value': 0.87, 'max_value': 0.99, 'step': 0.04, 'sampling': 'linear'}\n",
      "beta_2 (Float)\n",
      "{'default': 0.939, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'optimizer', 'values': ['adam', 'nadam']}}], 'min_value': 0.939, 'max_value': 0.999, 'step': 0.03, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Build the hyperparameter tuner\n",
    "tuner_softmax = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model_softmax,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    overwrite=True,\n",
    "    directory=log_dir + '/hparams',\n",
    "    project_name=\"nn-tp1\",\n",
    ")\n",
    "\n",
    "# Summary\n",
    "tuner_softmax.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing (batch_size is fixed, we will test changing it after we decide which model to use)\n",
    "tuner_softmax.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, callbacks=[early_stop_callback, tensorboard_callback])\n",
    "tuner_softmax.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que los mejores modelos para SoftmaxReg entonces resultan ser:\n",
    "- adam: sin dropout ni batch normalization, con learning rate de 0.001, beta 1 de 0.95 y beta 2 de 0.939\n",
    "- rmsprop: sin dropout ni batch normalization, con learning rate de 0.0001, momentum de 0.93 y rho de 0.93\n",
    "- sgd: sin dropout pero con batch normalization, con learning rate de 0.001, momentum de 0.93\n",
    "\n",
    "Por su rápida velocidad de convergencia, que limita la cantidad de tiempo gastado en entrenamiento, eligiremos el modelo Adam para SoftmaxReg."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-depth Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el optimizador Adam ya elegido, podemos hacer un tuning de sus parametros más preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
    "early_stop_callback_softmax_adam = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log stats and metrics for TensorBoard\n",
    "log_dir_softmax_adam = \"testing/softmax/adam/\" \n",
    "tensorboard_callback_softmax_adam = tf.keras.callbacks.TensorBoard(log_dir=log_dir_softmax_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to save the weights of the best model\n",
    "checkpoint_filepath_softmax_adam = log_dir_softmax_adam + 'checkpoint/'\n",
    "model_checkpoint_callback_softmax_adam = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_softmax_adam,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "softback_adam_callbacks = [early_stop_callback_softmax_adam, tensorboard_callback_softmax_adam, model_checkpoint_callback_softmax_adam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_softmax_adam(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Test with and without dropout, with different rates (0.1, 0.3, 0.5)\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.1, parent_name=\"dropout\", parent_values=[True])))\n",
    "\n",
    "    # Test with and without batch normalization\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Test initial weights\n",
    "    if hp.Boolean(\"with_initial_weights\"):\n",
    "        initial_weights = hp.Choice(\"initial_weights\", [\"random_normal_1e-1\", \"random_normal_1e-3\", \"glorot_normal\", \"glorot_uniform\"], parent_name=\"with_initial_weights\", parent_values=[True])\n",
    "        if(initial_weights == \"random_normal_1e-1\"):\n",
    "            initial_weights = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
    "        elif(initial_weights == \"random_normal_1e-3\"):\n",
    "            initial_weights = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001)\n",
    "        elif(initial_weights == \"glorot_normal\"):\n",
    "            initial_weights = tf.keras.initializers.GlorotNormal()\n",
    "        elif(initial_weights == \"glorot_uniform\"):\n",
    "            initial_weights = tf.keras.initializers.GlorotUniform()\n",
    "        model.add(layers.Dense(num_classes, activation='softmax', kernel_initializer=initial_weights))\n",
    "    else:\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    # Test learning rates 0.0001, 0.001 and 0.01   \n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    # Test betas\n",
    "    beta_1 = hp.Float(\"beta_1\", min_value=0.85, max_value=0.99, step=0.02)\n",
    "    beta_2 = hp.Float(\"beta_2\", min_value=0.919, max_value=0.999, step=0.02)\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "\n",
    "    # Test loss functions\n",
    "    loss = hp.Choice(\"loss\", [\"categorical_crossentropy\", \"poisson\", \"kl_divergence\", \"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\", \"mean_squared_logarithmic_error\", \"cosine_similarity\", \"log_cosh\"])\n",
    "\n",
    "    metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",
    "\n",
    "    # The loss function is fixed, we will test changing it after we decide which model to use\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAdamHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model_softmax_adam(hp)\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = hp.Int(\"batch_size\", min_value=32, max_value=512, sampling=\"log\", step=2), callbacks=softback_adam_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "dropout (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "batch_normalization (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "with_initial_weights (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "beta_1 (Float)\n",
      "{'default': 0.85, 'conditions': [], 'min_value': 0.85, 'max_value': 0.99, 'step': 0.02, 'sampling': 'linear'}\n",
      "beta_2 (Float)\n",
      "{'default': 0.919, 'conditions': [], 'min_value': 0.919, 'max_value': 0.999, 'step': 0.02, 'sampling': 'linear'}\n",
      "loss (Choice)\n",
      "{'default': 'categorical_crossentropy', 'conditions': [], 'values': ['categorical_crossentropy', 'poisson', 'kl_divergence', 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_logarithmic_error', 'cosine_similarity', 'log_cosh'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build the hyperparameter tuner\n",
    "tuner_softmax_adam = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model_softmax_adam,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    overwrite=True,\n",
    "    directory=log_dir_softmax_adam + '/hparams/',\n",
    "    project_name=\"nn-tp1\",\n",
    ")\n",
    "\n",
    "# Summary\n",
    "tuner_softmax_adam.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 33s]\n",
      "val_accuracy: 0.8248999714851379\n",
      "\n",
      "Best val_accuracy So Far: 0.8540999889373779\n",
      "Total elapsed time: 00h 20m 51s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in testing/softmax/adam//hparams/nn-tp1\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0072 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: True\n",
      "lr: 0.0009281211218873026\n",
      "beta_1: 0.95\n",
      "beta_2: 0.9390000000000001\n",
      "loss: mean_squared_logarithmic_error\n",
      "initial_weights: glorot_normal\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0068\n",
      "Score: 0.8540999889373779\n",
      "\n",
      "Trial 0050 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: False\n",
      "lr: 0.002623763922379883\n",
      "beta_1: 0.85\n",
      "beta_2: 0.9790000000000001\n",
      "loss: cosine_similarity\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0049\n",
      "Score: 0.8529000282287598\n",
      "\n",
      "Trial 0068 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: True\n",
      "lr: 0.0009281211218873026\n",
      "beta_1: 0.95\n",
      "beta_2: 0.9390000000000001\n",
      "loss: mean_squared_logarithmic_error\n",
      "initial_weights: glorot_normal\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0065\n",
      "Score: 0.8518999814987183\n",
      "\n",
      "Trial 0083 summary\n",
      "Hyperparameters:\n",
      "dropout: True\n",
      "batch_normalization: False\n",
      "with_initial_weights: True\n",
      "lr: 0.003605420810659384\n",
      "beta_1: 0.9299999999999999\n",
      "beta_2: 0.9590000000000001\n",
      "loss: mean_squared_error\n",
      "dropout_rate: 0.1\n",
      "initial_weights: random_normal_1e-3\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0074\n",
      "Score: 0.8518000245094299\n",
      "\n",
      "Trial 0049 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: False\n",
      "lr: 0.002623763922379883\n",
      "beta_1: 0.85\n",
      "beta_2: 0.9790000000000001\n",
      "loss: cosine_similarity\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0045\n",
      "Score: 0.8514999747276306\n",
      "\n",
      "Trial 0067 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: False\n",
      "lr: 0.000580266006484294\n",
      "beta_1: 0.87\n",
      "beta_2: 0.9590000000000001\n",
      "loss: cosine_similarity\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0055\n",
      "Score: 0.8514000177383423\n",
      "\n",
      "Trial 0073 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: False\n",
      "lr: 0.000580266006484294\n",
      "beta_1: 0.87\n",
      "beta_2: 0.9590000000000001\n",
      "loss: cosine_similarity\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0067\n",
      "Score: 0.8514000177383423\n",
      "\n",
      "Trial 0051 summary\n",
      "Hyperparameters:\n",
      "dropout: True\n",
      "batch_normalization: False\n",
      "with_initial_weights: False\n",
      "lr: 0.0030063293553329266\n",
      "beta_1: 0.85\n",
      "beta_2: 0.9390000000000001\n",
      "loss: log_cosh\n",
      "dropout_rate: 0.1\n",
      "tuner/epochs: 50\n",
      "tuner/initial_epoch: 17\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0047\n",
      "Score: 0.8507000207901001\n",
      "\n",
      "Trial 0036 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: False\n",
      "lr: 0.0016395772926195201\n",
      "beta_1: 0.97\n",
      "beta_2: 0.9390000000000001\n",
      "loss: log_cosh\n",
      "tuner/epochs: 6\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 3\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0004\n",
      "Score: 0.8496000170707703\n",
      "\n",
      "Trial 0047 summary\n",
      "Hyperparameters:\n",
      "dropout: True\n",
      "batch_normalization: False\n",
      "with_initial_weights: False\n",
      "lr: 0.0030063293553329266\n",
      "beta_1: 0.85\n",
      "beta_2: 0.9390000000000001\n",
      "loss: log_cosh\n",
      "dropout_rate: 0.1\n",
      "tuner/epochs: 17\n",
      "tuner/initial_epoch: 6\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0039\n",
      "Score: 0.8490999937057495\n"
     ]
    }
   ],
   "source": [
    "# Testing (batch_size is fixed, we test it by changing it manually)\n",
    "tuner_softmax_adam.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 32, callbacks=softback_adam_callbacks)\n",
    "tuner_softmax_adam.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos entonces que el mejor resultado (accuracy = 0.8541) lo obtuvimos con:\n",
    "- Pesos iniciales utilizando una distribución glorot_normal\n",
    "- Learning rate $\\approx$ 0.00093\n",
    "- Beta 1 = 0.95\n",
    "- Beta 2 = 0.94\n",
    "\n",
    "(Los resultados se encuentran en testing/softmax/adam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to build the models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.2, parent_name=\"dropout\", parent_values=[True])))\n",
    "\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    # For MLP we add more Dense layers before the Softmax one. We test with 1, 2 and 3 layers raging from 32 neurons to 512\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "      model.add(\n",
    "        layers.Dense(\n",
    "          units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, sampling=\"log\", step=2),\n",
    "          activation=hp.Choice(\"activation\", [\"relu\", \"tanh\", \"sigmoid\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"exponential\"]), # Plus, we test different activation functions for those Dense layers\n",
    "          )\n",
    "      )\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\", step=10)\n",
    "\n",
    "    optimizer_list = hp.Choice(\"optimizer\", [\"adam\", \"nadam\", \"sgd\", \"rmsprop\"])\n",
    "    if(optimizer_list == \"adam\" or optimizer_list == \"nadam\"):\n",
    "        beta_1 = hp.Float(\"beta_1\", min_value=0.87, max_value=0.99, step=0.04, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        beta_2 = hp.Float(\"beta_2\", min_value=0.939, max_value=0.999, step=0.03, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "    elif(optimizer_list == \"sgd\"):\n",
    "        momentum_1 = hp.Float(\"momentum_1\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"sgd\"])\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=momentum_1)\n",
    "    elif(optimizer_list == \"rmsprop\"):\n",
    "        momentum_2 = hp.Float(\"momentum_2\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        rho = hp.Float(\"rho\", min_value=0.9, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, rho=rho, momentum=momentum_2)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de este test se pueden encontrar en la carpeta testing/mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 9\n",
      "dropout (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "batch_normalization (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 2, 'sampling': 'log'}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid', 'elu', 'selu', 'softplus', 'softsign', 'exponential'], 'ordered': False}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': 10, 'sampling': 'log'}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'nadam', 'sgd', 'rmsprop'], 'ordered': False}\n",
      "beta_1 (Float)\n",
      "{'default': 0.87, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'optimizer', 'values': ['adam', 'nadam']}}], 'min_value': 0.87, 'max_value': 0.99, 'step': 0.04, 'sampling': 'linear'}\n",
      "beta_2 (Float)\n",
      "{'default': 0.939, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'optimizer', 'values': ['adam', 'nadam']}}], 'min_value': 0.939, 'max_value': 0.999, 'step': 0.03, 'sampling': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 14:45:21.067534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build the hyperparameter tuner\n",
    "tuner_mlp = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model_mlp,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    overwrite=True,\n",
    "    directory=log_dir + '/hparams',\n",
    "    project_name=\"nn-tp1\",\n",
    ")\n",
    "\n",
    "# Summary\n",
    "tuner_mlp.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing (batch_size is fixed, we will test changing it after we decide which model to use)\n",
    "tuner_mlp.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, callbacks=[early_stop_callback, tensorboard_callback])\n",
    "tuner_mlp.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos entonces que los mejores optimizadores para el modelo utilizando MLP son Adam y nADAM, con las siguientes características:\n",
    "- nAdam: sin dropout ni batch normalization, con una sola layer Dense de 256 con activación softplus, un learning rate de 0.001, beta 1 de 0.95 y beta 2 de 0.969 (val_accuracy = 0.8952). También tuvo buen rendimiento con elu con batch normalization, utilizando 3 capas Dense de 256, 128 y 32, con un learning rate de 0.001, beta 1 de 0.91 y beta 2 de 0.939 (val_accuracy = 0.8929).\n",
    "- Adam: sin dropout pero con batch normalization, con una sola layer Dense de 512 con activación softplus, un learning rate de 0.0001, beta 1 de 0.87 y beta 2 de 0.969 (val_accuracy = 0.8946). También tuvo buen rendimiento con elu, utilizando 3 capas Dense de 256, 32 y 512, con un learning rate de 0.001, beta 1 de 0.95 y beta 2 de 0.999 (val_accuracy = 0.8923).\n",
    "\n",
    "Elegimos entonces para MLP el optimizador nAdam, ya que presenta bajo condiciones similares un mejor rendimiento que Adam."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-depth Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el optimizador Nadam ya elegido, podemos hacer un tuning de sus parametros más preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
    "early_stop_callback_mlp_nadam = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log stats and metrics for TensorBoard\n",
    "log_dir_mlp_nadam = \"testing/mlp/nadam/\"\n",
    "tensorboard_callback_mlp_nadam = tf.keras.callbacks.TensorBoard(log_dir=log_dir_mlp_nadam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to save the weights of the best model\n",
    "checkpoint_filepath_mlp_nadam = log_dir_mlp_nadam + 'checkpoint/'\n",
    "model_checkpoint_callback_mlp_nadam = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_mlp_nadam,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_nadam_callbacks = [early_stop_callback_mlp_nadam, tensorboard_callback_mlp_nadam, model_checkpoint_callback_mlp_nadam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_mlp_nadam(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Test with and without dropout, with different rates (0.1, 0.3, 0.5)\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.1, parent_name=\"dropout\", parent_values=[True])))\n",
    "\n",
    "    # Test with and without batch normalization\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    # Test initial weights\n",
    "    if hp.Boolean(\"with_initial_weights\"):\n",
    "        initial_weights = hp.Choice(\"initial_weights\", [\"random_normal_1e-1\", \"random_normal_1e-3\", \"glorot_normal\", \"glorot_uniform\"], parent_name=\"with_initial_weights\", parent_values=[True])\n",
    "        if(initial_weights == \"random_normal_1e-1\"):\n",
    "            initial_weights = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
    "        elif(initial_weights == \"random_normal_1e-3\"):\n",
    "            initial_weights = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001)\n",
    "        elif(initial_weights == \"glorot_normal\"):\n",
    "            initial_weights = tf.keras.initializers.GlorotNormal()\n",
    "        elif(initial_weights == \"glorot_uniform\"):\n",
    "            initial_weights = tf.keras.initializers.GlorotUniform()\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, sampling=\"log\", step=2),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\", \"sigmoid\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"exponential\"]),\n",
    "                kernel_initializer=initial_weights\n",
    "                )\n",
    "            )\n",
    "        model.add(layers.Dense(num_classes, activation='softmax', kernel_initializer=initial_weights))\n",
    "    else:\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    # Test learning rates 0.0001, 0.001 and 0.01   \n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    # Test betas\n",
    "    beta_1 = hp.Float(\"beta_1\", min_value=0.85, max_value=0.99, step=0.02)\n",
    "    beta_2 = hp.Float(\"beta_2\", min_value=0.919, max_value=0.999, step=0.02)\n",
    "\n",
    "    optimizer = optimizers.Nadam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "\n",
    "    # Test loss functions\n",
    "    loss = hp.Choice(\"loss\", [\"categorical_crossentropy\", \"poisson\", \"kl_divergence\", \"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\", \"mean_squared_logarithmic_error\", \"cosine_similarity\", \"log_cosh\"])\n",
    "\n",
    "    metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",
    "\n",
    "    # The loss function is fixed, we will test changing it after we decide which model to use\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "dropout (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "batch_normalization (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "with_initial_weights (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "beta_1 (Float)\n",
      "{'default': 0.85, 'conditions': [], 'min_value': 0.85, 'max_value': 0.99, 'step': 0.02, 'sampling': 'linear'}\n",
      "beta_2 (Float)\n",
      "{'default': 0.919, 'conditions': [], 'min_value': 0.919, 'max_value': 0.999, 'step': 0.02, 'sampling': 'linear'}\n",
      "loss (Choice)\n",
      "{'default': 'categorical_crossentropy', 'conditions': [], 'values': ['categorical_crossentropy', 'poisson', 'kl_divergence', 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_logarithmic_error', 'cosine_similarity', 'log_cosh'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "# Build the hyperparameter tuner\n",
    "tuner_mlp_nadam = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model_mlp_nadam,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=200,\n",
    "    overwrite=True,\n",
    "    directory=log_dir_mlp_nadam + '/hparams/',\n",
    "    project_name=\"nn-tp1\",\n",
    ")\n",
    "\n",
    "# Summary\n",
    "tuner_mlp_nadam.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 00m 27s]\n",
      "val_accuracy: 0.8352000117301941\n",
      "\n",
      "Best val_accuracy So Far: 0.8981999754905701\n",
      "Total elapsed time: 01h 38m 42s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in testing/mlp/nadam//hparams/nn-tp1\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0208 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.0011208031806549457\n",
      "beta_1: 0.85\n",
      "beta_2: 0.9790000000000001\n",
      "loss: log_cosh\n",
      "initial_weights: random_normal_1e-3\n",
      "num_layers: 3\n",
      "units_0: 512\n",
      "activation: elu\n",
      "units_1: 64\n",
      "units_2: 64\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0203\n",
      "Score: 0.8981999754905701\n",
      "\n",
      "Trial 0203 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.0011208031806549457\n",
      "beta_1: 0.85\n",
      "beta_2: 0.9790000000000001\n",
      "loss: log_cosh\n",
      "initial_weights: random_normal_1e-3\n",
      "num_layers: 3\n",
      "units_0: 512\n",
      "activation: elu\n",
      "units_1: 64\n",
      "units_2: 64\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 23\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0190\n",
      "Score: 0.8962000012397766\n",
      "\n",
      "Trial 0144 summary\n",
      "Hyperparameters:\n",
      "dropout: True\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.00043521287221261765\n",
      "beta_1: 0.95\n",
      "beta_2: 0.9590000000000001\n",
      "loss: categorical_crossentropy\n",
      "initial_weights: glorot_normal\n",
      "num_layers: 2\n",
      "units_0: 64\n",
      "activation: selu\n",
      "dropout_rate: 0.1\n",
      "units_1: 256\n",
      "units_2: 128\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 23\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0138\n",
      "Score: 0.8949999809265137\n",
      "\n",
      "Trial 0190 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.0011208031806549457\n",
      "beta_1: 0.85\n",
      "beta_2: 0.9790000000000001\n",
      "loss: log_cosh\n",
      "initial_weights: random_normal_1e-3\n",
      "num_layers: 3\n",
      "units_0: 512\n",
      "activation: elu\n",
      "units_1: 64\n",
      "units_2: 64\n",
      "tuner/epochs: 23\n",
      "tuner/initial_epoch: 8\n",
      "tuner/bracket: 3\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0183\n",
      "Score: 0.8945000171661377\n",
      "\n",
      "Trial 0206 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: True\n",
      "lr: 0.00047588376222118143\n",
      "beta_1: 0.97\n",
      "beta_2: 0.919\n",
      "loss: log_cosh\n",
      "initial_weights: glorot_uniform\n",
      "num_layers: 1\n",
      "units_0: 512\n",
      "activation: selu\n",
      "units_1: 256\n",
      "units_2: 64\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 23\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0191\n",
      "Score: 0.8939999938011169\n",
      "\n",
      "Trial 0146 summary\n",
      "Hyperparameters:\n",
      "dropout: True\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.00043521287221261765\n",
      "beta_1: 0.95\n",
      "beta_2: 0.9590000000000001\n",
      "loss: categorical_crossentropy\n",
      "initial_weights: glorot_normal\n",
      "num_layers: 2\n",
      "units_0: 64\n",
      "activation: selu\n",
      "dropout_rate: 0.1\n",
      "units_1: 256\n",
      "units_2: 128\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0144\n",
      "Score: 0.8938000202178955\n",
      "\n",
      "Trial 0147 summary\n",
      "Hyperparameters:\n",
      "dropout: True\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.0005001113932035832\n",
      "beta_1: 0.89\n",
      "beta_2: 0.9390000000000001\n",
      "loss: mean_squared_error\n",
      "initial_weights: random_normal_1e-3\n",
      "num_layers: 1\n",
      "units_0: 256\n",
      "activation: tanh\n",
      "dropout_rate: 0.1\n",
      "units_1: 64\n",
      "units_2: 256\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 4\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 0143\n",
      "Score: 0.8935999870300293\n",
      "\n",
      "Trial 0245 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.00017241146658242794\n",
      "beta_1: 0.97\n",
      "beta_2: 0.9590000000000001\n",
      "loss: categorical_crossentropy\n",
      "initial_weights: glorot_normal\n",
      "num_layers: 1\n",
      "units_0: 256\n",
      "activation: selu\n",
      "units_1: 32\n",
      "units_2: 256\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0240\n",
      "Score: 0.8935999870300293\n",
      "\n",
      "Trial 0209 summary\n",
      "Hyperparameters:\n",
      "dropout: False\n",
      "batch_normalization: False\n",
      "with_initial_weights: True\n",
      "lr: 0.00047588376222118143\n",
      "beta_1: 0.97\n",
      "beta_2: 0.919\n",
      "loss: log_cosh\n",
      "initial_weights: glorot_uniform\n",
      "num_layers: 1\n",
      "units_0: 512\n",
      "activation: selu\n",
      "units_1: 256\n",
      "units_2: 64\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 3\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0206\n",
      "Score: 0.8931000232696533\n",
      "\n",
      "Trial 0143 summary\n",
      "Hyperparameters:\n",
      "dropout: True\n",
      "batch_normalization: True\n",
      "with_initial_weights: True\n",
      "lr: 0.0005001113932035832\n",
      "beta_1: 0.89\n",
      "beta_2: 0.9390000000000001\n",
      "loss: mean_squared_error\n",
      "initial_weights: random_normal_1e-3\n",
      "num_layers: 1\n",
      "units_0: 256\n",
      "activation: tanh\n",
      "dropout_rate: 0.1\n",
      "units_1: 64\n",
      "units_2: 256\n",
      "tuner/epochs: 67\n",
      "tuner/initial_epoch: 23\n",
      "tuner/bracket: 4\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 0132\n",
      "Score: 0.8927000164985657\n"
     ]
    }
   ],
   "source": [
    "# Testing (batch_size is fixed, we test it by changing it manually)\n",
    "tuner_mlp_nadam.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 32, callbacks=mlp_nadam_callbacks)\n",
    "tuner_mlp_nadam.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos entonces que el mejor resultado (accuracy = 0.8982) lo obtuvimos con:\n",
    "- Pesos iniciales utilizando una distribución random_normal con sigma 0.001\n",
    "- Learning rate $\\approx$ 0.00112\n",
    "- 3 layers de Dense (512 - 64 - 64) con activacion elu\n",
    "- Beta 1 = 0.85\n",
    "- Beta 2 = 0.98\n",
    "- Batch normalization, sin dropout\n",
    "- Loss: log_cosh\n",
    "\n",
    "(Los resultados se encuentran en testing/mlp/nadam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
