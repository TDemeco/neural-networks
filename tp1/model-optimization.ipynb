{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 - 22.45 Redes Neuronales - Regresion Lógistica y Lineal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lógistica - Busqueda de Modelos Óptimos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from os.path import exists\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow_addons as tfa\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import keras_tuner\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and load Fashion MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = np.max(train_X)\n",
    "train_X = train_X.astype('float32') / data_max\n",
    "test_X = test_X.astype('float32') / data_max\n",
    "np.max(train_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert it to categorical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(train_y) + 1\n",
    "train_y_cat = utils.to_categorical(train_y, num_classes)\n",
    "test_y_cat = utils.to_categorical(test_y, num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring preeliminary optimizer analysis parameters and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log stats and metrics for TensorBoard\n",
    "log_dir = \"logs/softmax\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Accuracy (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Loss (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_f1_score\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"F1 Score Macro (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_f1_score_micro\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"F1 Score Micro (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_recall\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Recall (val.)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"epoch_precision\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"Precision (val.)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop', 'adamw', 'nadam', 'adagrad', 'adadelta', 'ftrl', 'adamax', 'adafactor']))\n",
    "HP_LEARN_RATE = hp.HParam('learning_rate', hp.Discrete([0.0001, 0.001, 0.01, 0.1]))\n",
    "HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([ 0.9, 0.95, 0.99]))\n",
    "HP_NESTEROV = hp.HParam('nesterov', hp.Discrete([True, False]))\n",
    "HP_RHO = hp.HParam('rho', hp.Discrete([0.92, 0.95, 0.97]))\n",
    "HP_BETA_1 = hp.HParam('beta_1', hp.Discrete([0.86, 0.9, 0.94]))\n",
    "HP_BETA_2 = hp.HParam('beta_2', hp.Discrete([0.97, 0.99, 0.999]))\n",
    "HP_BETA_2_DECAY = hp.HParam('beta_2_decay', hp.Discrete([-0.9, -0.8, -0.7]))\n",
    "HP_WEIGHT_DECAY = hp.HParam('weight_decay', hp.Discrete([0.0001, 0.001, 0.004, 0.01]))\n",
    "HP_LEARN_RATE_POWER = hp.HParam('learning_rate_power', hp.Discrete([-0.8, -0.5, -0.0]))\n",
    "\n",
    "HPARAMS = [HP_OPTIMIZER, HP_LEARN_RATE, HP_MOMENTUM, HP_NESTEROV, HP_RHO, HP_BETA_1, HP_BETA_2, HP_BETA_2_DECAY, HP_WEIGHT_DECAY, HP_LEARN_RATE_POWER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=HPARAMS,\n",
    "    metrics=METRICS,\n",
    "  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preeliminary Optimizer Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizó una busqueda preeliminar de los optimizadores que mejor se comportaran, que a pesar de ser un análisis limitado permitió concentrarse en algunos optimizadores específicos para el análisis posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax function to get models for optimizer testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, run_dir):\n",
    "  softmax_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax),\n",
    "  ])\n",
    "\n",
    "  if(hparams[HP_OPTIMIZER] == 'sgd'):\n",
    "    optimizer = optimizers.SGD(learning_rate=hparams[HP_LEARN_RATE], momentum=hparams[HP_MOMENTUM], nesterov=hparams[HP_NESTEROV])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adam'):\n",
    "    optimizer = optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'rmsprop'):\n",
    "    optimizer = optimizers.RMSprop(learning_rate=hparams[HP_LEARN_RATE], rho=hparams[HP_RHO], momentum=hparams[HP_MOMENTUM])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adadelta'):\n",
    "    optimizer = optimizers.Adadelta(learning_rate=hparams[HP_LEARN_RATE], rho=hparams[HP_RHO])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adagrad'):\n",
    "    optimizer = optimizers.Adagrad(learning_rate=hparams[HP_LEARN_RATE])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'adamax'):\n",
    "    optimizer = optimizers.Adamax(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'nadam'):\n",
    "    optimizer = optimizers.Nadam(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
    "  elif(hparams[HP_OPTIMIZER] == 'ftrl'):\n",
    "    optimizer = optimizers.Ftrl(learning_rate=hparams[HP_LEARN_RATE], learning_rate_power=hparams[HP_LEARN_RATE_POWER])\n",
    "\n",
    "  softmax_model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",
    "  )\n",
    "\n",
    "  callbacks = [\n",
    "      early_stop_callback,\n",
    "      tf.keras.callbacks.TensorBoard(run_dir), # log metrics\n",
    "      hp.KerasCallback(run_dir, hparams),  # log hparams\n",
    "    ]\n",
    "\n",
    "  softmax_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs = 5, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for optimizer in HP_OPTIMIZER.domain.values:\n",
    "  for learning_rate in HP_LEARN_RATE.domain.values:\n",
    "    if(optimizer == 'adagrad'):\n",
    "      hparams = {\n",
    "        HP_OPTIMIZER: optimizer,\n",
    "        HP_LEARN_RATE: learning_rate,\n",
    "      }\n",
    "      run_name = \"/run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      train_test_model(hparams, log_dir + run_name)\n",
    "      session_num += 1\n",
    "    elif(optimizer == 'adadelta'):\n",
    "      for rho in HP_RHO.domain.values:\n",
    "        hparams = {\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "          HP_LEARN_RATE: learning_rate,\n",
    "          HP_RHO: rho,\n",
    "        }\n",
    "        run_name = \"/run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        train_test_model(hparams, log_dir + run_name)\n",
    "        session_num += 1\n",
    "    elif(optimizer == 'ftrl'):\n",
    "      for learning_rate_power in HP_LEARN_RATE_POWER.domain.values:\n",
    "        hparams = {\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "          HP_LEARN_RATE: learning_rate,\n",
    "          HP_LEARN_RATE_POWER: learning_rate_power,\n",
    "        }\n",
    "        run_name = \"/run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        train_test_model(hparams, log_dir + run_name)\n",
    "        session_num += 1\n",
    "    elif(optimizer == 'sgd' or optimizer == 'rmsprop'):\n",
    "      for momentum in HP_MOMENTUM.domain.values:\n",
    "        if(optimizer == 'sgd'):\n",
    "          for nesterov in HP_NESTEROV.domain.values:\n",
    "            hparams = {\n",
    "              HP_OPTIMIZER: optimizer,\n",
    "              HP_LEARN_RATE: learning_rate,\n",
    "              HP_MOMENTUM: momentum,\n",
    "              HP_NESTEROV: nesterov,\n",
    "            }\n",
    "            run_name = \"/run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            train_test_model(hparams, log_dir + run_name)\n",
    "            session_num += 1\n",
    "        else:\n",
    "          for rho in HP_RHO.domain.values:\n",
    "            hparams = {\n",
    "              HP_OPTIMIZER: optimizer,\n",
    "              HP_LEARN_RATE: learning_rate,\n",
    "              HP_MOMENTUM: momentum,\n",
    "              HP_RHO: rho,\n",
    "            }\n",
    "            run_name = \"/run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            train_test_model(hparams, log_dir + run_name)\n",
    "            session_num += 1\n",
    "    elif(optimizer == 'adam' or optimizer == 'adamax' or optimizer == 'nadam'):\n",
    "      for beta_1 in HP_BETA_1.domain.values:\n",
    "        for beta_2 in HP_BETA_2.domain.values:\n",
    "          hparams = {\n",
    "            HP_OPTIMIZER: optimizer,\n",
    "            HP_LEARN_RATE: learning_rate,\n",
    "            HP_BETA_1: beta_1,\n",
    "            HP_BETA_2: beta_2,\n",
    "          }\n",
    "          run_name = \"/run-%d\" % session_num\n",
    "          print('--- Starting trial: %s' % run_name)\n",
    "          print({h.name: hparams[h] for h in hparams})\n",
    "          train_test_model(hparams, log_dir + run_name)\n",
    "          session_num += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos entonces un análisis preeliminar de los distintos optimizadores disponibles, y observamos que los que mejor se comportan en terminos de maximizar la accuracy son SGD, RMSprop, Adam y nAdam.\n",
    "Estos resultados son claramente imperfectos ya que:\n",
    "- Fueron realizados a solo 5 epochs (lo que le da una ventaja a learning rates altos, y no todos los optimizadores se comportan bien con los mismos).\n",
    "- No se analizaron otras cuestiones como agregar capas de dropout, batch normalization o cambiar la función de costo, que podrían mejorar la performance de algunos optimizadores en específico.\n",
    "- Solo se realizó para SoftmaxReg, y otros optimizadores podrían ser mejor para MLP.\n",
    "Sin embargo, sirve como análisis preeliminar para observar cuales optimizadores se comportan bien en general para este problema y realizar un análisis más en profundidad de los mismos, para no requerir tanto tiempo de entrenamiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Hyperparameter tuning callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to log stats and metrics for TensorBoard\n",
    "log_dir = \"testing/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to save the weights of the best model\n",
    "checkpoint_filepath = log_dir + '/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to build the models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_softmax(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Test with and without dropout, with different rates (0.1, 0.3, 0.5)\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.2, parent_name=\"dropout\", parent_values=[True])))\n",
    "\n",
    "    # Test with and without batch normalization\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Test learning rates 0.0001, 0.001 and 0.01   \n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\", step=10)\n",
    "\n",
    "    # Test best optimizers from the preeliminary optimizer analysis\n",
    "    optimizer_list = hp.Choice(\"optimizer\", [\"adam\", \"nadam\", \"sgd\", \"rmsprop\"])\n",
    "    if(optimizer_list == \"adam\" or optimizer_list == \"nadam\"):\n",
    "        beta_1 = hp.Float(\"beta_1\", min_value=0.87, max_value=0.99, step=0.04, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        beta_2 = hp.Float(\"beta_2\", min_value=0.939, max_value=0.999, step=0.03, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "    elif(optimizer_list == \"sgd\"):\n",
    "        momentum_1 = hp.Float(\"momentum_1\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"sgd\"])\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=momentum_1)\n",
    "    elif(optimizer_list == \"rmsprop\"):\n",
    "        momentum_2 = hp.Float(\"momentum_2\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        rho = hp.Float(\"rho\", min_value=0.9, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, rho=rho, momentum=momentum_2)\n",
    "\n",
    "    # The loss function is fixed, we will test changing it after we decide which model to use\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de este test se pueden encontrar en la carpeta testing/softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the hyperparameter tuner\n",
    "tuner_softmax = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model_softmax,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    overwrite=True,\n",
    "    directory=log_dir + '/hparams',\n",
    "    project_name=\"nn-tp1\",\n",
    ")\n",
    "\n",
    "# Summary\n",
    "tuner_softmax.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing (batch_size is fixed, we will test changing it after we decide which model to use)\n",
    "tuner_softmax.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, callbacks=[early_stop_callback, tensorboard_callback])\n",
    "tuner_softmax.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que los mejores modelos para SoftmaxReg entonces resultan ser:\n",
    "- adam: sin dropout ni batch normalization, con learning rate de 0.001, beta 1 de 0.95 y beta 2 de 0.939\n",
    "- rmsprop: sin dropout ni batch normalization, con learning rate de 0.0001, momentum de 0.93 y rho de 0.93\n",
    "- sgd: sin dropout pero con batch normalization, con learning rate de 0.001, momentum de 0.93\n",
    "\n",
    "Por su rápida velocidad de convergencia, que limita la cantidad de tiempo gastado en entrenamiento, eligiremos el modelo Adam para SoftmaxReg."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to build the models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.2, parent_name=\"dropout\", parent_values=[True])))\n",
    "\n",
    "    if hp.Boolean(\"batch_normalization\"):\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    # For MLP we add more Dense layers before the Softmax one. We test with 1, 2 and 3 layers raging from 32 neurons to 512\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "      model.add(\n",
    "        layers.Dense(\n",
    "          units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, sampling=\"log\", step=2),\n",
    "          activation=hp.Choice(\"activation\", [\"relu\", \"tanh\", \"sigmoid\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"exponential\"]), # Plus, we test different activation functions for those Dense layers\n",
    "          )\n",
    "      )\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\", step=10)\n",
    "\n",
    "    optimizer_list = hp.Choice(\"optimizer\", [\"adam\", \"nadam\", \"sgd\", \"rmsprop\"])\n",
    "    if(optimizer_list == \"adam\" or optimizer_list == \"nadam\"):\n",
    "        beta_1 = hp.Float(\"beta_1\", min_value=0.87, max_value=0.99, step=0.04, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        beta_2 = hp.Float(\"beta_2\", min_value=0.939, max_value=0.999, step=0.03, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "    elif(optimizer_list == \"sgd\"):\n",
    "        momentum_1 = hp.Float(\"momentum_1\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"sgd\"])\n",
    "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=momentum_1)\n",
    "    elif(optimizer_list == \"rmsprop\"):\n",
    "        momentum_2 = hp.Float(\"momentum_2\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        rho = hp.Float(\"rho\", min_value=0.9, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, rho=rho, momentum=momentum_2)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de este test se pueden encontrar en la carpeta testing/mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the hyperparameter tuner\n",
    "tuner_mlp = keras_tuner.Hyperband(\n",
    "    hypermodel=build_model_mlp,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=50,\n",
    "    overwrite=True,\n",
    "    directory=log_dir + '/hparams',\n",
    "    project_name=\"nn-tp1\",\n",
    ")\n",
    "\n",
    "# Summary\n",
    "tuner_mlp.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing (batch_size is fixed, we will test changing it after we decide which model to use)\n",
    "tuner_mlp.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, callbacks=[early_stop_callback, tensorboard_callback])\n",
    "tuner_mlp.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos entonces que los mejores optimizadores para el modelo utilizando MLP son Adam y nADAM, con las siguientes características:\n",
    "- nAdam: sin dropout ni batch normalization, con una sola layer Dense de 256 con activación softplus, un learning rate de 0.001, beta 1 de 0.95 y beta 2 de 0.969 (val_accuracy = 0.8952). También tuvo buen rendimiento con elu con batch normalization, utilizando 3 capas Dense de 256, 128 y 32, con un learning rate de 0.001, beta 1 de 0.91 y beta 2 de 0.939 (val_accuracy = 0.8929).\n",
    "- Adam: sin dropout pero con batch normalization, con una sola layer Dense de 512 con activación softplus, un learning rate de 0.0001, beta 1 de 0.87 y beta 2 de 0.969 (val_accuracy = 0.8946). También tuvo buen rendimiento con elu, utilizando 3 capas Dense de 256, 32 y 512, con un learning rate de 0.001, beta 1 de 0.95 y beta 2 de 0.999 (val_accuracy = 0.8923).\n",
    "\n",
    "Elegimos entonces para MLP el optimizador nAdam, ya que presenta bajo condiciones similares un mejor rendimiento que Adam."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
