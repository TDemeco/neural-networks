{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TP1 - 22.45 Redes Neuronales - Regresión Logística y Lineal"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión Logística"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import required libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JxCoqneHzZWv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-22 10:05:01.934335: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-22 10:05:02.363356: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-05-22 10:05:03.183860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tobid/itba/neural-networks/tp1/env/lib\n",
            "2023-05-22 10:05:03.183923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tobid/itba/neural-networks/tp1/env/lib\n",
            "2023-05-22 10:05:03.183928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-05-22 10:05:04.219911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0e:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2023-05-22 10:05:04.253275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0e:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2023-05-22 10:05:04.253318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0e:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "/home/tobid/itba/neural-networks/tp1/env/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "from os.path import exists\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import utils\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import tensorflow_addons as tfa\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "import keras_tuner\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download and load Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV2lRjoRzhMp",
        "outputId": "09ef645e-5429-48bc-f417-675b9dc915cf"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_max = np.max(train_X)\n",
        "train_X = train_X.astype('float32') / data_max\n",
        "test_X = test_X.astype('float32') / data_max\n",
        "np.max(train_X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example: Show the first object of the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "h8Nl-PvPzjJT",
        "outputId": "38bf581a-4815-42df-9fe8-e791aca9f279"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f94bed4ad00>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(train_X[0,...], cmap=\"gray\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Show a few instances of each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(40, 40))  # width, height in inches\n",
        "\n",
        "# idx works on np.array and not lists.\n",
        "idx = np.argsort(train_y)\n",
        "\n",
        "train_X_sorted = np.array(train_X)[idx]\n",
        "train_y_sorted = np.array(train_y)[idx]\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in range(100):\n",
        "    count = int(np.floor(i / 10))\n",
        "    sub = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
        "    sub.imshow(train_X_sorted[i + count * 6000,:,:], interpolation='nearest', cmap='gray')\n",
        "    sub.set_title('Category: ' + str(train_y_sorted[i + count * 6000]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Look at the data distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique, counts = np.unique(train_y, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "counts = np.bincount(train_y)\n",
        "print(counts)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.bar(range(10), counts, width=0.8, align='center')\n",
        "ax.set(xticks=range(10), xlim=[-1, 10], title='Training data distribution')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique, counts = np.unique(test_y, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "counts = np.bincount(test_y)\n",
        "print(counts)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.bar(range(10), counts, width=0.8, align='center')\n",
        "ax.set(xticks=range(10), xlim=[-1, 10], title='Testing data distribution')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert the dataset from a vector form to a categorical distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = np.max(train_y) + 1\n",
        "train_y_cat = utils.to_categorical(train_y, num_classes)\n",
        "test_y_cat = utils.to_categorical(test_y, num_classes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuring the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs and metrics from TensorBoard\n",
        "log_dir = \"testing/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to save the weights of the best model\n",
        "checkpoint_filepath = log_dir + '/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model builder function (with Hyperparameter Tuning)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model_softmax(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.2, parent_name=\"dropout\", parent_values=[True])))\n",
        "\n",
        "    if hp.Boolean(\"batch_normalization\"):\n",
        "        model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "        \n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\", step=10)\n",
        "\n",
        "    optimizer_list = hp.Choice(\"optimizer\", [\"adam\", \"nadam\", \"sgd\", \"rmsprop\"])\n",
        "    if(optimizer_list == \"adam\" or optimizer_list == \"nadam\"):\n",
        "        beta_1 = hp.Float(\"beta_1\", min_value=0.87, max_value=0.99, step=0.04, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
        "        beta_2 = hp.Float(\"beta_2\", min_value=0.939, max_value=0.999, step=0.03, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
        "        optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
        "    elif(optimizer_list == \"sgd\"):\n",
        "        momentum_1 = hp.Float(\"momentum_1\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"sgd\"])\n",
        "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=momentum_1)\n",
        "    elif(optimizer_list == \"rmsprop\"):\n",
        "        momentum_2 = hp.Float(\"momentum_2\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
        "        rho = hp.Float(\"rho\", min_value=0.9, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
        "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, rho=rho, momentum=momentum_2)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model_mlp(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "        model.add(layers.Dropout(hp.Float(\"dropout_rate\", 0.1, 0.5, step=0.2, parent_name=\"dropout\", parent_values=[True])))\n",
        "\n",
        "    if hp.Boolean(\"batch_normalization\"):\n",
        "        model.add(layers.BatchNormalization())\n",
        "    \n",
        "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
        "      model.add(\n",
        "        layers.Dense(\n",
        "          units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, sampling=\"log\", step=2),\n",
        "          activation=hp.Choice(\"activation\", [\"relu\", \"tanh\", \"sigmoid\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"exponential\"]),\n",
        "          )\n",
        "      )\n",
        "\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "        \n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\", step=10)\n",
        "\n",
        "    optimizer_list = hp.Choice(\"optimizer\", [\"adam\", \"nadam\", \"sgd\", \"rmsprop\"])\n",
        "    if(optimizer_list == \"adam\" or optimizer_list == \"nadam\"):\n",
        "        beta_1 = hp.Float(\"beta_1\", min_value=0.87, max_value=0.99, step=0.04, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
        "        beta_2 = hp.Float(\"beta_2\", min_value=0.939, max_value=0.999, step=0.03, parent_name=\"optimizer\", parent_values=[\"adam\", \"nadam\"])\n",
        "        optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
        "    elif(optimizer_list == \"sgd\"):\n",
        "        momentum_1 = hp.Float(\"momentum_1\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"sgd\"])\n",
        "        optimizer = optimizers.SGD(learning_rate=learning_rate, momentum=momentum_1)\n",
        "    elif(optimizer_list == \"rmsprop\"):\n",
        "        momentum_2 = hp.Float(\"momentum_2\", min_value=0.90, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
        "        rho = hp.Float(\"rho\", min_value=0.9, max_value=0.99, step=0.03, parent_name=\"optimizer\", parent_values=[\"rmsprop\"])\n",
        "        optimizer = optimizers.RMSprop(learning_rate=learning_rate, rho=rho, momentum=momentum_2)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Testing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resultados en la carpeta testing/softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 6\n",
            "dropout (Boolean)\n",
            "{'default': False, 'conditions': []}\n",
            "batch_normalization (Boolean)\n",
            "{'default': False, 'conditions': []}\n",
            "lr (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': 10, 'sampling': 'log'}\n",
            "optimizer (Choice)\n",
            "{'default': 'adam', 'conditions': [], 'values': ['adam', 'nadam', 'sgd', 'rmsprop'], 'ordered': False}\n",
            "beta_1 (Float)\n",
            "{'default': 0.87, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'optimizer', 'values': ['adam', 'nadam']}}], 'min_value': 0.87, 'max_value': 0.99, 'step': 0.04, 'sampling': 'linear'}\n",
            "beta_2 (Float)\n",
            "{'default': 0.939, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'optimizer', 'values': ['adam', 'nadam']}}], 'min_value': 0.939, 'max_value': 0.999, 'step': 0.03, 'sampling': 'linear'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-22 10:05:57.661025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "tuner_softmax = keras_tuner.Hyperband(\n",
        "    hypermodel=build_model_softmax,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=50,\n",
        "    overwrite=True,\n",
        "    directory=log_dir + '/hparams',\n",
        "    project_name=\"nn-tp1\",\n",
        ")\n",
        "\n",
        "tuner_softmax.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 90 Complete [00h 00m 12s]\n",
            "val_accuracy: 0.8269000053405762\n",
            "\n",
            "Best val_accuracy So Far: 0.8475000262260437\n",
            "Total elapsed time: 00h 11m 51s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in testing/20230522-100521/hparams/nn-tp1\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 0046 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: False\n",
            "lr: 0.001\n",
            "optimizer: adam\n",
            "beta_1: 0.95\n",
            "beta_2: 0.939\n",
            "tuner/epochs: 17\n",
            "tuner/initial_epoch: 6\n",
            "tuner/bracket: 3\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0035\n",
            "Score: 0.8475000262260437\n",
            "\n",
            "Trial 0050 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: False\n",
            "lr: 0.001\n",
            "optimizer: adam\n",
            "beta_1: 0.95\n",
            "beta_2: 0.939\n",
            "tuner/epochs: 50\n",
            "tuner/initial_epoch: 17\n",
            "tuner/bracket: 3\n",
            "tuner/round: 3\n",
            "tuner/trial_id: 0046\n",
            "Score: 0.847100019454956\n",
            "\n",
            "Trial 0048 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: False\n",
            "lr: 0.0001\n",
            "optimizer: rmsprop\n",
            "momentum_2: 0.93\n",
            "rho: 0.93\n",
            "tuner/epochs: 17\n",
            "tuner/initial_epoch: 6\n",
            "tuner/bracket: 3\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0037\n",
            "Score: 0.847000002861023\n",
            "\n",
            "Trial 0051 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: False\n",
            "lr: 0.0001\n",
            "optimizer: rmsprop\n",
            "momentum_2: 0.93\n",
            "rho: 0.93\n",
            "tuner/epochs: 50\n",
            "tuner/initial_epoch: 17\n",
            "tuner/bracket: 3\n",
            "tuner/round: 3\n",
            "tuner/trial_id: 0048\n",
            "Score: 0.847000002861023\n",
            "\n",
            "Trial 0087 summary\n",
            "Hyperparameters:\n",
            "dropout: True\n",
            "batch_normalization: False\n",
            "lr: 0.001\n",
            "optimizer: nadam\n",
            "beta_1: 0.91\n",
            "beta_2: 0.969\n",
            "dropout_rate: 0.1\n",
            "tuner/epochs: 50\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.8468000292778015\n",
            "\n",
            "Trial 0047 summary\n",
            "Hyperparameters:\n",
            "dropout: True\n",
            "batch_normalization: False\n",
            "lr: 0.0001\n",
            "optimizer: rmsprop\n",
            "dropout_rate: 0.1\n",
            "momentum_2: 0.96\n",
            "rho: 0.96\n",
            "tuner/epochs: 17\n",
            "tuner/initial_epoch: 6\n",
            "tuner/bracket: 3\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0042\n",
            "Score: 0.8467000126838684\n",
            "\n",
            "Trial 0073 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: True\n",
            "lr: 0.001\n",
            "optimizer: sgd\n",
            "momentum_1: 0.9\n",
            "tuner/epochs: 50\n",
            "tuner/initial_epoch: 17\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0068\n",
            "Score: 0.8463000059127808\n",
            "\n",
            "Trial 0088 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: True\n",
            "lr: 0.001\n",
            "optimizer: sgd\n",
            "momentum_1: 0.93\n",
            "tuner/epochs: 50\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.8463000059127808\n",
            "\n",
            "Trial 0067 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: True\n",
            "lr: 0.0001\n",
            "optimizer: rmsprop\n",
            "momentum_2: 0.93\n",
            "rho: 0.96\n",
            "tuner/epochs: 17\n",
            "tuner/initial_epoch: 6\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0066\n",
            "Score: 0.8458999991416931\n",
            "\n",
            "Trial 0035 summary\n",
            "Hyperparameters:\n",
            "dropout: False\n",
            "batch_normalization: False\n",
            "lr: 0.001\n",
            "optimizer: adam\n",
            "beta_1: 0.95\n",
            "beta_2: 0.939\n",
            "tuner/epochs: 6\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 3\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0019\n",
            "Score: 0.8457000255584717\n"
          ]
        }
      ],
      "source": [
        "tuner_softmax.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, callbacks=[early_stop_callback, tensorboard_callback])\n",
        "tuner_softmax.results_summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos que los mejores modelos para SoftmaxReg entonces resultan ser:\n",
        "- adam: sin dropout ni batch normalization, con learning rate de 0.001, beta 1 de 0.95 y beta 2 de 0.939\n",
        "- rmsprop: sin dropout ni batch normalization, con learning rate de 0.0001, momentum de 0.93 y rho de 0.93\n",
        "- sgd: sin dropout pero con batch normalization, con learning rate de 0.001, momentum de 0.93\n",
        "\n",
        "Por su rápida velocidad de convergencia, que limita la cantidad de tiempo gastado en entrenamiento, eligiremos el modelo Adam para SoftmaxReg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuner_mlp = keras_tuner.Hyperband(\n",
        "    hypermodel=build_model_mlp,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=50,\n",
        "    overwrite=True,\n",
        "    directory=log_dir + '/hparams',\n",
        "    project_name=\"nn-tp1\",\n",
        ")\n",
        "\n",
        "tuner_softmax.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuner_mlp.search(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, callbacks=[early_stop_callback, tensorboard_callback])\n",
        "tuner_mlp.results_summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos que los mejores modelos para MLP entonces resultan ser:\n",
        "\n",
        "Por X elegimos Y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Preeliminary Optimizer Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resultados en la carpeta logs/softmax/optimizer-analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "METRICS = [\n",
        "    hp.Metric(\n",
        "        \"epoch_accuracy\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Accuracy (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_loss\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Loss (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_f1_score\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"F1 Score Macro (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_f1_score_micro\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"F1 Score Micro (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_recall\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Recall (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_precision\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Precision (val.)\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop', 'adamw', 'nadam', 'adagrad', 'adadelta', 'ftrl', 'adamax', 'adafactor']))\n",
        "HP_LEARN_RATE = hp.HParam('learning_rate', hp.Discrete([0.0001, 0.001, 0.01, 0.1]))\n",
        "HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([ 0.9, 0.95, 0.99]))\n",
        "HP_NESTEROV = hp.HParam('nesterov', hp.Discrete([True, False]))\n",
        "HP_RHO = hp.HParam('rho', hp.Discrete([0.92, 0.95, 0.97]))\n",
        "HP_BETA_1 = hp.HParam('beta_1', hp.Discrete([0.86, 0.9, 0.94]))\n",
        "HP_BETA_2 = hp.HParam('beta_2', hp.Discrete([0.97, 0.99, 0.999]))\n",
        "HP_BETA_2_DECAY = hp.HParam('beta_2_decay', hp.Discrete([-0.9, -0.8, -0.7]))\n",
        "HP_WEIGHT_DECAY = hp.HParam('weight_decay', hp.Discrete([0.0001, 0.001, 0.004, 0.01]))\n",
        "HP_LEARN_RATE_POWER = hp.HParam('learning_rate_power', hp.Discrete([-0.8, -0.5, -0.0]))\n",
        "\n",
        "HPARAMS = [HP_OPTIMIZER, HP_LEARN_RATE, HP_MOMENTUM, HP_NESTEROV, HP_RHO, HP_BETA_1, HP_BETA_2, HP_BETA_2_DECAY, HP_WEIGHT_DECAY, HP_LEARN_RATE_POWER]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_model(hparams, run_dir):\n",
        "  softmax_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax),\n",
        "  ])\n",
        "\n",
        "  if(hparams[HP_OPTIMIZER] == 'sgd'):\n",
        "    optimizer = optimizers.SGD(learning_rate=hparams[HP_LEARN_RATE], momentum=hparams[HP_MOMENTUM], nesterov=hparams[HP_NESTEROV])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'adam'):\n",
        "    optimizer = optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'rmsprop'):\n",
        "    optimizer = optimizers.RMSprop(learning_rate=hparams[HP_LEARN_RATE], rho=hparams[HP_RHO], momentum=hparams[HP_MOMENTUM])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'adadelta'):\n",
        "    optimizer = optimizers.Adadelta(learning_rate=hparams[HP_LEARN_RATE], rho=hparams[HP_RHO])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'adagrad'):\n",
        "    optimizer = optimizers.Adagrad(learning_rate=hparams[HP_LEARN_RATE])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'adamax'):\n",
        "    optimizer = optimizers.Adamax(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'nadam'):\n",
        "    optimizer = optimizers.Nadam(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'ftrl'):\n",
        "    optimizer = optimizers.Ftrl(learning_rate=hparams[HP_LEARN_RATE], learning_rate_power=hparams[HP_LEARN_RATE_POWER])\n",
        "\n",
        "  softmax_model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",
        "  )\n",
        "\n",
        "  callbacks = [\n",
        "      early_stop_callback,\n",
        "      tf.keras.callbacks.TensorBoard(run_dir),# log metrics\n",
        "      hp.KerasCallback(run_dir, hparams),  # log hparams\n",
        "    ]\n",
        "\n",
        "  softmax_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs = 5, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.summary.create_file_writer(log_dir).as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=HPARAMS,\n",
        "    metrics=METRICS,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session_num = 0\n",
        "\n",
        "for optimizer in HP_OPTIMIZER.domain.values:\n",
        "  for learning_rate in HP_LEARN_RATE.domain.values:\n",
        "    if(optimizer == 'adagrad'):\n",
        "      hparams = {\n",
        "        HP_OPTIMIZER: optimizer,\n",
        "        HP_LEARN_RATE: learning_rate,\n",
        "      }\n",
        "      run_name = \"/run-%d\" % session_num\n",
        "      print('--- Starting trial: %s' % run_name)\n",
        "      print({h.name: hparams[h] for h in hparams})\n",
        "      train_test_model(hparams, log_dir + run_name)\n",
        "      session_num += 1\n",
        "    elif(optimizer == 'adadelta'):\n",
        "      for rho in HP_RHO.domain.values:\n",
        "        hparams = {\n",
        "          HP_OPTIMIZER: optimizer,\n",
        "          HP_LEARN_RATE: learning_rate,\n",
        "          HP_RHO: rho,\n",
        "        }\n",
        "        run_name = \"/run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        train_test_model(hparams, log_dir + run_name)\n",
        "        session_num += 1\n",
        "    elif(optimizer == 'ftrl'):\n",
        "      for learning_rate_power in HP_LEARN_RATE_POWER.domain.values:\n",
        "        hparams = {\n",
        "          HP_OPTIMIZER: optimizer,\n",
        "          HP_LEARN_RATE: learning_rate,\n",
        "          HP_LEARN_RATE_POWER: learning_rate_power,\n",
        "        }\n",
        "        run_name = \"/run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        train_test_model(hparams, log_dir + run_name)\n",
        "        session_num += 1\n",
        "    elif(optimizer == 'sgd' or optimizer == 'rmsprop'):\n",
        "      for momentum in HP_MOMENTUM.domain.values:\n",
        "        if(optimizer == 'sgd'):\n",
        "          for nesterov in HP_NESTEROV.domain.values:\n",
        "            hparams = {\n",
        "              HP_OPTIMIZER: optimizer,\n",
        "              HP_LEARN_RATE: learning_rate,\n",
        "              HP_MOMENTUM: momentum,\n",
        "              HP_NESTEROV: nesterov,\n",
        "            }\n",
        "            run_name = \"/run-%d\" % session_num\n",
        "            print('--- Starting trial: %s' % run_name)\n",
        "            print({h.name: hparams[h] for h in hparams})\n",
        "            train_test_model(hparams, log_dir + run_name)\n",
        "            session_num += 1\n",
        "        else:\n",
        "          for rho in HP_RHO.domain.values:\n",
        "            hparams = {\n",
        "              HP_OPTIMIZER: optimizer,\n",
        "              HP_LEARN_RATE: learning_rate,\n",
        "              HP_MOMENTUM: momentum,\n",
        "              HP_RHO: rho,\n",
        "            }\n",
        "            run_name = \"/run-%d\" % session_num\n",
        "            print('--- Starting trial: %s' % run_name)\n",
        "            print({h.name: hparams[h] for h in hparams})\n",
        "            train_test_model(hparams, log_dir + run_name)\n",
        "            session_num += 1\n",
        "    elif(optimizer == 'adam' or optimizer == 'adamax' or optimizer == 'nadam'):\n",
        "      for beta_1 in HP_BETA_1.domain.values:\n",
        "        for beta_2 in HP_BETA_2.domain.values:\n",
        "          hparams = {\n",
        "            HP_OPTIMIZER: optimizer,\n",
        "            HP_LEARN_RATE: learning_rate,\n",
        "            HP_BETA_1: beta_1,\n",
        "            HP_BETA_2: beta_2,\n",
        "          }\n",
        "          run_name = \"/run-%d\" % session_num\n",
        "          print('--- Starting trial: %s' % run_name)\n",
        "          print({h.name: hparams[h] for h in hparams})\n",
        "          train_test_model(hparams, log_dir + run_name)\n",
        "          session_num += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir $log_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos entonces un análisis preeliminar de los distintos optimizadores disponibles, y observamos que los que mejor se comportan en terminos de maximizar la accuracy son SGD, RMSprop, Adam y Nadam.\n",
        "Estos resultados son claramente imperfectos ya que:\n",
        "- Fueron realizados a solo 5 epochs (lo que le da una ventaja a learning rates altos, y no todos los optimizadores se comportan bien con los mismos).\n",
        "- No se analizaron otras cuestiones como agregar capas de dropout, batch normalization o cambiar la función de costo, que podrían mejorar la performance de algunos optimizadores en específico.\n",
        "Sin embargo, sirve como análisis preeliminar para observar cuales optimizadores se comportan bien en general para este problema y realizar un análisis más en profundidad de los mismos, para no requerir tanto tiempo de entrenamiento."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### First Hyperparameter Tuning with Best Optimizers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Buscamos obtener los mejores hiperparámetros para los optimizadores que mejor se comportaron en el análisis preeliminar, para luego realizar un análisis más en profundidad de los mismos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "METRICS = [\n",
        "    hp.Metric(\n",
        "        \"epoch_accuracy\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Accuracy (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_loss\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Loss (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_f1_score\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"F1 Score Macro (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_f1_score_micro\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"F1 Score Micro (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_recall\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Recall (val.)\",\n",
        "    ),\n",
        "    hp.Metric(\n",
        "        \"epoch_precision\",\n",
        "        group=\"validation\",\n",
        "        display_name=\"Precision (val.)\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# HP_BATCH_RATE = hp.HParam('batch_size', hp.Discrete([32, 64, 128, 256]))\n",
        "# HP_LOSSES = hp.HParam('loss', hp.Discrete(['categorical_crossentropy', 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_logarithmic_error', 'cosine_similarity', 'log_cosh']))\n",
        "# HP_ACTIVATIONS = hp.HParam('activation', hp.Discrete(['relu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential']))\n",
        "# HP_DENSE_LAYERS = hp.HParam('dense_layers', hp.Discrete([1, 2, 3, 4]))\n",
        "# HP_DENSE_SIZE = hp.HParam('dense_size', hp.Discrete([32, 64, 128, 256, 512]))\n",
        "# HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.0, 0.1, 0.2, 0.3, 0.4, 0.5]))\n",
        "# HP_BATCH_NORMALIZATION = hp.HParam('batch_normalization', hp.Discrete([True, False]))\n",
        "# HP_WEIGHT_INITIAL = hp.HParam('weight_initialization', hp.Discrete(['glorot_uniform', 'glorot_normal', 'random_normal_std_1', 'random_normal_std_1e-3']))\n",
        "\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['sgd', 'adam', 'nadam']))\n",
        "HP_LEARN_RATE = hp.HParam('learning_rate', hp.Discrete([0.0001, 0.001, 0.01]))\n",
        "HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([ 0.9, 0.95, 0.99]))\n",
        "HP_NESTEROV = hp.HParam('nesterov', hp.Discrete([True, False]))\n",
        "HP_BETA_1 = hp.HParam('beta_1', hp.Discrete([0.86, 0.9, 0.94]))\n",
        "HP_BETA_2 = hp.HParam('beta_2', hp.Discrete([0.97, 0.999]))\n",
        "\n",
        "HPARAMS = [HP_LEARN_RATE, HP_MOMENTUM, HP_NESTEROV, HP_BETA_1, HP_BETA_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to save the weights of the best model\n",
        "checkpoint_filepath = '/tmp/checkpoint/softmax'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs and metrics from TensorBoard\n",
        "log_dir = \"logs/softmax/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_model(hparams, run_dir):\n",
        "  softmax_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "    tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax),\n",
        "  ])\n",
        "  \n",
        "\n",
        "  if(hparams[HP_OPTIMIZER] == 'sgd'):\n",
        "    optimizer = optimizers.SGD(learning_rate=hparams[HP_LEARN_RATE], momentum=hparams[HP_MOMENTUM], nesterov=hparams[HP_NESTEROV])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'adam'):\n",
        "    optimizer = optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
        "  elif(hparams[HP_OPTIMIZER] == 'nadam'):\n",
        "    optimizer = optimizers.Nadam(learning_rate=hparams[HP_LEARN_RATE], beta_1=hparams[HP_BETA_1], beta_2=hparams[HP_BETA_2])\n",
        "  \n",
        "  softmax_model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n",
        "  )\n",
        "\n",
        "  callbacks = [\n",
        "      early_stop_callback,\n",
        "      tf.keras.callbacks.TensorBoard(run_dir),# log metrics\n",
        "      hp.KerasCallback(run_dir, hparams),  # log hparams\n",
        "    ]\n",
        "\n",
        "  softmax_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs = 100, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-22 07:37:00.432862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "with tf.summary.create_file_writer(log_dir).as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=HPARAMS,\n",
        "    metrics=METRICS,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting trial: /run-0\n",
            "{'optimizer': 'adam', 'learning_rate': 0.0001, 'beta_1': 0.86, 'beta_2': 0.97}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 1.1357 - accuracy: 0.6423 - f1_score: 0.6197 - f1_score_micro: 0.6423 - precision: 0.8967 - recall: 0.3269 - val_loss: 0.7920 - val_accuracy: 0.7221 - val_f1_score: 0.7046 - val_f1_score_micro: 0.7221 - val_precision: 0.8637 - val_recall: 0.5511\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.7006 - accuracy: 0.7625 - f1_score: 0.7533 - f1_score_micro: 0.7625 - precision: 0.8705 - recall: 0.6143 - val_loss: 0.6657 - val_accuracy: 0.7658 - val_f1_score: 0.7552 - val_f1_score_micro: 0.7658 - val_precision: 0.8630 - val_recall: 0.6494\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.6155 - accuracy: 0.7936 - f1_score: 0.7880 - f1_score_micro: 0.7936 - precision: 0.8737 - recall: 0.6796 - val_loss: 0.6117 - val_accuracy: 0.7915 - val_f1_score: 0.7874 - val_f1_score_micro: 0.7915 - val_precision: 0.8662 - val_recall: 0.6860\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5727 - accuracy: 0.8077 - f1_score: 0.8033 - f1_score_micro: 0.8077 - precision: 0.8773 - recall: 0.7107 - val_loss: 0.5796 - val_accuracy: 0.8037 - val_f1_score: 0.7998 - val_f1_score_micro: 0.8037 - val_precision: 0.8714 - val_recall: 0.7107\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5449 - accuracy: 0.8166 - f1_score: 0.8132 - f1_score_micro: 0.8167 - precision: 0.8799 - recall: 0.7315 - val_loss: 0.5593 - val_accuracy: 0.8105 - val_f1_score: 0.8079 - val_f1_score_micro: 0.8105 - val_precision: 0.8747 - val_recall: 0.7297\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5254 - accuracy: 0.8231 - f1_score: 0.8202 - f1_score_micro: 0.8231 - precision: 0.8818 - recall: 0.7456 - val_loss: 0.5435 - val_accuracy: 0.8164 - val_f1_score: 0.8122 - val_f1_score_micro: 0.8164 - val_precision: 0.8744 - val_recall: 0.7419\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5108 - accuracy: 0.8288 - f1_score: 0.8261 - f1_score_micro: 0.8288 - precision: 0.8835 - recall: 0.7563 - val_loss: 0.5326 - val_accuracy: 0.8204 - val_f1_score: 0.8189 - val_f1_score_micro: 0.8204 - val_precision: 0.8748 - val_recall: 0.7500\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4990 - accuracy: 0.8325 - f1_score: 0.8302 - f1_score_micro: 0.8325 - precision: 0.8850 - recall: 0.7639 - val_loss: 0.5237 - val_accuracy: 0.8201 - val_f1_score: 0.8170 - val_f1_score_micro: 0.8201 - val_precision: 0.8734 - val_recall: 0.7574\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4895 - accuracy: 0.8352 - f1_score: 0.8330 - f1_score_micro: 0.8352 - precision: 0.8862 - recall: 0.7718 - val_loss: 0.5154 - val_accuracy: 0.8263 - val_f1_score: 0.8249 - val_f1_score_micro: 0.8263 - val_precision: 0.8778 - val_recall: 0.7632\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4813 - accuracy: 0.8381 - f1_score: 0.8360 - f1_score_micro: 0.8380 - precision: 0.8869 - recall: 0.7775 - val_loss: 0.5090 - val_accuracy: 0.8262 - val_f1_score: 0.8246 - val_f1_score_micro: 0.8262 - val_precision: 0.8761 - val_recall: 0.7691\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4746 - accuracy: 0.8407 - f1_score: 0.8389 - f1_score_micro: 0.8407 - precision: 0.8882 - recall: 0.7825 - val_loss: 0.5030 - val_accuracy: 0.8277 - val_f1_score: 0.8258 - val_f1_score_micro: 0.8277 - val_precision: 0.8771 - val_recall: 0.7715\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4687 - accuracy: 0.8425 - f1_score: 0.8408 - f1_score_micro: 0.8425 - precision: 0.8888 - recall: 0.7867 - val_loss: 0.4981 - val_accuracy: 0.8311 - val_f1_score: 0.8295 - val_f1_score_micro: 0.8311 - val_precision: 0.8787 - val_recall: 0.7747\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4633 - accuracy: 0.8441 - f1_score: 0.8424 - f1_score_micro: 0.8441 - precision: 0.8894 - recall: 0.7902 - val_loss: 0.4943 - val_accuracy: 0.8322 - val_f1_score: 0.8298 - val_f1_score_micro: 0.8322 - val_precision: 0.8799 - val_recall: 0.7796\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4589 - accuracy: 0.8456 - f1_score: 0.8439 - f1_score_micro: 0.8456 - precision: 0.8897 - recall: 0.7935 - val_loss: 0.4909 - val_accuracy: 0.8323 - val_f1_score: 0.8314 - val_f1_score_micro: 0.8323 - val_precision: 0.8803 - val_recall: 0.7824\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4548 - accuracy: 0.8467 - f1_score: 0.8452 - f1_score_micro: 0.8467 - precision: 0.8897 - recall: 0.7956 - val_loss: 0.4870 - val_accuracy: 0.8346 - val_f1_score: 0.8330 - val_f1_score_micro: 0.8346 - val_precision: 0.8801 - val_recall: 0.7830\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4510 - accuracy: 0.8479 - f1_score: 0.8465 - f1_score_micro: 0.8479 - precision: 0.8907 - recall: 0.7988 - val_loss: 0.4842 - val_accuracy: 0.8348 - val_f1_score: 0.8327 - val_f1_score_micro: 0.8348 - val_precision: 0.8794 - val_recall: 0.7868\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4476 - accuracy: 0.8486 - f1_score: 0.8472 - f1_score_micro: 0.8486 - precision: 0.8919 - recall: 0.8003 - val_loss: 0.4820 - val_accuracy: 0.8341 - val_f1_score: 0.8325 - val_f1_score_micro: 0.8341 - val_precision: 0.8805 - val_recall: 0.7876\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4446 - accuracy: 0.8500 - f1_score: 0.8486 - f1_score_micro: 0.8500 - precision: 0.8909 - recall: 0.8019 - val_loss: 0.4804 - val_accuracy: 0.8354 - val_f1_score: 0.8331 - val_f1_score_micro: 0.8354 - val_precision: 0.8790 - val_recall: 0.7893\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4415 - accuracy: 0.8503 - f1_score: 0.8489 - f1_score_micro: 0.8503 - precision: 0.8911 - recall: 0.8037 - val_loss: 0.4773 - val_accuracy: 0.8365 - val_f1_score: 0.8357 - val_f1_score_micro: 0.8365 - val_precision: 0.8831 - val_recall: 0.7907\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4391 - accuracy: 0.8513 - f1_score: 0.8500 - f1_score_micro: 0.8513 - precision: 0.8918 - recall: 0.8058 - val_loss: 0.4745 - val_accuracy: 0.8368 - val_f1_score: 0.8353 - val_f1_score_micro: 0.8368 - val_precision: 0.8811 - val_recall: 0.7927\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4365 - accuracy: 0.8530 - f1_score: 0.8518 - f1_score_micro: 0.8530 - precision: 0.8922 - recall: 0.8072 - val_loss: 0.4728 - val_accuracy: 0.8374 - val_f1_score: 0.8360 - val_f1_score_micro: 0.8374 - val_precision: 0.8831 - val_recall: 0.7926\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4343 - accuracy: 0.8535 - f1_score: 0.8523 - f1_score_micro: 0.8535 - precision: 0.8922 - recall: 0.8093 - val_loss: 0.4713 - val_accuracy: 0.8373 - val_f1_score: 0.8359 - val_f1_score_micro: 0.8373 - val_precision: 0.8819 - val_recall: 0.7952\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4320 - accuracy: 0.8534 - f1_score: 0.8522 - f1_score_micro: 0.8534 - precision: 0.8916 - recall: 0.8090 - val_loss: 0.4700 - val_accuracy: 0.8382 - val_f1_score: 0.8364 - val_f1_score_micro: 0.8382 - val_precision: 0.8803 - val_recall: 0.7963\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4303 - accuracy: 0.8541 - f1_score: 0.8529 - f1_score_micro: 0.8541 - precision: 0.8928 - recall: 0.8108 - val_loss: 0.4680 - val_accuracy: 0.8391 - val_f1_score: 0.8370 - val_f1_score_micro: 0.8391 - val_precision: 0.8830 - val_recall: 0.7974\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4284 - accuracy: 0.8547 - f1_score: 0.8535 - f1_score_micro: 0.8547 - precision: 0.8923 - recall: 0.8123 - val_loss: 0.4682 - val_accuracy: 0.8392 - val_f1_score: 0.8391 - val_f1_score_micro: 0.8392 - val_precision: 0.8809 - val_recall: 0.7968\n",
            "Epoch 26/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4268 - accuracy: 0.8551 - f1_score: 0.8540 - f1_score_micro: 0.8551 - precision: 0.8927 - recall: 0.8132 - val_loss: 0.4659 - val_accuracy: 0.8389 - val_f1_score: 0.8377 - val_f1_score_micro: 0.8389 - val_precision: 0.8813 - val_recall: 0.7984\n",
            "Epoch 27/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4250 - accuracy: 0.8560 - f1_score: 0.8549 - f1_score_micro: 0.8560 - precision: 0.8929 - recall: 0.8144 - val_loss: 0.4639 - val_accuracy: 0.8406 - val_f1_score: 0.8389 - val_f1_score_micro: 0.8406 - val_precision: 0.8821 - val_recall: 0.8009\n",
            "Epoch 28/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4234 - accuracy: 0.8566 - f1_score: 0.8556 - f1_score_micro: 0.8566 - precision: 0.8933 - recall: 0.8152 - val_loss: 0.4641 - val_accuracy: 0.8390 - val_f1_score: 0.8384 - val_f1_score_micro: 0.8390 - val_precision: 0.8823 - val_recall: 0.7993\n",
            "Epoch 29/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.8564 - f1_score: 0.8553 - f1_score_micro: 0.8564 - precision: 0.8933 - recall: 0.8160 - val_loss: 0.4619 - val_accuracy: 0.8403 - val_f1_score: 0.8394 - val_f1_score_micro: 0.8403 - val_precision: 0.8828 - val_recall: 0.8010\n",
            "Epoch 30/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8572 - f1_score: 0.8561 - f1_score_micro: 0.8572 - precision: 0.8931 - recall: 0.8171 - val_loss: 0.4613 - val_accuracy: 0.8402 - val_f1_score: 0.8387 - val_f1_score_micro: 0.8402 - val_precision: 0.8811 - val_recall: 0.8026\n",
            "Epoch 31/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4192 - accuracy: 0.8572 - f1_score: 0.8561 - f1_score_micro: 0.8572 - precision: 0.8937 - recall: 0.8178 - val_loss: 0.4607 - val_accuracy: 0.8404 - val_f1_score: 0.8401 - val_f1_score_micro: 0.8404 - val_precision: 0.8821 - val_recall: 0.8014\n",
            "Epoch 32/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4181 - accuracy: 0.8579 - f1_score: 0.8569 - f1_score_micro: 0.8579 - precision: 0.8939 - recall: 0.8187 - val_loss: 0.4595 - val_accuracy: 0.8411 - val_f1_score: 0.8405 - val_f1_score_micro: 0.8411 - val_precision: 0.8825 - val_recall: 0.8022\n",
            "Epoch 33/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4168 - accuracy: 0.8582 - f1_score: 0.8572 - f1_score_micro: 0.8582 - precision: 0.8938 - recall: 0.8192 - val_loss: 0.4592 - val_accuracy: 0.8402 - val_f1_score: 0.8397 - val_f1_score_micro: 0.8402 - val_precision: 0.8811 - val_recall: 0.8026\n",
            "Epoch 34/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4157 - accuracy: 0.8580 - f1_score: 0.8570 - f1_score_micro: 0.8580 - precision: 0.8938 - recall: 0.8204 - val_loss: 0.4586 - val_accuracy: 0.8417 - val_f1_score: 0.8408 - val_f1_score_micro: 0.8417 - val_precision: 0.8803 - val_recall: 0.8050\n",
            "Epoch 35/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4144 - accuracy: 0.8592 - f1_score: 0.8582 - f1_score_micro: 0.8592 - precision: 0.8941 - recall: 0.8206 - val_loss: 0.4571 - val_accuracy: 0.8434 - val_f1_score: 0.8417 - val_f1_score_micro: 0.8434 - val_precision: 0.8796 - val_recall: 0.8047\n",
            "Epoch 36/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4135 - accuracy: 0.8594 - f1_score: 0.8583 - f1_score_micro: 0.8594 - precision: 0.8949 - recall: 0.8210 - val_loss: 0.4559 - val_accuracy: 0.8425 - val_f1_score: 0.8410 - val_f1_score_micro: 0.8425 - val_precision: 0.8804 - val_recall: 0.8066\n",
            "Epoch 37/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4125 - accuracy: 0.8595 - f1_score: 0.8586 - f1_score_micro: 0.8595 - precision: 0.8942 - recall: 0.8216 - val_loss: 0.4557 - val_accuracy: 0.8422 - val_f1_score: 0.8409 - val_f1_score_micro: 0.8422 - val_precision: 0.8799 - val_recall: 0.8035\n",
            "Epoch 38/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4114 - accuracy: 0.8598 - f1_score: 0.8588 - f1_score_micro: 0.8598 - precision: 0.8942 - recall: 0.8223 - val_loss: 0.4567 - val_accuracy: 0.8436 - val_f1_score: 0.8431 - val_f1_score_micro: 0.8436 - val_precision: 0.8808 - val_recall: 0.8049\n",
            "Epoch 39/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4107 - accuracy: 0.8601 - f1_score: 0.8592 - f1_score_micro: 0.8601 - precision: 0.8945 - recall: 0.8227 - val_loss: 0.4545 - val_accuracy: 0.8418 - val_f1_score: 0.8406 - val_f1_score_micro: 0.8418 - val_precision: 0.8806 - val_recall: 0.8044\n",
            "Epoch 40/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4098 - accuracy: 0.8605 - f1_score: 0.8596 - f1_score_micro: 0.8605 - precision: 0.8944 - recall: 0.8239 - val_loss: 0.4534 - val_accuracy: 0.8435 - val_f1_score: 0.8428 - val_f1_score_micro: 0.8435 - val_precision: 0.8811 - val_recall: 0.8068\n",
            "Epoch 41/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4087 - accuracy: 0.8608 - f1_score: 0.8599 - f1_score_micro: 0.8608 - precision: 0.8947 - recall: 0.8243 - val_loss: 0.4536 - val_accuracy: 0.8425 - val_f1_score: 0.8415 - val_f1_score_micro: 0.8425 - val_precision: 0.8807 - val_recall: 0.8087\n",
            "Epoch 42/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4079 - accuracy: 0.8613 - f1_score: 0.8604 - f1_score_micro: 0.8613 - precision: 0.8948 - recall: 0.8249 - val_loss: 0.4522 - val_accuracy: 0.8439 - val_f1_score: 0.8425 - val_f1_score_micro: 0.8439 - val_precision: 0.8806 - val_recall: 0.8092\n",
            "Epoch 43/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4072 - accuracy: 0.8617 - f1_score: 0.8608 - f1_score_micro: 0.8617 - precision: 0.8950 - recall: 0.8251 - val_loss: 0.4519 - val_accuracy: 0.8431 - val_f1_score: 0.8417 - val_f1_score_micro: 0.8431 - val_precision: 0.8806 - val_recall: 0.8074\n",
            "Epoch 44/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4065 - accuracy: 0.8625 - f1_score: 0.8616 - f1_score_micro: 0.8625 - precision: 0.8955 - recall: 0.8257 - val_loss: 0.4522 - val_accuracy: 0.8425 - val_f1_score: 0.8418 - val_f1_score_micro: 0.8425 - val_precision: 0.8803 - val_recall: 0.8059\n",
            "Epoch 45/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4057 - accuracy: 0.8622 - f1_score: 0.8613 - f1_score_micro: 0.8622 - precision: 0.8956 - recall: 0.8257 - val_loss: 0.4511 - val_accuracy: 0.8439 - val_f1_score: 0.8430 - val_f1_score_micro: 0.8439 - val_precision: 0.8814 - val_recall: 0.8078\n",
            "Epoch 46/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4049 - accuracy: 0.8618 - f1_score: 0.8608 - f1_score_micro: 0.8618 - precision: 0.8946 - recall: 0.8267 - val_loss: 0.4502 - val_accuracy: 0.8449 - val_f1_score: 0.8437 - val_f1_score_micro: 0.8449 - val_precision: 0.8797 - val_recall: 0.8092\n",
            "Epoch 47/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4041 - accuracy: 0.8625 - f1_score: 0.8616 - f1_score_micro: 0.8625 - precision: 0.8954 - recall: 0.8265 - val_loss: 0.4508 - val_accuracy: 0.8448 - val_f1_score: 0.8431 - val_f1_score_micro: 0.8448 - val_precision: 0.8792 - val_recall: 0.8082\n",
            "Epoch 48/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4036 - accuracy: 0.8625 - f1_score: 0.8615 - f1_score_micro: 0.8625 - precision: 0.8954 - recall: 0.8271 - val_loss: 0.4499 - val_accuracy: 0.8441 - val_f1_score: 0.8437 - val_f1_score_micro: 0.8441 - val_precision: 0.8800 - val_recall: 0.8074\n",
            "Epoch 49/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4028 - accuracy: 0.8625 - f1_score: 0.8616 - f1_score_micro: 0.8625 - precision: 0.8956 - recall: 0.8273 - val_loss: 0.4505 - val_accuracy: 0.8432 - val_f1_score: 0.8429 - val_f1_score_micro: 0.8432 - val_precision: 0.8786 - val_recall: 0.8090\n",
            "Epoch 50/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4022 - accuracy: 0.8625 - f1_score: 0.8616 - f1_score_micro: 0.8625 - precision: 0.8952 - recall: 0.8276 - val_loss: 0.4491 - val_accuracy: 0.8442 - val_f1_score: 0.8429 - val_f1_score_micro: 0.8442 - val_precision: 0.8796 - val_recall: 0.8103\n",
            "Epoch 51/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4016 - accuracy: 0.8637 - f1_score: 0.8628 - f1_score_micro: 0.8637 - precision: 0.8958 - recall: 0.8283 - val_loss: 0.4483 - val_accuracy: 0.8460 - val_f1_score: 0.8448 - val_f1_score_micro: 0.8460 - val_precision: 0.8815 - val_recall: 0.8103\n",
            "Epoch 52/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8639 - f1_score: 0.8630 - f1_score_micro: 0.8639 - precision: 0.8961 - recall: 0.8288 - val_loss: 0.4485 - val_accuracy: 0.8443 - val_f1_score: 0.8439 - val_f1_score_micro: 0.8443 - val_precision: 0.8791 - val_recall: 0.8083\n",
            "Epoch 53/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4002 - accuracy: 0.8635 - f1_score: 0.8627 - f1_score_micro: 0.8635 - precision: 0.8957 - recall: 0.8287 - val_loss: 0.4479 - val_accuracy: 0.8445 - val_f1_score: 0.8439 - val_f1_score_micro: 0.8445 - val_precision: 0.8801 - val_recall: 0.8103\n",
            "Epoch 54/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.8638 - f1_score: 0.8629 - f1_score_micro: 0.8638 - precision: 0.8963 - recall: 0.8294 - val_loss: 0.4478 - val_accuracy: 0.8443 - val_f1_score: 0.8435 - val_f1_score_micro: 0.8443 - val_precision: 0.8803 - val_recall: 0.8086\n",
            "Epoch 55/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3992 - accuracy: 0.8640 - f1_score: 0.8632 - f1_score_micro: 0.8640 - precision: 0.8960 - recall: 0.8302 - val_loss: 0.4477 - val_accuracy: 0.8451 - val_f1_score: 0.8435 - val_f1_score_micro: 0.8451 - val_precision: 0.8796 - val_recall: 0.8125\n",
            "Epoch 56/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3985 - accuracy: 0.8647 - f1_score: 0.8639 - f1_score_micro: 0.8647 - precision: 0.8962 - recall: 0.8301 - val_loss: 0.4466 - val_accuracy: 0.8457 - val_f1_score: 0.8447 - val_f1_score_micro: 0.8457 - val_precision: 0.8807 - val_recall: 0.8113\n",
            "--- Starting trial: /run-1\n",
            "{'optimizer': 'adam', 'learning_rate': 0.0001, 'beta_1': 0.86, 'beta_2': 0.999}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 1ms/step - loss: 1.1741 - accuracy: 0.6459 - f1_score: 0.6246 - f1_score_micro: 0.6459 - precision: 0.9083 - recall: 0.2920 - val_loss: 0.8446 - val_accuracy: 0.7237 - val_f1_score: 0.7108 - val_f1_score_micro: 0.7237 - val_precision: 0.8880 - val_recall: 0.5011\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.7455 - accuracy: 0.7609 - f1_score: 0.7533 - f1_score_micro: 0.7609 - precision: 0.8816 - recall: 0.5767 - val_loss: 0.7009 - val_accuracy: 0.7677 - val_f1_score: 0.7628 - val_f1_score_micro: 0.7677 - val_precision: 0.8755 - val_recall: 0.6168\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.6470 - accuracy: 0.7910 - f1_score: 0.7865 - f1_score_micro: 0.7910 - precision: 0.8784 - recall: 0.6558 - val_loss: 0.6366 - val_accuracy: 0.7875 - val_f1_score: 0.7843 - val_f1_score_micro: 0.7875 - val_precision: 0.8725 - val_recall: 0.6680\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5952 - accuracy: 0.8059 - f1_score: 0.8023 - f1_score_micro: 0.8059 - precision: 0.8808 - recall: 0.6964 - val_loss: 0.5977 - val_accuracy: 0.7999 - val_f1_score: 0.7967 - val_f1_score_micro: 0.7999 - val_precision: 0.8764 - val_recall: 0.7011\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5619 - accuracy: 0.8152 - f1_score: 0.8122 - f1_score_micro: 0.8152 - precision: 0.8824 - recall: 0.7215 - val_loss: 0.5728 - val_accuracy: 0.8090 - val_f1_score: 0.8072 - val_f1_score_micro: 0.8090 - val_precision: 0.8769 - val_recall: 0.7190\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5387 - accuracy: 0.8224 - f1_score: 0.8198 - f1_score_micro: 0.8224 - precision: 0.8836 - recall: 0.7384 - val_loss: 0.5533 - val_accuracy: 0.8154 - val_f1_score: 0.8129 - val_f1_score_micro: 0.8154 - val_precision: 0.8786 - val_recall: 0.7356\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5209 - accuracy: 0.8280 - f1_score: 0.8256 - f1_score_micro: 0.8280 - precision: 0.8862 - recall: 0.7505 - val_loss: 0.5397 - val_accuracy: 0.8176 - val_f1_score: 0.8145 - val_f1_score_micro: 0.8176 - val_precision: 0.8775 - val_recall: 0.7478\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5074 - accuracy: 0.8317 - f1_score: 0.8295 - f1_score_micro: 0.8317 - precision: 0.8858 - recall: 0.7587 - val_loss: 0.5284 - val_accuracy: 0.8205 - val_f1_score: 0.8178 - val_f1_score_micro: 0.8205 - val_precision: 0.8778 - val_recall: 0.7518\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4964 - accuracy: 0.8350 - f1_score: 0.8331 - f1_score_micro: 0.8350 - precision: 0.8875 - recall: 0.7663 - val_loss: 0.5196 - val_accuracy: 0.8233 - val_f1_score: 0.8205 - val_f1_score_micro: 0.8233 - val_precision: 0.8775 - val_recall: 0.7603\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4870 - accuracy: 0.8375 - f1_score: 0.8356 - f1_score_micro: 0.8375 - precision: 0.8881 - recall: 0.7728 - val_loss: 0.5124 - val_accuracy: 0.8254 - val_f1_score: 0.8240 - val_f1_score_micro: 0.8254 - val_precision: 0.8772 - val_recall: 0.7642\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4793 - accuracy: 0.8403 - f1_score: 0.8386 - f1_score_micro: 0.8403 - precision: 0.8886 - recall: 0.7786 - val_loss: 0.5069 - val_accuracy: 0.8272 - val_f1_score: 0.8235 - val_f1_score_micro: 0.8272 - val_precision: 0.8766 - val_recall: 0.7686\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4729 - accuracy: 0.8424 - f1_score: 0.8408 - f1_score_micro: 0.8424 - precision: 0.8891 - recall: 0.7833 - val_loss: 0.5006 - val_accuracy: 0.8300 - val_f1_score: 0.8285 - val_f1_score_micro: 0.8300 - val_precision: 0.8808 - val_recall: 0.7702\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4669 - accuracy: 0.8443 - f1_score: 0.8428 - f1_score_micro: 0.8443 - precision: 0.8904 - recall: 0.7868 - val_loss: 0.4957 - val_accuracy: 0.8311 - val_f1_score: 0.8293 - val_f1_score_micro: 0.8311 - val_precision: 0.8782 - val_recall: 0.7745\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4618 - accuracy: 0.8455 - f1_score: 0.8440 - f1_score_micro: 0.8455 - precision: 0.8911 - recall: 0.7903 - val_loss: 0.4919 - val_accuracy: 0.8312 - val_f1_score: 0.8291 - val_f1_score_micro: 0.8312 - val_precision: 0.8799 - val_recall: 0.7764\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4573 - accuracy: 0.8473 - f1_score: 0.8458 - f1_score_micro: 0.8473 - precision: 0.8913 - recall: 0.7936 - val_loss: 0.4877 - val_accuracy: 0.8327 - val_f1_score: 0.8311 - val_f1_score_micro: 0.8327 - val_precision: 0.8812 - val_recall: 0.7801\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4532 - accuracy: 0.8480 - f1_score: 0.8466 - f1_score_micro: 0.8480 - precision: 0.8918 - recall: 0.7958 - val_loss: 0.4863 - val_accuracy: 0.8334 - val_f1_score: 0.8329 - val_f1_score_micro: 0.8334 - val_precision: 0.8786 - val_recall: 0.7829\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4495 - accuracy: 0.8488 - f1_score: 0.8476 - f1_score_micro: 0.8488 - precision: 0.8926 - recall: 0.7980 - val_loss: 0.4820 - val_accuracy: 0.8350 - val_f1_score: 0.8331 - val_f1_score_micro: 0.8350 - val_precision: 0.8786 - val_recall: 0.7841\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4460 - accuracy: 0.8497 - f1_score: 0.8485 - f1_score_micro: 0.8497 - precision: 0.8921 - recall: 0.8003 - val_loss: 0.4791 - val_accuracy: 0.8359 - val_f1_score: 0.8343 - val_f1_score_micro: 0.8359 - val_precision: 0.8800 - val_recall: 0.7865\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4430 - accuracy: 0.8505 - f1_score: 0.8493 - f1_score_micro: 0.8505 - precision: 0.8927 - recall: 0.8022 - val_loss: 0.4766 - val_accuracy: 0.8350 - val_f1_score: 0.8328 - val_f1_score_micro: 0.8350 - val_precision: 0.8811 - val_recall: 0.7873\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4401 - accuracy: 0.8514 - f1_score: 0.8501 - f1_score_micro: 0.8514 - precision: 0.8927 - recall: 0.8034 - val_loss: 0.4753 - val_accuracy: 0.8363 - val_f1_score: 0.8356 - val_f1_score_micro: 0.8363 - val_precision: 0.8826 - val_recall: 0.7893\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4376 - accuracy: 0.8527 - f1_score: 0.8516 - f1_score_micro: 0.8527 - precision: 0.8936 - recall: 0.8049 - val_loss: 0.4729 - val_accuracy: 0.8358 - val_f1_score: 0.8350 - val_f1_score_micro: 0.8358 - val_precision: 0.8821 - val_recall: 0.7897\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4350 - accuracy: 0.8527 - f1_score: 0.8516 - f1_score_micro: 0.8527 - precision: 0.8942 - recall: 0.8069 - val_loss: 0.4721 - val_accuracy: 0.8362 - val_f1_score: 0.8344 - val_f1_score_micro: 0.8362 - val_precision: 0.8794 - val_recall: 0.7935\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4328 - accuracy: 0.8534 - f1_score: 0.8523 - f1_score_micro: 0.8534 - precision: 0.8942 - recall: 0.8087 - val_loss: 0.4692 - val_accuracy: 0.8375 - val_f1_score: 0.8363 - val_f1_score_micro: 0.8375 - val_precision: 0.8812 - val_recall: 0.7937\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4305 - accuracy: 0.8535 - f1_score: 0.8523 - f1_score_micro: 0.8535 - precision: 0.8942 - recall: 0.8094 - val_loss: 0.4685 - val_accuracy: 0.8363 - val_f1_score: 0.8355 - val_f1_score_micro: 0.8363 - val_precision: 0.8818 - val_recall: 0.7943\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4288 - accuracy: 0.8553 - f1_score: 0.8543 - f1_score_micro: 0.8553 - precision: 0.8943 - recall: 0.8105 - val_loss: 0.4673 - val_accuracy: 0.8386 - val_f1_score: 0.8369 - val_f1_score_micro: 0.8386 - val_precision: 0.8797 - val_recall: 0.7935\n",
            "Epoch 26/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4267 - accuracy: 0.8553 - f1_score: 0.8542 - f1_score_micro: 0.8553 - precision: 0.8947 - recall: 0.8121 - val_loss: 0.4650 - val_accuracy: 0.8379 - val_f1_score: 0.8365 - val_f1_score_micro: 0.8379 - val_precision: 0.8822 - val_recall: 0.7949\n",
            "Epoch 27/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4251 - accuracy: 0.8559 - f1_score: 0.8549 - f1_score_micro: 0.8559 - precision: 0.8943 - recall: 0.8126 - val_loss: 0.4631 - val_accuracy: 0.8386 - val_f1_score: 0.8373 - val_f1_score_micro: 0.8386 - val_precision: 0.8817 - val_recall: 0.7969\n",
            "Epoch 28/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4236 - accuracy: 0.8565 - f1_score: 0.8555 - f1_score_micro: 0.8565 - precision: 0.8948 - recall: 0.8133 - val_loss: 0.4618 - val_accuracy: 0.8404 - val_f1_score: 0.8387 - val_f1_score_micro: 0.8404 - val_precision: 0.8819 - val_recall: 0.7978\n",
            "Epoch 29/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4219 - accuracy: 0.8570 - f1_score: 0.8560 - f1_score_micro: 0.8570 - precision: 0.8952 - recall: 0.8146 - val_loss: 0.4610 - val_accuracy: 0.8393 - val_f1_score: 0.8379 - val_f1_score_micro: 0.8393 - val_precision: 0.8817 - val_recall: 0.7976\n",
            "Epoch 30/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8578 - f1_score: 0.8568 - f1_score_micro: 0.8578 - precision: 0.8955 - recall: 0.8157 - val_loss: 0.4598 - val_accuracy: 0.8405 - val_f1_score: 0.8396 - val_f1_score_micro: 0.8405 - val_precision: 0.8818 - val_recall: 0.7982\n",
            "Epoch 31/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4189 - accuracy: 0.8583 - f1_score: 0.8574 - f1_score_micro: 0.8583 - precision: 0.8953 - recall: 0.8163 - val_loss: 0.4600 - val_accuracy: 0.8409 - val_f1_score: 0.8383 - val_f1_score_micro: 0.8409 - val_precision: 0.8813 - val_recall: 0.7977\n",
            "Epoch 32/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4174 - accuracy: 0.8584 - f1_score: 0.8575 - f1_score_micro: 0.8584 - precision: 0.8952 - recall: 0.8167 - val_loss: 0.4580 - val_accuracy: 0.8403 - val_f1_score: 0.8392 - val_f1_score_micro: 0.8403 - val_precision: 0.8818 - val_recall: 0.7984\n",
            "Epoch 33/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4163 - accuracy: 0.8590 - f1_score: 0.8581 - f1_score_micro: 0.8590 - precision: 0.8959 - recall: 0.8182 - val_loss: 0.4590 - val_accuracy: 0.8402 - val_f1_score: 0.8392 - val_f1_score_micro: 0.8402 - val_precision: 0.8806 - val_recall: 0.8001\n",
            "Epoch 34/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4153 - accuracy: 0.8587 - f1_score: 0.8578 - f1_score_micro: 0.8587 - precision: 0.8951 - recall: 0.8178 - val_loss: 0.4567 - val_accuracy: 0.8405 - val_f1_score: 0.8385 - val_f1_score_micro: 0.8405 - val_precision: 0.8823 - val_recall: 0.8006\n",
            "Epoch 35/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4139 - accuracy: 0.8592 - f1_score: 0.8583 - f1_score_micro: 0.8592 - precision: 0.8954 - recall: 0.8194 - val_loss: 0.4564 - val_accuracy: 0.8397 - val_f1_score: 0.8387 - val_f1_score_micro: 0.8397 - val_precision: 0.8802 - val_recall: 0.8008\n",
            "Epoch 36/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4126 - accuracy: 0.8599 - f1_score: 0.8590 - f1_score_micro: 0.8599 - precision: 0.8956 - recall: 0.8199 - val_loss: 0.4548 - val_accuracy: 0.8425 - val_f1_score: 0.8411 - val_f1_score_micro: 0.8425 - val_precision: 0.8817 - val_recall: 0.8013\n",
            "Epoch 37/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4118 - accuracy: 0.8596 - f1_score: 0.8587 - f1_score_micro: 0.8596 - precision: 0.8957 - recall: 0.8204 - val_loss: 0.4541 - val_accuracy: 0.8416 - val_f1_score: 0.8405 - val_f1_score_micro: 0.8416 - val_precision: 0.8820 - val_recall: 0.8019\n",
            "Epoch 38/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4108 - accuracy: 0.8601 - f1_score: 0.8592 - f1_score_micro: 0.8601 - precision: 0.8962 - recall: 0.8207 - val_loss: 0.4533 - val_accuracy: 0.8404 - val_f1_score: 0.8391 - val_f1_score_micro: 0.8404 - val_precision: 0.8822 - val_recall: 0.8019\n",
            "Epoch 39/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4098 - accuracy: 0.8605 - f1_score: 0.8596 - f1_score_micro: 0.8605 - precision: 0.8956 - recall: 0.8210 - val_loss: 0.4531 - val_accuracy: 0.8409 - val_f1_score: 0.8402 - val_f1_score_micro: 0.8409 - val_precision: 0.8803 - val_recall: 0.8026\n",
            "Epoch 40/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4087 - accuracy: 0.8613 - f1_score: 0.8604 - f1_score_micro: 0.8613 - precision: 0.8965 - recall: 0.8227 - val_loss: 0.4536 - val_accuracy: 0.8404 - val_f1_score: 0.8398 - val_f1_score_micro: 0.8404 - val_precision: 0.8797 - val_recall: 0.8016\n",
            "Epoch 41/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4079 - accuracy: 0.8618 - f1_score: 0.8609 - f1_score_micro: 0.8618 - precision: 0.8965 - recall: 0.8221 - val_loss: 0.4514 - val_accuracy: 0.8414 - val_f1_score: 0.8397 - val_f1_score_micro: 0.8414 - val_precision: 0.8803 - val_recall: 0.8047\n",
            "--- Starting trial: /run-2\n",
            "{'optimizer': 'adam', 'learning_rate': 0.0001, 'beta_1': 0.9, 'beta_2': 0.97}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 1.1840 - accuracy: 0.6283 - f1_score: 0.6048 - f1_score_micro: 0.6283 - precision: 0.8880 - recall: 0.3101 - val_loss: 0.7983 - val_accuracy: 0.7288 - val_f1_score: 0.7147 - val_f1_score_micro: 0.7288 - val_precision: 0.8652 - val_recall: 0.5494\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.7063 - accuracy: 0.7612 - f1_score: 0.7521 - f1_score_micro: 0.7612 - precision: 0.8693 - recall: 0.6128 - val_loss: 0.6664 - val_accuracy: 0.7703 - val_f1_score: 0.7653 - val_f1_score_micro: 0.7703 - val_precision: 0.8692 - val_recall: 0.6468\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.6181 - accuracy: 0.7916 - f1_score: 0.7863 - f1_score_micro: 0.7916 - precision: 0.8753 - recall: 0.6776 - val_loss: 0.6104 - val_accuracy: 0.7882 - val_f1_score: 0.7826 - val_f1_score_micro: 0.7882 - val_precision: 0.8695 - val_recall: 0.6890\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5737 - accuracy: 0.8073 - f1_score: 0.8033 - f1_score_micro: 0.8073 - precision: 0.8793 - recall: 0.7122 - val_loss: 0.5795 - val_accuracy: 0.8031 - val_f1_score: 0.7994 - val_f1_score_micro: 0.8031 - val_precision: 0.8722 - val_recall: 0.7122\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5456 - accuracy: 0.8174 - f1_score: 0.8140 - f1_score_micro: 0.8174 - precision: 0.8806 - recall: 0.7326 - val_loss: 0.5574 - val_accuracy: 0.8115 - val_f1_score: 0.8079 - val_f1_score_micro: 0.8115 - val_precision: 0.8725 - val_recall: 0.7298\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5259 - accuracy: 0.8238 - f1_score: 0.8208 - f1_score_micro: 0.8238 - precision: 0.8828 - recall: 0.7466 - val_loss: 0.5436 - val_accuracy: 0.8166 - val_f1_score: 0.8147 - val_f1_score_micro: 0.8166 - val_precision: 0.8760 - val_recall: 0.7406\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5110 - accuracy: 0.8285 - f1_score: 0.8259 - f1_score_micro: 0.8285 - precision: 0.8843 - recall: 0.7567 - val_loss: 0.5308 - val_accuracy: 0.8203 - val_f1_score: 0.8176 - val_f1_score_micro: 0.8203 - val_precision: 0.8754 - val_recall: 0.7520\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4992 - accuracy: 0.8325 - f1_score: 0.8303 - f1_score_micro: 0.8325 - precision: 0.8854 - recall: 0.7644 - val_loss: 0.5219 - val_accuracy: 0.8220 - val_f1_score: 0.8196 - val_f1_score_micro: 0.8220 - val_precision: 0.8752 - val_recall: 0.7608\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4894 - accuracy: 0.8364 - f1_score: 0.8343 - f1_score_micro: 0.8364 - precision: 0.8873 - recall: 0.7717 - val_loss: 0.5139 - val_accuracy: 0.8279 - val_f1_score: 0.8259 - val_f1_score_micro: 0.8279 - val_precision: 0.8781 - val_recall: 0.7642\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4815 - accuracy: 0.8383 - f1_score: 0.8363 - f1_score_micro: 0.8383 - precision: 0.8871 - recall: 0.7778 - val_loss: 0.5071 - val_accuracy: 0.8283 - val_f1_score: 0.8267 - val_f1_score_micro: 0.8283 - val_precision: 0.8763 - val_recall: 0.7697\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4747 - accuracy: 0.8403 - f1_score: 0.8385 - f1_score_micro: 0.8403 - precision: 0.8882 - recall: 0.7826 - val_loss: 0.5017 - val_accuracy: 0.8283 - val_f1_score: 0.8259 - val_f1_score_micro: 0.8283 - val_precision: 0.8752 - val_recall: 0.7732\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4689 - accuracy: 0.8421 - f1_score: 0.8404 - f1_score_micro: 0.8421 - precision: 0.8886 - recall: 0.7856 - val_loss: 0.4975 - val_accuracy: 0.8311 - val_f1_score: 0.8303 - val_f1_score_micro: 0.8311 - val_precision: 0.8777 - val_recall: 0.7771\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4636 - accuracy: 0.8442 - f1_score: 0.8427 - f1_score_micro: 0.8442 - precision: 0.8890 - recall: 0.7901 - val_loss: 0.4923 - val_accuracy: 0.8308 - val_f1_score: 0.8287 - val_f1_score_micro: 0.8308 - val_precision: 0.8778 - val_recall: 0.7795\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4590 - accuracy: 0.8457 - f1_score: 0.8441 - f1_score_micro: 0.8457 - precision: 0.8893 - recall: 0.7930 - val_loss: 0.4889 - val_accuracy: 0.8323 - val_f1_score: 0.8309 - val_f1_score_micro: 0.8323 - val_precision: 0.8781 - val_recall: 0.7815\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4547 - accuracy: 0.8469 - f1_score: 0.8455 - f1_score_micro: 0.8469 - precision: 0.8900 - recall: 0.7959 - val_loss: 0.4864 - val_accuracy: 0.8318 - val_f1_score: 0.8304 - val_f1_score_micro: 0.8318 - val_precision: 0.8791 - val_recall: 0.7839\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4510 - accuracy: 0.8471 - f1_score: 0.8457 - f1_score_micro: 0.8471 - precision: 0.8898 - recall: 0.7971 - val_loss: 0.4829 - val_accuracy: 0.8340 - val_f1_score: 0.8328 - val_f1_score_micro: 0.8340 - val_precision: 0.8793 - val_recall: 0.7861\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4474 - accuracy: 0.8487 - f1_score: 0.8474 - f1_score_micro: 0.8487 - precision: 0.8906 - recall: 0.7997 - val_loss: 0.4795 - val_accuracy: 0.8355 - val_f1_score: 0.8341 - val_f1_score_micro: 0.8355 - val_precision: 0.8794 - val_recall: 0.7867\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4442 - accuracy: 0.8498 - f1_score: 0.8486 - f1_score_micro: 0.8498 - precision: 0.8913 - recall: 0.8021 - val_loss: 0.4773 - val_accuracy: 0.8347 - val_f1_score: 0.8333 - val_f1_score_micro: 0.8347 - val_precision: 0.8785 - val_recall: 0.7892\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4416 - accuracy: 0.8499 - f1_score: 0.8486 - f1_score_micro: 0.8499 - precision: 0.8915 - recall: 0.8041 - val_loss: 0.4751 - val_accuracy: 0.8363 - val_f1_score: 0.8352 - val_f1_score_micro: 0.8363 - val_precision: 0.8800 - val_recall: 0.7928\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4389 - accuracy: 0.8511 - f1_score: 0.8499 - f1_score_micro: 0.8511 - precision: 0.8915 - recall: 0.8053 - val_loss: 0.4731 - val_accuracy: 0.8364 - val_f1_score: 0.8349 - val_f1_score_micro: 0.8364 - val_precision: 0.8801 - val_recall: 0.7926\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4364 - accuracy: 0.8525 - f1_score: 0.8513 - f1_score_micro: 0.8525 - precision: 0.8917 - recall: 0.8071 - val_loss: 0.4726 - val_accuracy: 0.8365 - val_f1_score: 0.8348 - val_f1_score_micro: 0.8365 - val_precision: 0.8779 - val_recall: 0.7935\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4343 - accuracy: 0.8529 - f1_score: 0.8517 - f1_score_micro: 0.8529 - precision: 0.8925 - recall: 0.8088 - val_loss: 0.4722 - val_accuracy: 0.8363 - val_f1_score: 0.8364 - val_f1_score_micro: 0.8363 - val_precision: 0.8792 - val_recall: 0.7946\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4321 - accuracy: 0.8538 - f1_score: 0.8527 - f1_score_micro: 0.8538 - precision: 0.8926 - recall: 0.8106 - val_loss: 0.4679 - val_accuracy: 0.8381 - val_f1_score: 0.8364 - val_f1_score_micro: 0.8381 - val_precision: 0.8812 - val_recall: 0.7974\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4301 - accuracy: 0.8547 - f1_score: 0.8536 - f1_score_micro: 0.8547 - precision: 0.8935 - recall: 0.8116 - val_loss: 0.4669 - val_accuracy: 0.8367 - val_f1_score: 0.8353 - val_f1_score_micro: 0.8367 - val_precision: 0.8796 - val_recall: 0.7972\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4283 - accuracy: 0.8547 - f1_score: 0.8536 - f1_score_micro: 0.8547 - precision: 0.8927 - recall: 0.8125 - val_loss: 0.4652 - val_accuracy: 0.8384 - val_f1_score: 0.8378 - val_f1_score_micro: 0.8384 - val_precision: 0.8800 - val_recall: 0.7987\n",
            "Epoch 26/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4266 - accuracy: 0.8555 - f1_score: 0.8545 - f1_score_micro: 0.8555 - precision: 0.8939 - recall: 0.8130 - val_loss: 0.4638 - val_accuracy: 0.8391 - val_f1_score: 0.8374 - val_f1_score_micro: 0.8391 - val_precision: 0.8810 - val_recall: 0.7988\n",
            "Epoch 27/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4248 - accuracy: 0.8564 - f1_score: 0.8553 - f1_score_micro: 0.8564 - precision: 0.8934 - recall: 0.8151 - val_loss: 0.4633 - val_accuracy: 0.8393 - val_f1_score: 0.8381 - val_f1_score_micro: 0.8393 - val_precision: 0.8792 - val_recall: 0.8002\n",
            "Epoch 28/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4233 - accuracy: 0.8566 - f1_score: 0.8555 - f1_score_micro: 0.8566 - precision: 0.8936 - recall: 0.8148 - val_loss: 0.4628 - val_accuracy: 0.8387 - val_f1_score: 0.8378 - val_f1_score_micro: 0.8387 - val_precision: 0.8793 - val_recall: 0.8009\n",
            "Epoch 29/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4217 - accuracy: 0.8569 - f1_score: 0.8558 - f1_score_micro: 0.8569 - precision: 0.8946 - recall: 0.8161 - val_loss: 0.4606 - val_accuracy: 0.8405 - val_f1_score: 0.8387 - val_f1_score_micro: 0.8405 - val_precision: 0.8811 - val_recall: 0.8019\n",
            "Epoch 30/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8580 - f1_score: 0.8569 - f1_score_micro: 0.8580 - precision: 0.8940 - recall: 0.8165 - val_loss: 0.4606 - val_accuracy: 0.8389 - val_f1_score: 0.8384 - val_f1_score_micro: 0.8389 - val_precision: 0.8797 - val_recall: 0.8015\n",
            "Epoch 31/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4190 - accuracy: 0.8580 - f1_score: 0.8569 - f1_score_micro: 0.8580 - precision: 0.8938 - recall: 0.8177 - val_loss: 0.4595 - val_accuracy: 0.8402 - val_f1_score: 0.8393 - val_f1_score_micro: 0.8402 - val_precision: 0.8820 - val_recall: 0.8014\n",
            "Epoch 32/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4177 - accuracy: 0.8586 - f1_score: 0.8575 - f1_score_micro: 0.8586 - precision: 0.8941 - recall: 0.8181 - val_loss: 0.4594 - val_accuracy: 0.8384 - val_f1_score: 0.8385 - val_f1_score_micro: 0.8384 - val_precision: 0.8795 - val_recall: 0.8002\n",
            "Epoch 33/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4164 - accuracy: 0.8585 - f1_score: 0.8576 - f1_score_micro: 0.8585 - precision: 0.8943 - recall: 0.8196 - val_loss: 0.4585 - val_accuracy: 0.8407 - val_f1_score: 0.8406 - val_f1_score_micro: 0.8407 - val_precision: 0.8794 - val_recall: 0.8034\n",
            "Epoch 34/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4151 - accuracy: 0.8591 - f1_score: 0.8582 - f1_score_micro: 0.8591 - precision: 0.8950 - recall: 0.8202 - val_loss: 0.4568 - val_accuracy: 0.8417 - val_f1_score: 0.8399 - val_f1_score_micro: 0.8417 - val_precision: 0.8803 - val_recall: 0.8044\n",
            "Epoch 35/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4143 - accuracy: 0.8591 - f1_score: 0.8581 - f1_score_micro: 0.8591 - precision: 0.8950 - recall: 0.8207 - val_loss: 0.4553 - val_accuracy: 0.8410 - val_f1_score: 0.8398 - val_f1_score_micro: 0.8410 - val_precision: 0.8808 - val_recall: 0.8045\n",
            "Epoch 36/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4132 - accuracy: 0.8595 - f1_score: 0.8586 - f1_score_micro: 0.8595 - precision: 0.8947 - recall: 0.8206 - val_loss: 0.4548 - val_accuracy: 0.8413 - val_f1_score: 0.8397 - val_f1_score_micro: 0.8413 - val_precision: 0.8806 - val_recall: 0.8059\n",
            "Epoch 37/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4123 - accuracy: 0.8601 - f1_score: 0.8591 - f1_score_micro: 0.8601 - precision: 0.8943 - recall: 0.8219 - val_loss: 0.4541 - val_accuracy: 0.8425 - val_f1_score: 0.8410 - val_f1_score_micro: 0.8425 - val_precision: 0.8806 - val_recall: 0.8049\n",
            "Epoch 38/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4113 - accuracy: 0.8603 - f1_score: 0.8594 - f1_score_micro: 0.8603 - precision: 0.8943 - recall: 0.8220 - val_loss: 0.4535 - val_accuracy: 0.8433 - val_f1_score: 0.8422 - val_f1_score_micro: 0.8433 - val_precision: 0.8797 - val_recall: 0.8052\n",
            "Epoch 39/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4103 - accuracy: 0.8606 - f1_score: 0.8597 - f1_score_micro: 0.8606 - precision: 0.8947 - recall: 0.8231 - val_loss: 0.4527 - val_accuracy: 0.8423 - val_f1_score: 0.8409 - val_f1_score_micro: 0.8423 - val_precision: 0.8802 - val_recall: 0.8065\n",
            "Epoch 40/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4093 - accuracy: 0.8615 - f1_score: 0.8606 - f1_score_micro: 0.8615 - precision: 0.8946 - recall: 0.8232 - val_loss: 0.4530 - val_accuracy: 0.8414 - val_f1_score: 0.8393 - val_f1_score_micro: 0.8414 - val_precision: 0.8784 - val_recall: 0.8067\n",
            "Epoch 41/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4087 - accuracy: 0.8614 - f1_score: 0.8605 - f1_score_micro: 0.8614 - precision: 0.8953 - recall: 0.8237 - val_loss: 0.4518 - val_accuracy: 0.8427 - val_f1_score: 0.8419 - val_f1_score_micro: 0.8427 - val_precision: 0.8806 - val_recall: 0.8065\n",
            "Epoch 42/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4077 - accuracy: 0.8618 - f1_score: 0.8609 - f1_score_micro: 0.8618 - precision: 0.8954 - recall: 0.8242 - val_loss: 0.4518 - val_accuracy: 0.8430 - val_f1_score: 0.8419 - val_f1_score_micro: 0.8430 - val_precision: 0.8803 - val_recall: 0.8070\n",
            "Epoch 43/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4070 - accuracy: 0.8622 - f1_score: 0.8612 - f1_score_micro: 0.8622 - precision: 0.8960 - recall: 0.8250 - val_loss: 0.4512 - val_accuracy: 0.8429 - val_f1_score: 0.8423 - val_f1_score_micro: 0.8429 - val_precision: 0.8807 - val_recall: 0.8059\n",
            "--- Starting trial: /run-3\n",
            "{'optimizer': 'adam', 'learning_rate': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 1ms/step - loss: 1.1982 - accuracy: 0.6333 - f1_score: 0.6148 - f1_score_micro: 0.6333 - precision: 0.9067 - recall: 0.2915 - val_loss: 0.8482 - val_accuracy: 0.7186 - val_f1_score: 0.7049 - val_f1_score_micro: 0.7186 - val_precision: 0.8783 - val_recall: 0.5046\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.7509 - accuracy: 0.7525 - f1_score: 0.7430 - f1_score_micro: 0.7525 - precision: 0.8766 - recall: 0.5785 - val_loss: 0.7027 - val_accuracy: 0.7603 - val_f1_score: 0.7512 - val_f1_score_micro: 0.7603 - val_precision: 0.8638 - val_recall: 0.6188\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.6519 - accuracy: 0.7832 - f1_score: 0.7775 - f1_score_micro: 0.7832 - precision: 0.8755 - recall: 0.6540 - val_loss: 0.6376 - val_accuracy: 0.7832 - val_f1_score: 0.7783 - val_f1_score_micro: 0.7832 - val_precision: 0.8667 - val_recall: 0.6649\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5989 - accuracy: 0.8018 - f1_score: 0.7977 - f1_score_micro: 0.8018 - precision: 0.8788 - recall: 0.6926 - val_loss: 0.5994 - val_accuracy: 0.7943 - val_f1_score: 0.7901 - val_f1_score_micro: 0.7943 - val_precision: 0.8693 - val_recall: 0.6950\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5651 - accuracy: 0.8127 - f1_score: 0.8094 - f1_score_micro: 0.8127 - precision: 0.8811 - recall: 0.7178 - val_loss: 0.5750 - val_accuracy: 0.8046 - val_f1_score: 0.8037 - val_f1_score_micro: 0.8046 - val_precision: 0.8732 - val_recall: 0.7167\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5410 - accuracy: 0.8209 - f1_score: 0.8182 - f1_score_micro: 0.8209 - precision: 0.8831 - recall: 0.7349 - val_loss: 0.5546 - val_accuracy: 0.8118 - val_f1_score: 0.8097 - val_f1_score_micro: 0.8118 - val_precision: 0.8764 - val_recall: 0.7325\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5230 - accuracy: 0.8263 - f1_score: 0.8239 - f1_score_micro: 0.8263 - precision: 0.8847 - recall: 0.7477 - val_loss: 0.5403 - val_accuracy: 0.8171 - val_f1_score: 0.8151 - val_f1_score_micro: 0.8171 - val_precision: 0.8771 - val_recall: 0.7437\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5091 - accuracy: 0.8301 - f1_score: 0.8279 - f1_score_micro: 0.8301 - precision: 0.8850 - recall: 0.7566 - val_loss: 0.5284 - val_accuracy: 0.8210 - val_f1_score: 0.8180 - val_f1_score_micro: 0.8210 - val_precision: 0.8760 - val_recall: 0.7536\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4978 - accuracy: 0.8336 - f1_score: 0.8315 - f1_score_micro: 0.8336 - precision: 0.8874 - recall: 0.7644 - val_loss: 0.5195 - val_accuracy: 0.8226 - val_f1_score: 0.8207 - val_f1_score_micro: 0.8226 - val_precision: 0.8759 - val_recall: 0.7609\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4884 - accuracy: 0.8372 - f1_score: 0.8354 - f1_score_micro: 0.8372 - precision: 0.8881 - recall: 0.7713 - val_loss: 0.5121 - val_accuracy: 0.8253 - val_f1_score: 0.8234 - val_f1_score_micro: 0.8253 - val_precision: 0.8760 - val_recall: 0.7638\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4804 - accuracy: 0.8387 - f1_score: 0.8370 - f1_score_micro: 0.8387 - precision: 0.8888 - recall: 0.7771 - val_loss: 0.5056 - val_accuracy: 0.8264 - val_f1_score: 0.8249 - val_f1_score_micro: 0.8264 - val_precision: 0.8748 - val_recall: 0.7681\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4737 - accuracy: 0.8414 - f1_score: 0.8398 - f1_score_micro: 0.8414 - precision: 0.8896 - recall: 0.7811 - val_loss: 0.5022 - val_accuracy: 0.8264 - val_f1_score: 0.8242 - val_f1_score_micro: 0.8264 - val_precision: 0.8731 - val_recall: 0.7720\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4679 - accuracy: 0.8430 - f1_score: 0.8415 - f1_score_micro: 0.8430 - precision: 0.8904 - recall: 0.7866 - val_loss: 0.4965 - val_accuracy: 0.8275 - val_f1_score: 0.8250 - val_f1_score_micro: 0.8275 - val_precision: 0.8744 - val_recall: 0.7732\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4628 - accuracy: 0.8444 - f1_score: 0.8429 - f1_score_micro: 0.8444 - precision: 0.8913 - recall: 0.7900 - val_loss: 0.4917 - val_accuracy: 0.8308 - val_f1_score: 0.8294 - val_f1_score_micro: 0.8308 - val_precision: 0.8799 - val_recall: 0.7760\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4584 - accuracy: 0.8448 - f1_score: 0.8435 - f1_score_micro: 0.8448 - precision: 0.8909 - recall: 0.7925 - val_loss: 0.4883 - val_accuracy: 0.8295 - val_f1_score: 0.8268 - val_f1_score_micro: 0.8295 - val_precision: 0.8757 - val_recall: 0.7793\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4541 - accuracy: 0.8472 - f1_score: 0.8459 - f1_score_micro: 0.8472 - precision: 0.8913 - recall: 0.7957 - val_loss: 0.4852 - val_accuracy: 0.8326 - val_f1_score: 0.8317 - val_f1_score_micro: 0.8326 - val_precision: 0.8804 - val_recall: 0.7814\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4505 - accuracy: 0.8477 - f1_score: 0.8465 - f1_score_micro: 0.8477 - precision: 0.8918 - recall: 0.7974 - val_loss: 0.4829 - val_accuracy: 0.8313 - val_f1_score: 0.8295 - val_f1_score_micro: 0.8313 - val_precision: 0.8770 - val_recall: 0.7816\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4471 - accuracy: 0.8486 - f1_score: 0.8474 - f1_score_micro: 0.8486 - precision: 0.8918 - recall: 0.7989 - val_loss: 0.4793 - val_accuracy: 0.8346 - val_f1_score: 0.8336 - val_f1_score_micro: 0.8346 - val_precision: 0.8811 - val_recall: 0.7853\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4436 - accuracy: 0.8501 - f1_score: 0.8490 - f1_score_micro: 0.8501 - precision: 0.8923 - recall: 0.8013 - val_loss: 0.4766 - val_accuracy: 0.8333 - val_f1_score: 0.8317 - val_f1_score_micro: 0.8333 - val_precision: 0.8803 - val_recall: 0.7859\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4410 - accuracy: 0.8507 - f1_score: 0.8495 - f1_score_micro: 0.8507 - precision: 0.8926 - recall: 0.8026 - val_loss: 0.4747 - val_accuracy: 0.8356 - val_f1_score: 0.8335 - val_f1_score_micro: 0.8356 - val_precision: 0.8802 - val_recall: 0.7890\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4384 - accuracy: 0.8511 - f1_score: 0.8499 - f1_score_micro: 0.8512 - precision: 0.8920 - recall: 0.8033 - val_loss: 0.4725 - val_accuracy: 0.8371 - val_f1_score: 0.8352 - val_f1_score_micro: 0.8371 - val_precision: 0.8804 - val_recall: 0.7902\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4358 - accuracy: 0.8528 - f1_score: 0.8517 - f1_score_micro: 0.8528 - precision: 0.8933 - recall: 0.8066 - val_loss: 0.4704 - val_accuracy: 0.8364 - val_f1_score: 0.8345 - val_f1_score_micro: 0.8364 - val_precision: 0.8810 - val_recall: 0.7907\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4338 - accuracy: 0.8525 - f1_score: 0.8515 - f1_score_micro: 0.8525 - precision: 0.8930 - recall: 0.8068 - val_loss: 0.4694 - val_accuracy: 0.8393 - val_f1_score: 0.8380 - val_f1_score_micro: 0.8393 - val_precision: 0.8809 - val_recall: 0.7929\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4315 - accuracy: 0.8535 - f1_score: 0.8523 - f1_score_micro: 0.8535 - precision: 0.8935 - recall: 0.8085 - val_loss: 0.4677 - val_accuracy: 0.8392 - val_f1_score: 0.8384 - val_f1_score_micro: 0.8392 - val_precision: 0.8825 - val_recall: 0.7937\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4294 - accuracy: 0.8546 - f1_score: 0.8536 - f1_score_micro: 0.8546 - precision: 0.8939 - recall: 0.8096 - val_loss: 0.4666 - val_accuracy: 0.8374 - val_f1_score: 0.8367 - val_f1_score_micro: 0.8374 - val_precision: 0.8811 - val_recall: 0.7933\n",
            "Epoch 26/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4276 - accuracy: 0.8550 - f1_score: 0.8540 - f1_score_micro: 0.8550 - precision: 0.8939 - recall: 0.8109 - val_loss: 0.4661 - val_accuracy: 0.8400 - val_f1_score: 0.8392 - val_f1_score_micro: 0.8400 - val_precision: 0.8812 - val_recall: 0.7937\n",
            "Epoch 27/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4259 - accuracy: 0.8555 - f1_score: 0.8545 - f1_score_micro: 0.8555 - precision: 0.8943 - recall: 0.8121 - val_loss: 0.4632 - val_accuracy: 0.8398 - val_f1_score: 0.8391 - val_f1_score_micro: 0.8398 - val_precision: 0.8823 - val_recall: 0.7965\n",
            "Epoch 28/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4242 - accuracy: 0.8551 - f1_score: 0.8542 - f1_score_micro: 0.8551 - precision: 0.8938 - recall: 0.8126 - val_loss: 0.4618 - val_accuracy: 0.8396 - val_f1_score: 0.8383 - val_f1_score_micro: 0.8396 - val_precision: 0.8823 - val_recall: 0.7978\n",
            "Epoch 29/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4227 - accuracy: 0.8553 - f1_score: 0.8543 - f1_score_micro: 0.8553 - precision: 0.8941 - recall: 0.8139 - val_loss: 0.4610 - val_accuracy: 0.8412 - val_f1_score: 0.8403 - val_f1_score_micro: 0.8412 - val_precision: 0.8814 - val_recall: 0.7982\n",
            "Epoch 30/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4211 - accuracy: 0.8564 - f1_score: 0.8554 - f1_score_micro: 0.8564 - precision: 0.8946 - recall: 0.8143 - val_loss: 0.4593 - val_accuracy: 0.8410 - val_f1_score: 0.8398 - val_f1_score_micro: 0.8410 - val_precision: 0.8827 - val_recall: 0.7983\n",
            "Epoch 31/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4196 - accuracy: 0.8571 - f1_score: 0.8561 - f1_score_micro: 0.8571 - precision: 0.8945 - recall: 0.8153 - val_loss: 0.4590 - val_accuracy: 0.8418 - val_f1_score: 0.8407 - val_f1_score_micro: 0.8418 - val_precision: 0.8824 - val_recall: 0.7991\n",
            "Epoch 32/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4182 - accuracy: 0.8572 - f1_score: 0.8562 - f1_score_micro: 0.8572 - precision: 0.8944 - recall: 0.8165 - val_loss: 0.4583 - val_accuracy: 0.8420 - val_f1_score: 0.8404 - val_f1_score_micro: 0.8420 - val_precision: 0.8823 - val_recall: 0.8007\n",
            "Epoch 33/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4170 - accuracy: 0.8579 - f1_score: 0.8570 - f1_score_micro: 0.8579 - precision: 0.8949 - recall: 0.8177 - val_loss: 0.4568 - val_accuracy: 0.8398 - val_f1_score: 0.8387 - val_f1_score_micro: 0.8398 - val_precision: 0.8815 - val_recall: 0.8007\n",
            "Epoch 34/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4158 - accuracy: 0.8586 - f1_score: 0.8576 - f1_score_micro: 0.8586 - precision: 0.8951 - recall: 0.8177 - val_loss: 0.4563 - val_accuracy: 0.8407 - val_f1_score: 0.8392 - val_f1_score_micro: 0.8407 - val_precision: 0.8818 - val_recall: 0.8021\n",
            "Epoch 35/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4146 - accuracy: 0.8588 - f1_score: 0.8579 - f1_score_micro: 0.8588 - precision: 0.8952 - recall: 0.8187 - val_loss: 0.4552 - val_accuracy: 0.8413 - val_f1_score: 0.8399 - val_f1_score_micro: 0.8413 - val_precision: 0.8818 - val_recall: 0.8026\n",
            "Epoch 36/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4134 - accuracy: 0.8595 - f1_score: 0.8586 - f1_score_micro: 0.8595 - precision: 0.8953 - recall: 0.8196 - val_loss: 0.4569 - val_accuracy: 0.8407 - val_f1_score: 0.8411 - val_f1_score_micro: 0.8407 - val_precision: 0.8798 - val_recall: 0.8016\n",
            "Epoch 37/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4124 - accuracy: 0.8587 - f1_score: 0.8579 - f1_score_micro: 0.8587 - precision: 0.8949 - recall: 0.8191 - val_loss: 0.4541 - val_accuracy: 0.8424 - val_f1_score: 0.8413 - val_f1_score_micro: 0.8424 - val_precision: 0.8813 - val_recall: 0.8041\n",
            "Epoch 38/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4114 - accuracy: 0.8597 - f1_score: 0.8588 - f1_score_micro: 0.8597 - precision: 0.8954 - recall: 0.8212 - val_loss: 0.4532 - val_accuracy: 0.8408 - val_f1_score: 0.8401 - val_f1_score_micro: 0.8408 - val_precision: 0.8816 - val_recall: 0.8016\n",
            "Epoch 39/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4104 - accuracy: 0.8601 - f1_score: 0.8593 - f1_score_micro: 0.8601 - precision: 0.8965 - recall: 0.8209 - val_loss: 0.4526 - val_accuracy: 0.8432 - val_f1_score: 0.8428 - val_f1_score_micro: 0.8432 - val_precision: 0.8821 - val_recall: 0.8027\n",
            "Epoch 40/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4095 - accuracy: 0.8608 - f1_score: 0.8600 - f1_score_micro: 0.8608 - precision: 0.8956 - recall: 0.8214 - val_loss: 0.4513 - val_accuracy: 0.8432 - val_f1_score: 0.8422 - val_f1_score_micro: 0.8432 - val_precision: 0.8812 - val_recall: 0.8047\n",
            "Epoch 41/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4083 - accuracy: 0.8600 - f1_score: 0.8591 - f1_score_micro: 0.8600 - precision: 0.8953 - recall: 0.8220 - val_loss: 0.4522 - val_accuracy: 0.8436 - val_f1_score: 0.8431 - val_f1_score_micro: 0.8436 - val_precision: 0.8800 - val_recall: 0.8046\n",
            "Epoch 42/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4076 - accuracy: 0.8612 - f1_score: 0.8604 - f1_score_micro: 0.8612 - precision: 0.8962 - recall: 0.8227 - val_loss: 0.4504 - val_accuracy: 0.8440 - val_f1_score: 0.8425 - val_f1_score_micro: 0.8440 - val_precision: 0.8807 - val_recall: 0.8062\n",
            "Epoch 43/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4068 - accuracy: 0.8614 - f1_score: 0.8605 - f1_score_micro: 0.8614 - precision: 0.8965 - recall: 0.8225 - val_loss: 0.4502 - val_accuracy: 0.8431 - val_f1_score: 0.8425 - val_f1_score_micro: 0.8431 - val_precision: 0.8808 - val_recall: 0.8045\n",
            "Epoch 44/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4057 - accuracy: 0.8622 - f1_score: 0.8614 - f1_score_micro: 0.8622 - precision: 0.8961 - recall: 0.8237 - val_loss: 0.4511 - val_accuracy: 0.8417 - val_f1_score: 0.8391 - val_f1_score_micro: 0.8417 - val_precision: 0.8806 - val_recall: 0.8045\n",
            "Epoch 45/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4049 - accuracy: 0.8616 - f1_score: 0.8608 - f1_score_micro: 0.8616 - precision: 0.8965 - recall: 0.8242 - val_loss: 0.4499 - val_accuracy: 0.8438 - val_f1_score: 0.8431 - val_f1_score_micro: 0.8438 - val_precision: 0.8810 - val_recall: 0.8063\n",
            "Epoch 46/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4042 - accuracy: 0.8621 - f1_score: 0.8613 - f1_score_micro: 0.8621 - precision: 0.8973 - recall: 0.8244 - val_loss: 0.4482 - val_accuracy: 0.8433 - val_f1_score: 0.8423 - val_f1_score_micro: 0.8433 - val_precision: 0.8822 - val_recall: 0.8058\n",
            "Epoch 47/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8627 - f1_score: 0.8619 - f1_score_micro: 0.8627 - precision: 0.8968 - recall: 0.8252 - val_loss: 0.4485 - val_accuracy: 0.8440 - val_f1_score: 0.8431 - val_f1_score_micro: 0.8440 - val_precision: 0.8801 - val_recall: 0.8063\n",
            "--- Starting trial: /run-4\n",
            "{'optimizer': 'adam', 'learning_rate': 0.0001, 'beta_1': 0.94, 'beta_2': 0.97}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 1.1190 - accuracy: 0.6513 - f1_score: 0.6273 - f1_score_micro: 0.6513 - precision: 0.8841 - recall: 0.3413 - val_loss: 0.7829 - val_accuracy: 0.7306 - val_f1_score: 0.7159 - val_f1_score_micro: 0.7306 - val_precision: 0.8596 - val_recall: 0.5579\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.6920 - accuracy: 0.7663 - f1_score: 0.7581 - f1_score_micro: 0.7663 - precision: 0.8682 - recall: 0.6214 - val_loss: 0.6609 - val_accuracy: 0.7764 - val_f1_score: 0.7693 - val_f1_score_micro: 0.7764 - val_precision: 0.8681 - val_recall: 0.6564\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.6101 - accuracy: 0.7956 - f1_score: 0.7907 - f1_score_micro: 0.7956 - precision: 0.8755 - recall: 0.6833 - val_loss: 0.6081 - val_accuracy: 0.7942 - val_f1_score: 0.7900 - val_f1_score_micro: 0.7942 - val_precision: 0.8694 - val_recall: 0.6907\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.5682 - accuracy: 0.8098 - f1_score: 0.8059 - f1_score_micro: 0.8098 - precision: 0.8796 - recall: 0.7152 - val_loss: 0.5772 - val_accuracy: 0.8033 - val_f1_score: 0.7991 - val_f1_score_micro: 0.8033 - val_precision: 0.8695 - val_recall: 0.7137\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.5414 - accuracy: 0.8195 - f1_score: 0.8163 - f1_score_micro: 0.8195 - precision: 0.8807 - recall: 0.7345 - val_loss: 0.5572 - val_accuracy: 0.8131 - val_f1_score: 0.8104 - val_f1_score_micro: 0.8131 - val_precision: 0.8748 - val_recall: 0.7323\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.5224 - accuracy: 0.8255 - f1_score: 0.8227 - f1_score_micro: 0.8255 - precision: 0.8831 - recall: 0.7472 - val_loss: 0.5423 - val_accuracy: 0.8166 - val_f1_score: 0.8141 - val_f1_score_micro: 0.8166 - val_precision: 0.8729 - val_recall: 0.7451\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.5082 - accuracy: 0.8296 - f1_score: 0.8272 - f1_score_micro: 0.8296 - precision: 0.8842 - recall: 0.7587 - val_loss: 0.5300 - val_accuracy: 0.8198 - val_f1_score: 0.8169 - val_f1_score_micro: 0.8198 - val_precision: 0.8746 - val_recall: 0.7514\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4967 - accuracy: 0.8335 - f1_score: 0.8313 - f1_score_micro: 0.8335 - precision: 0.8859 - recall: 0.7663 - val_loss: 0.5212 - val_accuracy: 0.8221 - val_f1_score: 0.8193 - val_f1_score_micro: 0.8221 - val_precision: 0.8748 - val_recall: 0.7559\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4874 - accuracy: 0.8365 - f1_score: 0.8344 - f1_score_micro: 0.8365 - precision: 0.8871 - recall: 0.7728 - val_loss: 0.5134 - val_accuracy: 0.8261 - val_f1_score: 0.8238 - val_f1_score_micro: 0.8261 - val_precision: 0.8764 - val_recall: 0.7627\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4796 - accuracy: 0.8393 - f1_score: 0.8374 - f1_score_micro: 0.8394 - precision: 0.8873 - recall: 0.7783 - val_loss: 0.5075 - val_accuracy: 0.8273 - val_f1_score: 0.8261 - val_f1_score_micro: 0.8273 - val_precision: 0.8765 - val_recall: 0.7659\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4732 - accuracy: 0.8413 - f1_score: 0.8395 - f1_score_micro: 0.8413 - precision: 0.8884 - recall: 0.7829 - val_loss: 0.5015 - val_accuracy: 0.8305 - val_f1_score: 0.8281 - val_f1_score_micro: 0.8305 - val_precision: 0.8787 - val_recall: 0.7729\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4672 - accuracy: 0.8429 - f1_score: 0.8412 - f1_score_micro: 0.8429 - precision: 0.8893 - recall: 0.7874 - val_loss: 0.4968 - val_accuracy: 0.8307 - val_f1_score: 0.8286 - val_f1_score_micro: 0.8307 - val_precision: 0.8772 - val_recall: 0.7756\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4620 - accuracy: 0.8443 - f1_score: 0.8427 - f1_score_micro: 0.8443 - precision: 0.8892 - recall: 0.7897 - val_loss: 0.4928 - val_accuracy: 0.8314 - val_f1_score: 0.8289 - val_f1_score_micro: 0.8314 - val_precision: 0.8775 - val_recall: 0.7789\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4578 - accuracy: 0.8456 - f1_score: 0.8442 - f1_score_micro: 0.8456 - precision: 0.8899 - recall: 0.7936 - val_loss: 0.4889 - val_accuracy: 0.8332 - val_f1_score: 0.8317 - val_f1_score_micro: 0.8332 - val_precision: 0.8792 - val_recall: 0.7804\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4532 - accuracy: 0.8475 - f1_score: 0.8461 - f1_score_micro: 0.8475 - precision: 0.8912 - recall: 0.7962 - val_loss: 0.4870 - val_accuracy: 0.8335 - val_f1_score: 0.8320 - val_f1_score_micro: 0.8335 - val_precision: 0.8782 - val_recall: 0.7811\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4498 - accuracy: 0.8478 - f1_score: 0.8465 - f1_score_micro: 0.8478 - precision: 0.8913 - recall: 0.7987 - val_loss: 0.4836 - val_accuracy: 0.8363 - val_f1_score: 0.8355 - val_f1_score_micro: 0.8363 - val_precision: 0.8811 - val_recall: 0.7848\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4465 - accuracy: 0.8487 - f1_score: 0.8473 - f1_score_micro: 0.8487 - precision: 0.8912 - recall: 0.8001 - val_loss: 0.4807 - val_accuracy: 0.8335 - val_f1_score: 0.8322 - val_f1_score_micro: 0.8335 - val_precision: 0.8781 - val_recall: 0.7870\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4437 - accuracy: 0.8497 - f1_score: 0.8483 - f1_score_micro: 0.8497 - precision: 0.8913 - recall: 0.8023 - val_loss: 0.4784 - val_accuracy: 0.8371 - val_f1_score: 0.8356 - val_f1_score_micro: 0.8371 - val_precision: 0.8803 - val_recall: 0.7891\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4407 - accuracy: 0.8514 - f1_score: 0.8502 - f1_score_micro: 0.8514 - precision: 0.8914 - recall: 0.8043 - val_loss: 0.4762 - val_accuracy: 0.8364 - val_f1_score: 0.8347 - val_f1_score_micro: 0.8364 - val_precision: 0.8799 - val_recall: 0.7928\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4382 - accuracy: 0.8513 - f1_score: 0.8500 - f1_score_micro: 0.8513 - precision: 0.8920 - recall: 0.8057 - val_loss: 0.4748 - val_accuracy: 0.8376 - val_f1_score: 0.8373 - val_f1_score_micro: 0.8376 - val_precision: 0.8808 - val_recall: 0.7926\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4358 - accuracy: 0.8526 - f1_score: 0.8515 - f1_score_micro: 0.8526 - precision: 0.8922 - recall: 0.8078 - val_loss: 0.4724 - val_accuracy: 0.8374 - val_f1_score: 0.8357 - val_f1_score_micro: 0.8374 - val_precision: 0.8792 - val_recall: 0.7939\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4336 - accuracy: 0.8528 - f1_score: 0.8516 - f1_score_micro: 0.8528 - precision: 0.8923 - recall: 0.8084 - val_loss: 0.4699 - val_accuracy: 0.8386 - val_f1_score: 0.8374 - val_f1_score_micro: 0.8386 - val_precision: 0.8800 - val_recall: 0.7944\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4314 - accuracy: 0.8537 - f1_score: 0.8526 - f1_score_micro: 0.8537 - precision: 0.8926 - recall: 0.8098 - val_loss: 0.4687 - val_accuracy: 0.8383 - val_f1_score: 0.8372 - val_f1_score_micro: 0.8383 - val_precision: 0.8826 - val_recall: 0.7968\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4296 - accuracy: 0.8539 - f1_score: 0.8528 - f1_score_micro: 0.8539 - precision: 0.8931 - recall: 0.8109 - val_loss: 0.4666 - val_accuracy: 0.8393 - val_f1_score: 0.8380 - val_f1_score_micro: 0.8393 - val_precision: 0.8815 - val_recall: 0.7974\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4278 - accuracy: 0.8549 - f1_score: 0.8538 - f1_score_micro: 0.8549 - precision: 0.8931 - recall: 0.8127 - val_loss: 0.4655 - val_accuracy: 0.8393 - val_f1_score: 0.8380 - val_f1_score_micro: 0.8393 - val_precision: 0.8813 - val_recall: 0.7988\n",
            "Epoch 26/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4258 - accuracy: 0.8554 - f1_score: 0.8544 - f1_score_micro: 0.8554 - precision: 0.8930 - recall: 0.8137 - val_loss: 0.4639 - val_accuracy: 0.8394 - val_f1_score: 0.8383 - val_f1_score_micro: 0.8394 - val_precision: 0.8822 - val_recall: 0.7984\n",
            "Epoch 27/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4242 - accuracy: 0.8560 - f1_score: 0.8549 - f1_score_micro: 0.8560 - precision: 0.8938 - recall: 0.8144 - val_loss: 0.4639 - val_accuracy: 0.8403 - val_f1_score: 0.8389 - val_f1_score_micro: 0.8403 - val_precision: 0.8829 - val_recall: 0.7986\n",
            "Epoch 28/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4227 - accuracy: 0.8560 - f1_score: 0.8549 - f1_score_micro: 0.8560 - precision: 0.8934 - recall: 0.8147 - val_loss: 0.4626 - val_accuracy: 0.8393 - val_f1_score: 0.8385 - val_f1_score_micro: 0.8393 - val_precision: 0.8813 - val_recall: 0.7994\n",
            "Epoch 29/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4213 - accuracy: 0.8569 - f1_score: 0.8559 - f1_score_micro: 0.8569 - precision: 0.8937 - recall: 0.8163 - val_loss: 0.4623 - val_accuracy: 0.8408 - val_f1_score: 0.8379 - val_f1_score_micro: 0.8408 - val_precision: 0.8808 - val_recall: 0.8026\n",
            "Epoch 30/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8580 - f1_score: 0.8571 - f1_score_micro: 0.8580 - precision: 0.8937 - recall: 0.8176 - val_loss: 0.4599 - val_accuracy: 0.8415 - val_f1_score: 0.8402 - val_f1_score_micro: 0.8415 - val_precision: 0.8807 - val_recall: 0.8023\n",
            "Epoch 31/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4186 - accuracy: 0.8580 - f1_score: 0.8570 - f1_score_micro: 0.8580 - precision: 0.8943 - recall: 0.8177 - val_loss: 0.4588 - val_accuracy: 0.8420 - val_f1_score: 0.8402 - val_f1_score_micro: 0.8420 - val_precision: 0.8804 - val_recall: 0.8032\n",
            "Epoch 32/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4173 - accuracy: 0.8586 - f1_score: 0.8576 - f1_score_micro: 0.8586 - precision: 0.8936 - recall: 0.8182 - val_loss: 0.4589 - val_accuracy: 0.8419 - val_f1_score: 0.8415 - val_f1_score_micro: 0.8419 - val_precision: 0.8806 - val_recall: 0.8028\n",
            "Epoch 33/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4161 - accuracy: 0.8587 - f1_score: 0.8578 - f1_score_micro: 0.8587 - precision: 0.8944 - recall: 0.8200 - val_loss: 0.4573 - val_accuracy: 0.8409 - val_f1_score: 0.8394 - val_f1_score_micro: 0.8409 - val_precision: 0.8819 - val_recall: 0.8031\n",
            "Epoch 34/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4149 - accuracy: 0.8590 - f1_score: 0.8581 - f1_score_micro: 0.8590 - precision: 0.8942 - recall: 0.8195 - val_loss: 0.4561 - val_accuracy: 0.8421 - val_f1_score: 0.8404 - val_f1_score_micro: 0.8421 - val_precision: 0.8808 - val_recall: 0.8052\n",
            "Epoch 35/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4138 - accuracy: 0.8594 - f1_score: 0.8584 - f1_score_micro: 0.8594 - precision: 0.8943 - recall: 0.8202 - val_loss: 0.4559 - val_accuracy: 0.8423 - val_f1_score: 0.8414 - val_f1_score_micro: 0.8423 - val_precision: 0.8808 - val_recall: 0.8037\n",
            "Epoch 36/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4129 - accuracy: 0.8598 - f1_score: 0.8589 - f1_score_micro: 0.8598 - precision: 0.8938 - recall: 0.8216 - val_loss: 0.4553 - val_accuracy: 0.8423 - val_f1_score: 0.8411 - val_f1_score_micro: 0.8423 - val_precision: 0.8793 - val_recall: 0.8045\n",
            "Epoch 37/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4118 - accuracy: 0.8604 - f1_score: 0.8595 - f1_score_micro: 0.8604 - precision: 0.8949 - recall: 0.8221 - val_loss: 0.4541 - val_accuracy: 0.8430 - val_f1_score: 0.8417 - val_f1_score_micro: 0.8430 - val_precision: 0.8798 - val_recall: 0.8055\n",
            "Epoch 38/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4105 - accuracy: 0.8607 - f1_score: 0.8598 - f1_score_micro: 0.8607 - precision: 0.8950 - recall: 0.8229 - val_loss: 0.4538 - val_accuracy: 0.8430 - val_f1_score: 0.8415 - val_f1_score_micro: 0.8430 - val_precision: 0.8803 - val_recall: 0.8047\n",
            "Epoch 39/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4098 - accuracy: 0.8606 - f1_score: 0.8597 - f1_score_micro: 0.8606 - precision: 0.8951 - recall: 0.8227 - val_loss: 0.4535 - val_accuracy: 0.8428 - val_f1_score: 0.8416 - val_f1_score_micro: 0.8428 - val_precision: 0.8802 - val_recall: 0.8053\n",
            "Epoch 40/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4090 - accuracy: 0.8614 - f1_score: 0.8605 - f1_score_micro: 0.8614 - precision: 0.8948 - recall: 0.8236 - val_loss: 0.4524 - val_accuracy: 0.8443 - val_f1_score: 0.8431 - val_f1_score_micro: 0.8443 - val_precision: 0.8799 - val_recall: 0.8067\n",
            "Epoch 41/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4080 - accuracy: 0.8613 - f1_score: 0.8605 - f1_score_micro: 0.8613 - precision: 0.8948 - recall: 0.8241 - val_loss: 0.4521 - val_accuracy: 0.8437 - val_f1_score: 0.8430 - val_f1_score_micro: 0.8437 - val_precision: 0.8812 - val_recall: 0.8070\n",
            "Epoch 42/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4074 - accuracy: 0.8615 - f1_score: 0.8606 - f1_score_micro: 0.8615 - precision: 0.8951 - recall: 0.8244 - val_loss: 0.4522 - val_accuracy: 0.8435 - val_f1_score: 0.8424 - val_f1_score_micro: 0.8435 - val_precision: 0.8787 - val_recall: 0.8071\n",
            "Epoch 43/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4064 - accuracy: 0.8615 - f1_score: 0.8606 - f1_score_micro: 0.8615 - precision: 0.8954 - recall: 0.8250 - val_loss: 0.4507 - val_accuracy: 0.8428 - val_f1_score: 0.8416 - val_f1_score_micro: 0.8428 - val_precision: 0.8814 - val_recall: 0.8065\n",
            "Epoch 44/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4056 - accuracy: 0.8619 - f1_score: 0.8610 - f1_score_micro: 0.8619 - precision: 0.8952 - recall: 0.8250 - val_loss: 0.4511 - val_accuracy: 0.8437 - val_f1_score: 0.8429 - val_f1_score_micro: 0.8437 - val_precision: 0.8804 - val_recall: 0.8062\n",
            "Epoch 45/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4050 - accuracy: 0.8615 - f1_score: 0.8606 - f1_score_micro: 0.8615 - precision: 0.8950 - recall: 0.8253 - val_loss: 0.4504 - val_accuracy: 0.8427 - val_f1_score: 0.8413 - val_f1_score_micro: 0.8427 - val_precision: 0.8786 - val_recall: 0.8071\n",
            "--- Starting trial: /run-5\n",
            "{'optimizer': 'adam', 'learning_rate': 0.0001, 'beta_1': 0.94, 'beta_2': 0.999}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 1.2219 - accuracy: 0.6253 - f1_score: 0.6087 - f1_score_micro: 0.6253 - precision: 0.9148 - recall: 0.2733 - val_loss: 0.8532 - val_accuracy: 0.7187 - val_f1_score: 0.7025 - val_f1_score_micro: 0.7187 - val_precision: 0.8824 - val_recall: 0.5066\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.7505 - accuracy: 0.7586 - f1_score: 0.7501 - f1_score_micro: 0.7586 - precision: 0.8808 - recall: 0.5743 - val_loss: 0.7055 - val_accuracy: 0.7628 - val_f1_score: 0.7587 - val_f1_score_micro: 0.7628 - val_precision: 0.8694 - val_recall: 0.6112\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.6499 - accuracy: 0.7871 - f1_score: 0.7822 - f1_score_micro: 0.7871 - precision: 0.8785 - recall: 0.6524 - val_loss: 0.6399 - val_accuracy: 0.7862 - val_f1_score: 0.7835 - val_f1_score_micro: 0.7862 - val_precision: 0.8708 - val_recall: 0.6653\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5972 - accuracy: 0.8039 - f1_score: 0.8002 - f1_score_micro: 0.8039 - precision: 0.8806 - recall: 0.6943 - val_loss: 0.6009 - val_accuracy: 0.7981 - val_f1_score: 0.7948 - val_f1_score_micro: 0.7981 - val_precision: 0.8704 - val_recall: 0.6977\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.8138 - f1_score: 0.8106 - f1_score_micro: 0.8138 - precision: 0.8815 - recall: 0.7196 - val_loss: 0.5734 - val_accuracy: 0.8088 - val_f1_score: 0.8056 - val_f1_score_micro: 0.8088 - val_precision: 0.8736 - val_recall: 0.7197\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5394 - accuracy: 0.8214 - f1_score: 0.8188 - f1_score_micro: 0.8214 - precision: 0.8835 - recall: 0.7366 - val_loss: 0.5549 - val_accuracy: 0.8144 - val_f1_score: 0.8112 - val_f1_score_micro: 0.8144 - val_precision: 0.8737 - val_recall: 0.7333\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5217 - accuracy: 0.8263 - f1_score: 0.8239 - f1_score_micro: 0.8263 - precision: 0.8850 - recall: 0.7485 - val_loss: 0.5398 - val_accuracy: 0.8180 - val_f1_score: 0.8151 - val_f1_score_micro: 0.8180 - val_precision: 0.8754 - val_recall: 0.7443\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.5079 - accuracy: 0.8307 - f1_score: 0.8286 - f1_score_micro: 0.8307 - precision: 0.8859 - recall: 0.7582 - val_loss: 0.5302 - val_accuracy: 0.8209 - val_f1_score: 0.8190 - val_f1_score_micro: 0.8209 - val_precision: 0.8742 - val_recall: 0.7537\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4968 - accuracy: 0.8339 - f1_score: 0.8320 - f1_score_micro: 0.8339 - precision: 0.8866 - recall: 0.7666 - val_loss: 0.5203 - val_accuracy: 0.8249 - val_f1_score: 0.8229 - val_f1_score_micro: 0.8249 - val_precision: 0.8760 - val_recall: 0.7574\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4877 - accuracy: 0.8364 - f1_score: 0.8345 - f1_score_micro: 0.8364 - precision: 0.8876 - recall: 0.7721 - val_loss: 0.5127 - val_accuracy: 0.8256 - val_f1_score: 0.8230 - val_f1_score_micro: 0.8256 - val_precision: 0.8768 - val_recall: 0.7643\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4797 - accuracy: 0.8395 - f1_score: 0.8378 - f1_score_micro: 0.8395 - precision: 0.8882 - recall: 0.7781 - val_loss: 0.5068 - val_accuracy: 0.8285 - val_f1_score: 0.8269 - val_f1_score_micro: 0.8285 - val_precision: 0.8773 - val_recall: 0.7673\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4733 - accuracy: 0.8414 - f1_score: 0.8399 - f1_score_micro: 0.8414 - precision: 0.8887 - recall: 0.7816 - val_loss: 0.5015 - val_accuracy: 0.8287 - val_f1_score: 0.8273 - val_f1_score_micro: 0.8287 - val_precision: 0.8780 - val_recall: 0.7713\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4674 - accuracy: 0.8434 - f1_score: 0.8420 - f1_score_micro: 0.8434 - precision: 0.8895 - recall: 0.7861 - val_loss: 0.4961 - val_accuracy: 0.8314 - val_f1_score: 0.8292 - val_f1_score_micro: 0.8314 - val_precision: 0.8783 - val_recall: 0.7746\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4624 - accuracy: 0.8447 - f1_score: 0.8433 - f1_score_micro: 0.8447 - precision: 0.8902 - recall: 0.7891 - val_loss: 0.4918 - val_accuracy: 0.8307 - val_f1_score: 0.8290 - val_f1_score_micro: 0.8307 - val_precision: 0.8785 - val_recall: 0.7779\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4582 - accuracy: 0.8451 - f1_score: 0.8438 - f1_score_micro: 0.8451 - precision: 0.8905 - recall: 0.7921 - val_loss: 0.4881 - val_accuracy: 0.8322 - val_f1_score: 0.8305 - val_f1_score_micro: 0.8322 - val_precision: 0.8802 - val_recall: 0.7797\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4538 - accuracy: 0.8469 - f1_score: 0.8456 - f1_score_micro: 0.8469 - precision: 0.8913 - recall: 0.7943 - val_loss: 0.4859 - val_accuracy: 0.8329 - val_f1_score: 0.8315 - val_f1_score_micro: 0.8329 - val_precision: 0.8800 - val_recall: 0.7825\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4501 - accuracy: 0.8484 - f1_score: 0.8471 - f1_score_micro: 0.8484 - precision: 0.8917 - recall: 0.7974 - val_loss: 0.4832 - val_accuracy: 0.8337 - val_f1_score: 0.8328 - val_f1_score_micro: 0.8337 - val_precision: 0.8787 - val_recall: 0.7847\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4469 - accuracy: 0.8485 - f1_score: 0.8473 - f1_score_micro: 0.8485 - precision: 0.8917 - recall: 0.7989 - val_loss: 0.4797 - val_accuracy: 0.8349 - val_f1_score: 0.8338 - val_f1_score_micro: 0.8349 - val_precision: 0.8800 - val_recall: 0.7859\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4436 - accuracy: 0.8499 - f1_score: 0.8486 - f1_score_micro: 0.8499 - precision: 0.8925 - recall: 0.8016 - val_loss: 0.4772 - val_accuracy: 0.8345 - val_f1_score: 0.8326 - val_f1_score_micro: 0.8345 - val_precision: 0.8789 - val_recall: 0.7874\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4406 - accuracy: 0.8509 - f1_score: 0.8498 - f1_score_micro: 0.8509 - precision: 0.8927 - recall: 0.8023 - val_loss: 0.4762 - val_accuracy: 0.8357 - val_f1_score: 0.8327 - val_f1_score_micro: 0.8357 - val_precision: 0.8794 - val_recall: 0.7873\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4382 - accuracy: 0.8517 - f1_score: 0.8506 - f1_score_micro: 0.8517 - precision: 0.8928 - recall: 0.8042 - val_loss: 0.4729 - val_accuracy: 0.8366 - val_f1_score: 0.8354 - val_f1_score_micro: 0.8366 - val_precision: 0.8815 - val_recall: 0.7896\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4359 - accuracy: 0.8528 - f1_score: 0.8516 - f1_score_micro: 0.8528 - precision: 0.8930 - recall: 0.8055 - val_loss: 0.4712 - val_accuracy: 0.8364 - val_f1_score: 0.8351 - val_f1_score_micro: 0.8364 - val_precision: 0.8818 - val_recall: 0.7905\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4334 - accuracy: 0.8525 - f1_score: 0.8514 - f1_score_micro: 0.8525 - precision: 0.8935 - recall: 0.8077 - val_loss: 0.4701 - val_accuracy: 0.8368 - val_f1_score: 0.8360 - val_f1_score_micro: 0.8368 - val_precision: 0.8814 - val_recall: 0.7899\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4315 - accuracy: 0.8538 - f1_score: 0.8527 - f1_score_micro: 0.8538 - precision: 0.8932 - recall: 0.8084 - val_loss: 0.4675 - val_accuracy: 0.8376 - val_f1_score: 0.8364 - val_f1_score_micro: 0.8376 - val_precision: 0.8808 - val_recall: 0.7927\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4294 - accuracy: 0.8550 - f1_score: 0.8540 - f1_score_micro: 0.8550 - precision: 0.8939 - recall: 0.8096 - val_loss: 0.4663 - val_accuracy: 0.8377 - val_f1_score: 0.8365 - val_f1_score_micro: 0.8377 - val_precision: 0.8820 - val_recall: 0.7939\n",
            "Epoch 26/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4275 - accuracy: 0.8551 - f1_score: 0.8541 - f1_score_micro: 0.8551 - precision: 0.8942 - recall: 0.8106 - val_loss: 0.4667 - val_accuracy: 0.8378 - val_f1_score: 0.8364 - val_f1_score_micro: 0.8378 - val_precision: 0.8802 - val_recall: 0.7935\n",
            "Epoch 27/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4256 - accuracy: 0.8556 - f1_score: 0.8546 - f1_score_micro: 0.8556 - precision: 0.8937 - recall: 0.8116 - val_loss: 0.4646 - val_accuracy: 0.8387 - val_f1_score: 0.8375 - val_f1_score_micro: 0.8387 - val_precision: 0.8822 - val_recall: 0.7952\n",
            "Epoch 28/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4241 - accuracy: 0.8565 - f1_score: 0.8555 - f1_score_micro: 0.8565 - precision: 0.8941 - recall: 0.8128 - val_loss: 0.4622 - val_accuracy: 0.8395 - val_f1_score: 0.8386 - val_f1_score_micro: 0.8395 - val_precision: 0.8822 - val_recall: 0.7956\n",
            "Epoch 29/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4225 - accuracy: 0.8565 - f1_score: 0.8556 - f1_score_micro: 0.8565 - precision: 0.8949 - recall: 0.8132 - val_loss: 0.4612 - val_accuracy: 0.8391 - val_f1_score: 0.8380 - val_f1_score_micro: 0.8391 - val_precision: 0.8805 - val_recall: 0.7949\n",
            "Epoch 30/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4210 - accuracy: 0.8567 - f1_score: 0.8557 - f1_score_micro: 0.8567 - precision: 0.8942 - recall: 0.8145 - val_loss: 0.4616 - val_accuracy: 0.8390 - val_f1_score: 0.8376 - val_f1_score_micro: 0.8390 - val_precision: 0.8813 - val_recall: 0.7955\n",
            "Epoch 31/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4196 - accuracy: 0.8577 - f1_score: 0.8567 - f1_score_micro: 0.8577 - precision: 0.8952 - recall: 0.8155 - val_loss: 0.4599 - val_accuracy: 0.8391 - val_f1_score: 0.8387 - val_f1_score_micro: 0.8391 - val_precision: 0.8804 - val_recall: 0.7958\n",
            "Epoch 32/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4182 - accuracy: 0.8576 - f1_score: 0.8567 - f1_score_micro: 0.8576 - precision: 0.8950 - recall: 0.8159 - val_loss: 0.4590 - val_accuracy: 0.8409 - val_f1_score: 0.8384 - val_f1_score_micro: 0.8409 - val_precision: 0.8802 - val_recall: 0.8002\n",
            "Epoch 33/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4169 - accuracy: 0.8590 - f1_score: 0.8581 - f1_score_micro: 0.8590 - precision: 0.8953 - recall: 0.8177 - val_loss: 0.4577 - val_accuracy: 0.8408 - val_f1_score: 0.8392 - val_f1_score_micro: 0.8408 - val_precision: 0.8812 - val_recall: 0.8001\n",
            "Epoch 34/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4156 - accuracy: 0.8587 - f1_score: 0.8577 - f1_score_micro: 0.8587 - precision: 0.8950 - recall: 0.8177 - val_loss: 0.4565 - val_accuracy: 0.8404 - val_f1_score: 0.8389 - val_f1_score_micro: 0.8404 - val_precision: 0.8819 - val_recall: 0.7997\n",
            "Epoch 35/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4146 - accuracy: 0.8591 - f1_score: 0.8581 - f1_score_micro: 0.8591 - precision: 0.8952 - recall: 0.8182 - val_loss: 0.4563 - val_accuracy: 0.8397 - val_f1_score: 0.8386 - val_f1_score_micro: 0.8397 - val_precision: 0.8812 - val_recall: 0.7988\n",
            "Epoch 36/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4134 - accuracy: 0.8591 - f1_score: 0.8582 - f1_score_micro: 0.8592 - precision: 0.8959 - recall: 0.8191 - val_loss: 0.4546 - val_accuracy: 0.8421 - val_f1_score: 0.8407 - val_f1_score_micro: 0.8421 - val_precision: 0.8818 - val_recall: 0.8018\n",
            "Epoch 37/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4124 - accuracy: 0.8595 - f1_score: 0.8586 - f1_score_micro: 0.8595 - precision: 0.8955 - recall: 0.8188 - val_loss: 0.4545 - val_accuracy: 0.8414 - val_f1_score: 0.8407 - val_f1_score_micro: 0.8414 - val_precision: 0.8806 - val_recall: 0.8017\n",
            "Epoch 38/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4111 - accuracy: 0.8599 - f1_score: 0.8590 - f1_score_micro: 0.8599 - precision: 0.8959 - recall: 0.8198 - val_loss: 0.4533 - val_accuracy: 0.8417 - val_f1_score: 0.8408 - val_f1_score_micro: 0.8417 - val_precision: 0.8816 - val_recall: 0.8022\n",
            "Epoch 39/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4104 - accuracy: 0.8609 - f1_score: 0.8600 - f1_score_micro: 0.8609 - precision: 0.8964 - recall: 0.8211 - val_loss: 0.4522 - val_accuracy: 0.8421 - val_f1_score: 0.8407 - val_f1_score_micro: 0.8421 - val_precision: 0.8813 - val_recall: 0.8037\n",
            "Epoch 40/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4092 - accuracy: 0.8611 - f1_score: 0.8602 - f1_score_micro: 0.8611 - precision: 0.8961 - recall: 0.8214 - val_loss: 0.4527 - val_accuracy: 0.8429 - val_f1_score: 0.8421 - val_f1_score_micro: 0.8429 - val_precision: 0.8809 - val_recall: 0.8038\n",
            "Epoch 41/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4082 - accuracy: 0.8612 - f1_score: 0.8603 - f1_score_micro: 0.8612 - precision: 0.8964 - recall: 0.8218 - val_loss: 0.4528 - val_accuracy: 0.8422 - val_f1_score: 0.8408 - val_f1_score_micro: 0.8422 - val_precision: 0.8810 - val_recall: 0.8054\n",
            "Epoch 42/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4074 - accuracy: 0.8612 - f1_score: 0.8603 - f1_score_micro: 0.8612 - precision: 0.8967 - recall: 0.8229 - val_loss: 0.4505 - val_accuracy: 0.8433 - val_f1_score: 0.8422 - val_f1_score_micro: 0.8433 - val_precision: 0.8822 - val_recall: 0.8051\n",
            "Epoch 43/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4065 - accuracy: 0.8618 - f1_score: 0.8610 - f1_score_micro: 0.8618 - precision: 0.8967 - recall: 0.8231 - val_loss: 0.4517 - val_accuracy: 0.8422 - val_f1_score: 0.8413 - val_f1_score_micro: 0.8422 - val_precision: 0.8806 - val_recall: 0.8025\n",
            "Epoch 44/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4058 - accuracy: 0.8621 - f1_score: 0.8612 - f1_score_micro: 0.8621 - precision: 0.8965 - recall: 0.8231 - val_loss: 0.4498 - val_accuracy: 0.8418 - val_f1_score: 0.8405 - val_f1_score_micro: 0.8418 - val_precision: 0.8814 - val_recall: 0.8057\n",
            "Epoch 45/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4048 - accuracy: 0.8627 - f1_score: 0.8619 - f1_score_micro: 0.8627 - precision: 0.8972 - recall: 0.8230 - val_loss: 0.4501 - val_accuracy: 0.8425 - val_f1_score: 0.8405 - val_f1_score_micro: 0.8425 - val_precision: 0.8785 - val_recall: 0.8044\n",
            "Epoch 46/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4041 - accuracy: 0.8629 - f1_score: 0.8620 - f1_score_micro: 0.8628 - precision: 0.8969 - recall: 0.8243 - val_loss: 0.4488 - val_accuracy: 0.8429 - val_f1_score: 0.8419 - val_f1_score_micro: 0.8429 - val_precision: 0.8806 - val_recall: 0.8051\n",
            "Epoch 47/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4034 - accuracy: 0.8626 - f1_score: 0.8617 - f1_score_micro: 0.8626 - precision: 0.8969 - recall: 0.8251 - val_loss: 0.4478 - val_accuracy: 0.8434 - val_f1_score: 0.8422 - val_f1_score_micro: 0.8434 - val_precision: 0.8810 - val_recall: 0.8057\n",
            "Epoch 48/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8627 - f1_score: 0.8619 - f1_score_micro: 0.8627 - precision: 0.8969 - recall: 0.8252 - val_loss: 0.4489 - val_accuracy: 0.8423 - val_f1_score: 0.8420 - val_f1_score_micro: 0.8423 - val_precision: 0.8798 - val_recall: 0.8058\n",
            "Epoch 49/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8634 - f1_score: 0.8626 - f1_score_micro: 0.8634 - precision: 0.8969 - recall: 0.8259 - val_loss: 0.4472 - val_accuracy: 0.8437 - val_f1_score: 0.8426 - val_f1_score_micro: 0.8437 - val_precision: 0.8810 - val_recall: 0.8060\n",
            "Epoch 50/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4012 - accuracy: 0.8634 - f1_score: 0.8626 - f1_score_micro: 0.8634 - precision: 0.8972 - recall: 0.8258 - val_loss: 0.4471 - val_accuracy: 0.8422 - val_f1_score: 0.8417 - val_f1_score_micro: 0.8422 - val_precision: 0.8794 - val_recall: 0.8065\n",
            "Epoch 51/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8637 - f1_score: 0.8630 - f1_score_micro: 0.8637 - precision: 0.8976 - recall: 0.8264 - val_loss: 0.4475 - val_accuracy: 0.8426 - val_f1_score: 0.8406 - val_f1_score_micro: 0.8426 - val_precision: 0.8803 - val_recall: 0.8081\n",
            "Epoch 52/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3999 - accuracy: 0.8638 - f1_score: 0.8630 - f1_score_micro: 0.8638 - precision: 0.8968 - recall: 0.8271 - val_loss: 0.4469 - val_accuracy: 0.8437 - val_f1_score: 0.8427 - val_f1_score_micro: 0.8437 - val_precision: 0.8808 - val_recall: 0.8087\n",
            "Epoch 53/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3993 - accuracy: 0.8640 - f1_score: 0.8632 - f1_score_micro: 0.8640 - precision: 0.8975 - recall: 0.8271 - val_loss: 0.4459 - val_accuracy: 0.8432 - val_f1_score: 0.8420 - val_f1_score_micro: 0.8432 - val_precision: 0.8795 - val_recall: 0.8069\n",
            "Epoch 54/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3984 - accuracy: 0.8641 - f1_score: 0.8633 - f1_score_micro: 0.8641 - precision: 0.8974 - recall: 0.8276 - val_loss: 0.4456 - val_accuracy: 0.8435 - val_f1_score: 0.8416 - val_f1_score_micro: 0.8435 - val_precision: 0.8815 - val_recall: 0.8076\n",
            "--- Starting trial: /run-6\n",
            "{'optimizer': 'adam', 'learning_rate': 0.001, 'beta_1': 0.86, 'beta_2': 0.97}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.6323 - accuracy: 0.7915 - f1_score: 0.7884 - f1_score_micro: 0.7915 - precision: 0.8739 - recall: 0.6792 - val_loss: 0.5253 - val_accuracy: 0.8196 - val_f1_score: 0.8203 - val_f1_score_micro: 0.8196 - val_precision: 0.8672 - val_recall: 0.7630\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4700 - accuracy: 0.8413 - f1_score: 0.8397 - f1_score_micro: 0.8413 - precision: 0.8838 - recall: 0.7902 - val_loss: 0.4830 - val_accuracy: 0.8363 - val_f1_score: 0.8356 - val_f1_score_micro: 0.8363 - val_precision: 0.8767 - val_recall: 0.7901\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4419 - accuracy: 0.8488 - f1_score: 0.8477 - f1_score_micro: 0.8488 - precision: 0.8859 - recall: 0.8057 - val_loss: 0.4942 - val_accuracy: 0.8306 - val_f1_score: 0.8229 - val_f1_score_micro: 0.8306 - val_precision: 0.8687 - val_recall: 0.7967\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4275 - accuracy: 0.8538 - f1_score: 0.8527 - f1_score_micro: 0.8538 - precision: 0.8890 - recall: 0.8166 - val_loss: 0.4713 - val_accuracy: 0.8329 - val_f1_score: 0.8350 - val_f1_score_micro: 0.8329 - val_precision: 0.8692 - val_recall: 0.7939\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8567 - f1_score: 0.8558 - f1_score_micro: 0.8567 - precision: 0.8896 - recall: 0.8213 - val_loss: 0.4546 - val_accuracy: 0.8419 - val_f1_score: 0.8422 - val_f1_score_micro: 0.8419 - val_precision: 0.8791 - val_recall: 0.8072\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4130 - accuracy: 0.8586 - f1_score: 0.8577 - f1_score_micro: 0.8586 - precision: 0.8911 - recall: 0.8243 - val_loss: 0.4564 - val_accuracy: 0.8418 - val_f1_score: 0.8420 - val_f1_score_micro: 0.8418 - val_precision: 0.8751 - val_recall: 0.8059\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.4074 - accuracy: 0.8610 - f1_score: 0.8602 - f1_score_micro: 0.8610 - precision: 0.8910 - recall: 0.8288 - val_loss: 0.4515 - val_accuracy: 0.8424 - val_f1_score: 0.8434 - val_f1_score_micro: 0.8424 - val_precision: 0.8755 - val_recall: 0.8093\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4042 - accuracy: 0.8605 - f1_score: 0.8597 - f1_score_micro: 0.8605 - precision: 0.8921 - recall: 0.8298 - val_loss: 0.4487 - val_accuracy: 0.8437 - val_f1_score: 0.8425 - val_f1_score_micro: 0.8437 - val_precision: 0.8748 - val_recall: 0.8104\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.8625 - f1_score: 0.8616 - f1_score_micro: 0.8625 - precision: 0.8924 - recall: 0.8330 - val_loss: 0.4492 - val_accuracy: 0.8440 - val_f1_score: 0.8427 - val_f1_score_micro: 0.8440 - val_precision: 0.8755 - val_recall: 0.8164\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3972 - accuracy: 0.8635 - f1_score: 0.8627 - f1_score_micro: 0.8635 - precision: 0.8930 - recall: 0.8339 - val_loss: 0.4440 - val_accuracy: 0.8469 - val_f1_score: 0.8456 - val_f1_score_micro: 0.8469 - val_precision: 0.8770 - val_recall: 0.8164\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3948 - accuracy: 0.8638 - f1_score: 0.8630 - f1_score_micro: 0.8638 - precision: 0.8928 - recall: 0.8346 - val_loss: 0.4419 - val_accuracy: 0.8476 - val_f1_score: 0.8461 - val_f1_score_micro: 0.8476 - val_precision: 0.8792 - val_recall: 0.8169\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3922 - accuracy: 0.8661 - f1_score: 0.8652 - f1_score_micro: 0.8661 - precision: 0.8943 - recall: 0.8364 - val_loss: 0.4537 - val_accuracy: 0.8452 - val_f1_score: 0.8421 - val_f1_score_micro: 0.8452 - val_precision: 0.8782 - val_recall: 0.8164\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3901 - accuracy: 0.8663 - f1_score: 0.8655 - f1_score_micro: 0.8663 - precision: 0.8937 - recall: 0.8379 - val_loss: 0.4641 - val_accuracy: 0.8376 - val_f1_score: 0.8377 - val_f1_score_micro: 0.8376 - val_precision: 0.8666 - val_recall: 0.8090\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3894 - accuracy: 0.8654 - f1_score: 0.8646 - f1_score_micro: 0.8654 - precision: 0.8936 - recall: 0.8373 - val_loss: 0.4615 - val_accuracy: 0.8429 - val_f1_score: 0.8376 - val_f1_score_micro: 0.8429 - val_precision: 0.8716 - val_recall: 0.8156\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3879 - accuracy: 0.8664 - f1_score: 0.8656 - f1_score_micro: 0.8664 - precision: 0.8936 - recall: 0.8391 - val_loss: 0.4599 - val_accuracy: 0.8418 - val_f1_score: 0.8407 - val_f1_score_micro: 0.8418 - val_precision: 0.8667 - val_recall: 0.8092\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3863 - accuracy: 0.8676 - f1_score: 0.8668 - f1_score_micro: 0.8676 - precision: 0.8944 - recall: 0.8398 - val_loss: 0.4498 - val_accuracy: 0.8475 - val_f1_score: 0.8448 - val_f1_score_micro: 0.8475 - val_precision: 0.8746 - val_recall: 0.8181\n",
            "--- Starting trial: /run-7\n",
            "{'optimizer': 'adam', 'learning_rate': 0.001, 'beta_1': 0.86, 'beta_2': 0.999}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.6504 - accuracy: 0.7837 - f1_score: 0.7796 - f1_score_micro: 0.7837 - precision: 0.8746 - recall: 0.6634 - val_loss: 0.5293 - val_accuracy: 0.8180 - val_f1_score: 0.8152 - val_f1_score_micro: 0.8180 - val_precision: 0.8703 - val_recall: 0.7568\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4818 - accuracy: 0.8375 - f1_score: 0.8359 - f1_score_micro: 0.8375 - precision: 0.8826 - recall: 0.7809 - val_loss: 0.4951 - val_accuracy: 0.8300 - val_f1_score: 0.8255 - val_f1_score_micro: 0.8300 - val_precision: 0.8714 - val_recall: 0.7864\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4495 - accuracy: 0.8472 - f1_score: 0.8460 - f1_score_micro: 0.8472 - precision: 0.8867 - recall: 0.8015 - val_loss: 0.4726 - val_accuracy: 0.8352 - val_f1_score: 0.8353 - val_f1_score_micro: 0.8352 - val_precision: 0.8760 - val_recall: 0.7936\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4327 - accuracy: 0.8515 - f1_score: 0.8506 - f1_score_micro: 0.8515 - precision: 0.8887 - recall: 0.8116 - val_loss: 0.4624 - val_accuracy: 0.8397 - val_f1_score: 0.8369 - val_f1_score_micro: 0.8397 - val_precision: 0.8790 - val_recall: 0.8027\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.8551 - f1_score: 0.8542 - f1_score_micro: 0.8551 - precision: 0.8907 - recall: 0.8176 - val_loss: 0.4579 - val_accuracy: 0.8413 - val_f1_score: 0.8387 - val_f1_score_micro: 0.8413 - val_precision: 0.8784 - val_recall: 0.8063\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4142 - accuracy: 0.8575 - f1_score: 0.8566 - f1_score_micro: 0.8575 - precision: 0.8910 - recall: 0.8222 - val_loss: 0.4576 - val_accuracy: 0.8393 - val_f1_score: 0.8353 - val_f1_score_micro: 0.8393 - val_precision: 0.8776 - val_recall: 0.8061\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4076 - accuracy: 0.8593 - f1_score: 0.8585 - f1_score_micro: 0.8593 - precision: 0.8919 - recall: 0.8251 - val_loss: 0.4519 - val_accuracy: 0.8433 - val_f1_score: 0.8418 - val_f1_score_micro: 0.8433 - val_precision: 0.8779 - val_recall: 0.8074\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8609 - f1_score: 0.8602 - f1_score_micro: 0.8609 - precision: 0.8930 - recall: 0.8282 - val_loss: 0.4467 - val_accuracy: 0.8443 - val_f1_score: 0.8414 - val_f1_score_micro: 0.8443 - val_precision: 0.8786 - val_recall: 0.8122\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3993 - accuracy: 0.8615 - f1_score: 0.8607 - f1_score_micro: 0.8615 - precision: 0.8930 - recall: 0.8296 - val_loss: 0.4498 - val_accuracy: 0.8444 - val_f1_score: 0.8421 - val_f1_score_micro: 0.8444 - val_precision: 0.8789 - val_recall: 0.8120\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3964 - accuracy: 0.8622 - f1_score: 0.8614 - f1_score_micro: 0.8622 - precision: 0.8931 - recall: 0.8319 - val_loss: 0.4475 - val_accuracy: 0.8432 - val_f1_score: 0.8415 - val_f1_score_micro: 0.8432 - val_precision: 0.8749 - val_recall: 0.8117\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3929 - accuracy: 0.8629 - f1_score: 0.8622 - f1_score_micro: 0.8629 - precision: 0.8937 - recall: 0.8333 - val_loss: 0.4450 - val_accuracy: 0.8447 - val_f1_score: 0.8418 - val_f1_score_micro: 0.8447 - val_precision: 0.8772 - val_recall: 0.8149\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3908 - accuracy: 0.8640 - f1_score: 0.8632 - f1_score_micro: 0.8640 - precision: 0.8942 - recall: 0.8346 - val_loss: 0.4558 - val_accuracy: 0.8431 - val_f1_score: 0.8382 - val_f1_score_micro: 0.8431 - val_precision: 0.8756 - val_recall: 0.8124\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8658 - f1_score: 0.8650 - f1_score_micro: 0.8658 - precision: 0.8945 - recall: 0.8346 - val_loss: 0.4456 - val_accuracy: 0.8432 - val_f1_score: 0.8406 - val_f1_score_micro: 0.8432 - val_precision: 0.8757 - val_recall: 0.8153\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3860 - accuracy: 0.8650 - f1_score: 0.8643 - f1_score_micro: 0.8650 - precision: 0.8945 - recall: 0.8361 - val_loss: 0.4510 - val_accuracy: 0.8433 - val_f1_score: 0.8409 - val_f1_score_micro: 0.8433 - val_precision: 0.8733 - val_recall: 0.8156\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.3845 - accuracy: 0.8658 - f1_score: 0.8651 - f1_score_micro: 0.8658 - precision: 0.8949 - recall: 0.8375 - val_loss: 0.4468 - val_accuracy: 0.8431 - val_f1_score: 0.8406 - val_f1_score_micro: 0.8431 - val_precision: 0.8746 - val_recall: 0.8152\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3831 - accuracy: 0.8670 - f1_score: 0.8663 - f1_score_micro: 0.8670 - precision: 0.8961 - recall: 0.8382 - val_loss: 0.4388 - val_accuracy: 0.8478 - val_f1_score: 0.8455 - val_f1_score_micro: 0.8478 - val_precision: 0.8788 - val_recall: 0.8165\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 1s 2ms/step - loss: 0.3819 - accuracy: 0.8664 - f1_score: 0.8657 - f1_score_micro: 0.8664 - precision: 0.8947 - recall: 0.8383 - val_loss: 0.4366 - val_accuracy: 0.8475 - val_f1_score: 0.8470 - val_f1_score_micro: 0.8475 - val_precision: 0.8779 - val_recall: 0.8158\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3809 - accuracy: 0.8663 - f1_score: 0.8655 - f1_score_micro: 0.8663 - precision: 0.8967 - recall: 0.8385 - val_loss: 0.4522 - val_accuracy: 0.8431 - val_f1_score: 0.8386 - val_f1_score_micro: 0.8431 - val_precision: 0.8727 - val_recall: 0.8179\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3798 - accuracy: 0.8673 - f1_score: 0.8665 - f1_score_micro: 0.8673 - precision: 0.8953 - recall: 0.8396 - val_loss: 0.4412 - val_accuracy: 0.8457 - val_f1_score: 0.8441 - val_f1_score_micro: 0.8457 - val_precision: 0.8751 - val_recall: 0.8197\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3774 - accuracy: 0.8678 - f1_score: 0.8670 - f1_score_micro: 0.8678 - precision: 0.8964 - recall: 0.8407 - val_loss: 0.4443 - val_accuracy: 0.8483 - val_f1_score: 0.8452 - val_f1_score_micro: 0.8483 - val_precision: 0.8759 - val_recall: 0.8189\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3770 - accuracy: 0.8678 - f1_score: 0.8670 - f1_score_micro: 0.8678 - precision: 0.8960 - recall: 0.8404 - val_loss: 0.4423 - val_accuracy: 0.8451 - val_f1_score: 0.8431 - val_f1_score_micro: 0.8451 - val_precision: 0.8753 - val_recall: 0.8176\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3757 - accuracy: 0.8688 - f1_score: 0.8680 - f1_score_micro: 0.8688 - precision: 0.8965 - recall: 0.8421 - val_loss: 0.4405 - val_accuracy: 0.8453 - val_f1_score: 0.8448 - val_f1_score_micro: 0.8453 - val_precision: 0.8771 - val_recall: 0.8140\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3757 - accuracy: 0.8683 - f1_score: 0.8676 - f1_score_micro: 0.8683 - precision: 0.8963 - recall: 0.8419 - val_loss: 0.4490 - val_accuracy: 0.8412 - val_f1_score: 0.8395 - val_f1_score_micro: 0.8412 - val_precision: 0.8712 - val_recall: 0.8147\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3741 - accuracy: 0.8687 - f1_score: 0.8680 - f1_score_micro: 0.8687 - precision: 0.8966 - recall: 0.8418 - val_loss: 0.4476 - val_accuracy: 0.8434 - val_f1_score: 0.8411 - val_f1_score_micro: 0.8434 - val_precision: 0.8723 - val_recall: 0.8150\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.3743 - accuracy: 0.8690 - f1_score: 0.8683 - f1_score_micro: 0.8690 - precision: 0.8971 - recall: 0.8426 - val_loss: 0.4506 - val_accuracy: 0.8436 - val_f1_score: 0.8388 - val_f1_score_micro: 0.8436 - val_precision: 0.8731 - val_recall: 0.8177\n",
            "--- Starting trial: /run-8\n",
            "{'optimizer': 'adam', 'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.97}\n",
            "Epoch 1/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.6402 - accuracy: 0.7847 - f1_score: 0.7805 - f1_score_micro: 0.7847 - precision: 0.8728 - recall: 0.6751 - val_loss: 0.5194 - val_accuracy: 0.8229 - val_f1_score: 0.8224 - val_f1_score_micro: 0.8229 - val_precision: 0.8736 - val_recall: 0.7618\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4712 - accuracy: 0.8397 - f1_score: 0.8380 - f1_score_micro: 0.8397 - precision: 0.8817 - recall: 0.7878 - val_loss: 0.4818 - val_accuracy: 0.8349 - val_f1_score: 0.8339 - val_f1_score_micro: 0.8349 - val_precision: 0.8791 - val_recall: 0.7901\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4418 - accuracy: 0.8481 - f1_score: 0.8470 - f1_score_micro: 0.8481 - precision: 0.8872 - recall: 0.8073 - val_loss: 0.4655 - val_accuracy: 0.8402 - val_f1_score: 0.8384 - val_f1_score_micro: 0.8402 - val_precision: 0.8789 - val_recall: 0.8032\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4286 - accuracy: 0.8536 - f1_score: 0.8526 - f1_score_micro: 0.8536 - precision: 0.8886 - recall: 0.8160 - val_loss: 0.4686 - val_accuracy: 0.8367 - val_f1_score: 0.8367 - val_f1_score_micro: 0.8367 - val_precision: 0.8731 - val_recall: 0.8031\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4186 - accuracy: 0.8565 - f1_score: 0.8556 - f1_score_micro: 0.8565 - precision: 0.8894 - recall: 0.8201 - val_loss: 0.4588 - val_accuracy: 0.8413 - val_f1_score: 0.8390 - val_f1_score_micro: 0.8413 - val_precision: 0.8763 - val_recall: 0.8098\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4135 - accuracy: 0.8575 - f1_score: 0.8566 - f1_score_micro: 0.8575 - precision: 0.8890 - recall: 0.8243 - val_loss: 0.4530 - val_accuracy: 0.8429 - val_f1_score: 0.8436 - val_f1_score_micro: 0.8429 - val_precision: 0.8766 - val_recall: 0.8075\n",
            "Epoch 7/100\n",
            "771/938 [=======================>......] - ETA: 0s - loss: 0.4092 - accuracy: 0.8588 - f1_score: 0.8579 - f1_score_micro: 0.8588 - precision: 0.8908 - recall: 0.8254"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m--- Starting trial: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m run_name)\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m({h\u001b[39m.\u001b[39mname: hparams[h] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hparams})\n\u001b[0;32m---> 31\u001b[0m train_test_model(hparams, log_dir \u001b[39m+\u001b[39;49m run_name)\n\u001b[1;32m     32\u001b[0m session_num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "Cell \u001b[0;32mIn[12], line 27\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[0;34m(hparams, run_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m softmax_model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m     17\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, tfa\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mF1Score(average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m,num_classes\u001b[39m=\u001b[39mnum_classes),tfa\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mF1Score(average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m,num_classes\u001b[39m=\u001b[39mnum_classes, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf1_score_micro\u001b[39m\u001b[39m\"\u001b[39m), tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mPrecision(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m), tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mRecall(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     early_stop_callback,\n\u001b[1;32m     23\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mTensorBoard(run_dir),\u001b[39m# log metrics\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     hp\u001b[39m.\u001b[39mKerasCallback(run_dir, hparams),  \u001b[39m# log hparams\u001b[39;00m\n\u001b[1;32m     25\u001b[0m   ]\n\u001b[0;32m---> 27\u001b[0m softmax_model\u001b[39m.\u001b[39;49mfit(train_X, train_y_cat, validation_data\u001b[39m=\u001b[39;49m(test_X, test_y_cat), batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m~/anaconda3/envs/nn-tp1/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "session_num = 0\n",
        "\n",
        "for optimizer in HP_OPTIMIZER.domain.values:\n",
        "  for learning_rate in HP_LEARN_RATE.domain.values:\n",
        "    if(optimizer == 'sgd'):\n",
        "      for momentum in HP_MOMENTUM.domain.values:\n",
        "        for nesterov in HP_NESTEROV.domain.values:\n",
        "          hparams = {\n",
        "            HP_OPTIMIZER: optimizer,\n",
        "            HP_LEARN_RATE: learning_rate,\n",
        "            HP_MOMENTUM: momentum,\n",
        "            HP_NESTEROV: nesterov,\n",
        "          }\n",
        "          run_name = \"/run-%d\" % session_num\n",
        "          print('--- Starting trial: %s' % run_name)\n",
        "          print({h.name: hparams[h] for h in hparams})\n",
        "          train_test_model(hparams, log_dir + run_name)\n",
        "          session_num += 1\n",
        "    elif(optimizer == 'adam' or optimizer == 'nadam'):\n",
        "      for beta_1 in HP_BETA_1.domain.values:\n",
        "        for beta_2 in HP_BETA_2.domain.values:\n",
        "          hparams = {\n",
        "            HP_OPTIMIZER: optimizer,\n",
        "            HP_LEARN_RATE: learning_rate,\n",
        "            HP_BETA_1: beta_1,\n",
        "            HP_BETA_2: beta_2,\n",
        "          }\n",
        "          run_name = \"/run-%d\" % session_num\n",
        "          print('--- Starting trial: %s' % run_name)\n",
        "          print({h.name: hparams[h] for h in hparams})\n",
        "          train_test_model(hparams, log_dir + run_name)\n",
        "          session_num += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Final testing with best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc7g_gev2apS"
      },
      "outputs": [],
      "source": [
        "softmax_model =  Sequential()\n",
        "#model.add(preprocessing.RandomFlip(\"horizontal\", input_shape=(28,28,1)))\n",
        "# model.add(layers.Dropout(0.1, input_shape=(28,28)))\n",
        "softmax_model.add(layers.Flatten(input_shape=(28,28)))\n",
        "softmax_model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "softmax_model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Softmax model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to stop training if, after 5 epochs, the accuracy is not improving\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to save the weights of the best model\n",
        "checkpoint_filepath = '/tmp/checkpoint/softmax'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs and metrics from TensorBoard\n",
        "log_dir_fit = \"logs/fit/softmax/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_fit, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu4LHsfg2fx0"
      },
      "outputs": [],
      "source": [
        "optimizer = SGD(learning_rate=0.0002, momentum=0.95)\n",
        "softmax_model.compile(loss = 'categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.95, beta_2=0.94),metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fit the model to the train data and validate it with the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We load the previously best weights to save time on training\n",
        "# if (exists(checkpoint_filepath)):\n",
        "    # softmax_model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvNQ1f9C3Ory",
        "outputId": "1b5f8cfa-4f8d-4532-c436-497bd26313d6"
      },
      "outputs": [],
      "source": [
        "softmax_history = softmax_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 32, epochs=100, callbacks=[tensorboard_callback, early_stop_callback])\n",
        "# softmax_history = softmax_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs=10, callbacks=[model_checkpoint_callback, tensorboard_callback, early_stop_callback])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot important metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TensorBoard session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "3-YPKw9V3pfW",
        "outputId": "a509a96e-d168-4c1d-ccb9-185ddda13bf4"
      },
      "outputs": [],
      "source": [
        "plt.plot(softmax_history.history[\"loss\"], label=\"Train\")\n",
        "plt.plot(softmax_history.history[\"val_loss\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss evolution through epochs - Softmax')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "vYsmvi626L2g",
        "outputId": "227c99c2-6808-47e3-ad82-be7470ced0a1"
      },
      "outputs": [],
      "source": [
        "plt.plot(softmax_history.history[\"accuracy\"], label=\"Train\")\n",
        "plt.plot(softmax_history.history[\"val_accuracy\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy evolution through epochs - Softmax')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(softmax_history.history[\"precision\"], label=\"Train\")\n",
        "plt.plot(softmax_history.history[\"val_precision\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision evolution through epochs - Softmax')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(softmax_history.history[\"recall\"], label=\"Train\")\n",
        "plt.plot(softmax_history.history[\"val_recall\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Recall evolution through epochs - Softmax')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### F1-Score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Macro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(softmax_history.history[\"f1_score\"], label=\"Train\")\n",
        "plt.plot(softmax_history.history[\"val_f1_score\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score (Macro)')\n",
        "plt.title('F1 Score (Macro) evolution through epochs - Softmax')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Micro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(softmax_history.history[\"f1_score_micro\"], label=\"Train\")\n",
        "plt.plot(softmax_history.history[\"val_f1_score_micro\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score (Micro)')\n",
        "plt.title('F1 Score (Micro) evolution through epochs - Softmax')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este problema los F1 Score tanto macro como micro son muy similares a causa de que no existe desbalance de clases."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ROC and AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot linewidth.\n",
        "lw = 2\n",
        "\n",
        "# Get score\n",
        "y_score = softmax_model.predict(test_X)\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(test_y_cat[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_y_cat.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Compute macro-average ROC curve and ROC area\n",
        "\n",
        "# First aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
        "\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(num_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr /= num_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure(1, figsize=(10,10))\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i], lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "#colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "#for i, color in zip(range(num_classes), colors):\n",
        "#    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "#             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "#             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Extension of Receiver Operating Characteristic to multi-class - Softmax')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Zoom in view of the upper left corner.\n",
        "plt.figure(2, figsize=(20,10))\n",
        "plt.xlim(0, 0.2)\n",
        "plt.ylim(0.8, 1)\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "#colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "#for i, color in zip(range(num_classes), colors):\n",
        "#    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "#             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "#             ''.format(i, roc_auc[i]))\n",
        "    \n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i], lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Zoomed - Extension of Receiver Operating Characteristic to multi-class - Softmax')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al igual que para el F1 Score, las curvas ROC tanto macro como micro dan muy similares porque no hay desbalance de clases."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp_model = Sequential()\n",
        "mlp_model.add(layers.Flatten(input_shape=(28,28)))\n",
        "mlp_model.add(layers.Dense(256, activation='relu'))\n",
        "mlp_model.add(layers.Dense(64, activation='relu'))\n",
        "mlp_model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "mlp_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp_model.compile(loss = 'categorical_crossentropy', optimizer=SGD(learning_rate=0.0002, momentum=0.95),metrics=[\"accuracy\", tfa.metrics.F1Score(average='macro',num_classes=num_classes),tfa.metrics.F1Score(average='micro',num_classes=num_classes, name=\"f1_score_micro\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to stop training if, after 2 epochs, the accuracy is not improving\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback to save the weights of the best model\n",
        "checkpoint_filepath = '/tmp/checkpoint/mlp'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs and metrics from TensorBoard\n",
        "log_dir_fit = \"logs/fit/mlp/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_fit, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fit the model to the train data and validate it with the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We load the previously best weights to save time on training\n",
        "# if (exists(checkpoint_filepath)):\n",
        "    # mlp_model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp_history = mlp_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs=10, callbacks=[tensorboard_callback, early_stop_callback])\n",
        "# softmax_history = softmax_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs=10, callbacks=[model_checkpoint_callback, tensorboard_callback, early_stop_callback])\n",
        "# mlp_history = mlp_model.fit(train_X, train_y_cat, validation_data=(test_X, test_y_cat), batch_size = 64, epochs=10, callbacks=[model_checkpoint_callback, tensorboard_callback, early_stop_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot important metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TensorBoard session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(mlp_history.history[\"loss\"], label=\"Train\")\n",
        "plt.plot(mlp_history.history[\"val_loss\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss evolution through epochs - MLP')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(mlp_history.history[\"accuracy\"], label=\"Train\")\n",
        "plt.plot(mlp_history.history[\"val_accuracy\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy evolution through epochs - MLP')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(mlp_history.history[\"precision\"], label=\"Train\")\n",
        "plt.plot(mlp_history.history[\"val_precision\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision evolution through epochs - MLP')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(mlp_history.history[\"recall\"], label=\"Train\")\n",
        "plt.plot(mlp_history.history[\"val_recall\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Recall evolution through epochs - MLP')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### F1 Score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Macro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(mlp_history.history[\"f1_score\"], label=\"Train\")\n",
        "plt.plot(mlp_history.history[\"val_f1_score\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score (Macro)')\n",
        "plt.title('F1 Score (Macro) evolution through epochs - MLP')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Micro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(mlp_history.history[\"f1_score_micro\"], label=\"Train\")\n",
        "plt.plot(mlp_history.history[\"val_f1_score_micro\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score (Micro)')\n",
        "plt.title('F1 Score (Micro) evolution through epochs - MLP')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ROC and AUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot linewidth.\n",
        "lw = 2\n",
        "\n",
        "# Get score\n",
        "y_score = mlp_model.predict(test_X)\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(test_y_cat[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(test_y_cat.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Compute macro-average ROC curve and ROC area\n",
        "\n",
        "# First aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
        "\n",
        "# Then interpolate all ROC curves at this points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(num_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Finally average it and compute AUC\n",
        "mean_tpr /= num_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure(1, figsize=(10,10))\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i], lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "#colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "#for i, color in zip(range(num_classes), colors):\n",
        "#    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "#             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "#             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Extension of Receiver Operating Characteristic to multi-class - MLP')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Zoom in view of the upper left corner.\n",
        "plt.figure(2, figsize=(20,10))\n",
        "plt.xlim(0, 0.2)\n",
        "plt.ylim(0.8, 1)\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "#colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "#for i, color in zip(range(num_classes), colors):\n",
        "#    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "#             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "#             ''.format(i, roc_auc[i]))\n",
        "    \n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i], lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Zoomed - Extension of Receiver Operating Characteristic to multi-class - MLP')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
